{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linux install\n",
    "\n",
    "* curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# start service\n",
    "\n",
    "* ollama serve\n",
    "\n",
    "# stop \n",
    "\n",
    "* sudo systemctl stop ollama.service\n",
    "\n",
    "# disable it if you want\n",
    "* systemctl disable ollama.service\n",
    "\n",
    "# confirm its status\n",
    "* systemctl status ollama.service\n",
    "\n",
    "# list models \n",
    "\n",
    "* ollama list\n",
    "\n",
    "# run model\n",
    "\n",
    "* ollama run llama2\n",
    "* ollama run qwen2:0.5b # with a specific version.\n",
    "* ollama run qwen2:0.5b --stream\n",
    "* more models supported and docs here: https://ollama.com/library\n",
    "\n",
    "# docs \n",
    "\n",
    "* https://ollama.ai/docs\n",
    "\n",
    "\n",
    "## api\n",
    "\n",
    "* https://github.com/ollama/ollama/blob/main/docs/api.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trouble shooting\n",
    "\n",
    "## CUDA error: CUBLAS_STATUS_NOT_SUPPORTED\n",
    "\n",
    "* sudo apt install nvidia-cuda-toolkit"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
