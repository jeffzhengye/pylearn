{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3',  \n",
    "                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "sentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\n",
    "sentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n",
    "               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n",
    "\n",
    "embeddings_1 = model.encode(sentences_1, \n",
    "                            batch_size=12, \n",
    "                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n",
    "                            )['dense_vecs']\n",
    "embeddings_2 = model.encode(sentences_2)['dense_vecs']\n",
    "similarity = embeddings_1 @ embeddings_2.T\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentences_1, \n",
    "                            batch_size=12, \n",
    "                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dense_vecs', 'lexical_weights', 'colbert_vecs'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3acb0da06346ac86f936e34defc593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644c52623b134e8d831d3c0e7ec6e8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e86e1a077624a94aa412e448e0a88aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944648d748f74feaaa82d52ecf8066c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3dd3e31755403b91e015e61f5fa999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/431 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e04ec0b62cf4c00bf459427756456aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------using 2*GPUs----------\n",
      "-1.5234375\n",
      "[-5.60546875, 5.76171875]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagReranker\n",
    "reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "score = reranker.compute_score(['query', 'passage'])\n",
    "print(score)\n",
    "\n",
    "scores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm_embedder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------using 2*GPUs----------\n",
      "[[0.89705944 0.853418  ]\n",
      " [0.8462473  0.9091402 ]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "\n",
    "from FlagEmbedding import LLMEmbedder\n",
    "\n",
    "\n",
    "# Define queries and keys\n",
    "\n",
    "queries = [\"test query 1\", \"test query 2\"]\n",
    "\n",
    "keys = [\"test key 1\", \"test key 2\"]\n",
    "\n",
    "\n",
    "# Load model (automatically use GPUs)\n",
    "\n",
    "model = LLMEmbedder(\"BAAI/llm-embedder\", use_fp16=False)\n",
    "\n",
    "\n",
    "# Encode for a specific task (qa, icl, chat, lrlm, tool, convsearch)\n",
    "task = \"qa\"\n",
    "\n",
    "query_embeddings = model.encode_queries(queries, task=task)\n",
    "\n",
    "key_embeddings = model.encode_keys(keys, task=task)\n",
    "\n",
    "\n",
    "similarity = query_embeddings @ key_embeddings.T\n",
    "\n",
    "print(similarity)\n",
    "\n",
    "# [[0.8971, 0.8534]\n",
    "\n",
    "# [0.8462, 0.9091]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(query_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual\n",
    "\n",
    "## bge-base-en-v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8750]]) tensor([[0.7816]])\n"
     ]
    }
   ],
   "source": [
    "####### Use Visualized BGE doing composed image retrieval\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"D:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual\")\n",
    "import torch\n",
    "from FlagEmbedding.visual.modeling import Visualized_BGE\n",
    "\n",
    "model_weight = \"D:/dataset/nlp/models/bge/Visualized_base_en_v1.5.pth\"\n",
    "model = Visualized_BGE(model_name_bge = \"BAAI/bge-base-en-v1.5\", model_weight=model_weight)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    query_emb = model.encode(image=\"d:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual/imgs/cir_query.png\", text=\"Make the background dark, as if the camera has taken the photo at night\")\n",
    "    candi_emb_1 = model.encode(image=\"d:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual/imgs/cir_candi_1.png\")\n",
    "    candi_emb_2 = model.encode(image=\"d:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual/imgs/cir_candi_2.png\")\n",
    "\n",
    "sim_1 = query_emb @ candi_emb_1.T\n",
    "sim_2 = query_emb @ candi_emb_2.T\n",
    "print(sim_1, sim_2) # tensor([[0.8750]]) tensor([[0.7816]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4483]]) tensor([[0.4368]]) tensor([[0.4372]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # query_emb = model.encode(image=\"d:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual/imgs/SFT-CIRR.png\", text=\"horse\")\n",
    "    query_emb = model.encode(text=\"horse\")\n",
    "    candi_emb_1 = model.encode(image=\"d:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual/imgs/cir_candi_1.png\")\n",
    "    candi_emb_2 = model.encode(image=\"d:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual/imgs/cir_candi_2.png\")\n",
    "    candi_emb_3 = model.encode(image=\"d:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual/imgs/wiki_candi_1.jpg\")\n",
    "\n",
    "sim_1 = query_emb @ candi_emb_1.T\n",
    "sim_2 = query_emb @ candi_emb_2.T\n",
    "sim_3 = query_emb @ candi_emb_3.T\n",
    "print(sim_1, sim_2, sim_3) # tensor([[0.8750]]) tensor([[0.7816]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAAI/bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6983]]) tensor([[0.6557]])\n"
     ]
    }
   ],
   "source": [
    "####### Use Visualized BGE doing composed image retrieval\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"D:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual\")\n",
    "import torch\n",
    "from FlagEmbedding.visual.modeling import Visualized_BGE\n",
    "\n",
    "model_weight = \"D:/dataset/nlp/models/bge/Visualized_m3.pth\"\n",
    "model = Visualized_BGE(model_name_bge = \"BAAI/bge-m3\", model_weight=model_weight)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    query_emb = model.encode(image=\"d:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual/imgs/cir_query.png\", text=\"Make the background dark, as if the camera has taken the photo at night\")\n",
    "    candi_emb_1 = model.encode(image=\"d:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual/imgs/cir_candi_1.png\")\n",
    "    candi_emb_2 = model.encode(image=\"d:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual/imgs/cir_candi_2.png\")\n",
    "\n",
    "sim_1 = query_emb @ candi_emb_1.T\n",
    "sim_2 = query_emb @ candi_emb_2.T\n",
    "print(sim_1, sim_2) # tensor([[0.8750]]) tensor([[0.7816]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2184]]) tensor([[0.2960]]) tensor([[0.2056]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # query_emb = model.encode(image=\"d:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual/imgs/SFT-CIRR.png\", text=\"horse\")\n",
    "    # query_emb = model.encode(text=\"é©¬\")\n",
    "    query_emb = model.encode(image=\"d:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual/imgs/cir_candi_1.png\")\n",
    "    candi_emb_1 = model.encode(image=\"d:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual/imgs/cir_candi_1.png\")\n",
    "    candi_emb_2 = model.encode(image=\"d:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual/imgs/cir_candi_2.png\")\n",
    "    candi_emb_3 = model.encode(image=\"d:/code/nlp/embeddings/FlagEmbedding/FlagEmbedding/visual/imgs/wiki_candi_1.jpg\")\n",
    "\n",
    "sim_1 = query_emb @ candi_emb_1.T\n",
    "sim_2 = query_emb @ candi_emb_2.T\n",
    "sim_3 = query_emb @ candi_emb_3.T\n",
    "print(sim_1, sim_2, sim_3) # tensor([[0.8750]]) tensor([[0.7816]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trouble shooting\n",
    "\n",
    "## module 'lib' has no attribute 'X509_V_FLAG_NOTIFY_POLICY'\n",
    "\n",
    "* pip install pyopenssl==24.0.0\n",
    "\n",
    "## Getting: ValueError: Attempting to unscale FP16 gradients\n",
    "\n",
    "* https://huggingface.co/docs/peft/v0.8.0/en/developer_guides/troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
