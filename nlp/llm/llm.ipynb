{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awesome LLM\n",
    "* https://github.com/Hannibal046/Awesome-LLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open source models\n",
    "\n",
    "## Character\n",
    "\n",
    "### ChatPLUG\n",
    "\n",
    "* https://github.com/X-PLUG/ChatPLUG\n",
    "\n",
    "## åŒ»ç–—å¤§æ¨¡å‹\n",
    "\n",
    "* [å¤æ—¦å¤§å­¦å›¢é˜Ÿå‘å¸ƒä¸­æ–‡åŒ»ç–—å¥åº·ä¸ªäººåŠ©æ‰‹ï¼ŒåŒæ—¶å¼€æº 47 ä¸‡é«˜è´¨é‡æ•°æ®é›†](https://www.toutiao.com/w/1781243874145280/)\n",
    "  * DISC-MedLLM æ˜¯ä¸€ä¸ªä¸“ä¸ºåŒ»ç–—å¥åº·å¯¹è¯åœºæ™¯è€Œæ‰“é€ çš„é¢†åŸŸå¤§æ¨¡å‹ï¼Œå®ƒå¯ä»¥æ»¡è¶³æ‚¨çš„å„ç§åŒ»ç–—ä¿å¥éœ€æ±‚ï¼ŒåŒ…æ‹¬ç–¾ç—…é—®è¯Šå’Œæ²»ç–—æ–¹æ¡ˆå’¨è¯¢ç­‰ï¼Œä¸ºæ‚¨æä¾›é«˜è´¨é‡çš„å¥åº·æ”¯æŒæœåŠ¡ã€‚\n",
    "* [åä½—å¤§æ¨¡å‹](https://modelscope.cn/models/MaingBen/HuatuoGPT-reward-model-7B/summary)\n",
    "* DoctorGPT\n",
    "* Apolloï¼šä¸€ä¸ªå¼€æºè½»é‡çº§å¤šè¯­è¨€åŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆ0.5Bã€1.8Bã€2Bã€6Bå’Œ7Bï¼‰\n",
    "  * é€šè¿‡ä»£ç†è°ƒä¼˜ï¼ˆProxy Tuningï¼‰æ¥æå‡å¤§å‹æ¨¡å‹çš„å¤šè¯­è¨€åŒ»ç–—èƒ½åŠ›\n",
    "\n",
    "\n",
    "## gpt4-pdf-chatbot-langchain\n",
    "* https://github.com/mayooear/gpt4-pdf-chatbot-langchain\n",
    "* é’ˆå¯¹ PDF æ–‡ä»¶æ„å»ºçš„ GPT æœºå™¨äººï¼Œä¸Šä¼ ä½ çš„ PDF æ–‡ä»¶ï¼Œä½¿ç”¨çš„æŠ€æœ¯å †æ ˆåŒ…æ‹¬ LangChainã€Pineconeã€Typescriptã€Openai å’Œ Next.jsã€‚åŸºäº Open AI å’Œ LangChainï¼Œå¯ä»¥åˆ†æ PDF æ–‡æ¡£ä¸­çš„æ–‡å­—å’Œå†…å®¹ï¼Œé€šè¿‡ embedding API ç”Ÿæˆå‘é‡ï¼Œç„¶åå­˜å‚¨åˆ°æ•°æ®åº“ä¸­. æœ€ååšæˆç±»ä¼¼äº ChatGPT çš„æœºå™¨äººï¼Œé€šè¿‡æœºå™¨äººå¿«é€Ÿçš„è¿›è¡ŒæŸ¥è¯¢ã€è¾“å‡ºç­”æ¡ˆã€‚\n",
    "\n",
    "##  LLama\n",
    "* paper: https://arxiv.org/pdf/2302.13971.pdf\n",
    "  * data Wikipedia [4.5%]: which use either the Latin or Cyrillic scripts: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk.\n",
    "* https://github.com/facebookresearch/llama\n",
    "\n",
    "## å°ç¥æ¦œ  IDEA æ²ˆå‘æ´‹ Ziya-LLaMA-7B-Reward\n",
    "* https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-7B-Reward\n",
    "* è‡ªæ ‡æ³¨é«˜è´¨é‡åå¥½æ’åºæ•°æ®40190æ¡ã€‚ä¸¥æ ¼è¿‡æ»¤çš„å¤–éƒ¨å¼€æºæ•°æ®3600æ¡ï¼Œæ¥æºåŒ…æ‹¬ï¼šOpenAssistant Conversations Dataset (OASST1)ã€Anthropic HH-RLHFã€GPT-4-LLMå’Œwebgpt_comparisionsã€‚æ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿä¸­è‹±åŒè¯­ç”Ÿæˆçš„å¥–åŠ±ç¯å¢ƒï¼Œå¯¹LLMç”Ÿæˆç»“æœæä¾›å‡†ç¡®çš„å¥–åŠ±åé¦ˆã€‚\n",
    "\n",
    "### Huatuo-Llama-Med-Chinese\n",
    "* https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese\n",
    "* \n",
    "\n",
    "## MOSS\n",
    "\n",
    "## ChatGLM\n",
    "* https://github.com/THUDM/ChatGLM-6B\n",
    "* An Open Bilingual Dialogue Language Model | å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹\n",
    "\n",
    "### è¡ç”Ÿé¡¹ç›®\n",
    "* https://github.com/yanqiangmiffy/Chinese-LangChain\n",
    "* https://github.com/imClumsyPanda/langchain-ChatGLM\n",
    "* https://github.com/l15y/wenda\n",
    "* https://github.com/SCIR-HI/Med-ChatGLM\n",
    "* æ›´å¤šï¼šhttps://www.toutiao.com/article/7225991356471312948/?log_from=57e0e99656ef9_1683088485260\n",
    "\n",
    "## xmtf\n",
    "* Crosslingual Generalization through Multitask Finetuning\n",
    "* xP3 æ˜¯ 46 ç§è¯­è¨€çš„æœ‰ç›‘ç£æ•°æ®é›†ï¼Œå¸¦æœ‰è‹±è¯­å’Œæœºå™¨ç¿»è¯‘çš„ prompts\n",
    "\n",
    "## stanford_alpaca\n",
    "* https://github.com/tatsu-lab/stanford_alpaca\n",
    "  * it was finetune on LLama 7B (affordable)\n",
    "* https://crfm.stanford.edu/2023/03/13/alpaca.html\n",
    "  * how the finetuning data is collected, and the evaluation results.\n",
    "\n",
    "## Baize\n",
    "* https://github.com/project-baize/baize\n",
    "* paper: https://arxiv.org/pdf/2304.01196.pdf\n",
    "* demo: https://huggingface.co/spaces/project-baize/Baize-7B\n",
    "* propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself.\n",
    "* based on LLaMA\n",
    "\n",
    "## [Cabrita](https://github.com/22-hours/cabrita)  ***** \n",
    "* try locally: https://github.com/22-hours/cabrita/blob/main/notebooks/cabrita-lora.ipynb\n",
    "* A portuguese finetuned instruction LLaMA. \n",
    "* just translated the alpaca_data.json to portuguese using ChatGPT, and then finetune.\n",
    "\n",
    "## Chinese-Vicuna\n",
    "* A Chinese Instruction-following LLaMA-based Model. ä¸€ä¸ªä¸­æ–‡ä½èµ„æºçš„llama+loraæ–¹æ¡ˆï¼Œç»“æ„å‚è€ƒalpaca.\n",
    "* https://github.com/Facico/Chinese-Vicuna\n",
    "\n",
    "## GPT4-x-Alpaca\n",
    "* GPT4-x-Alpaca is a LLaMA 13B model fine-tuned with a collection of GPT4 conversations, GPTeacher. Thereâ€™s not a lot of information on its training and performance.\n",
    "* https://huggingface.co/chavinlo/gpt4-x-alpaca\n",
    "\n",
    "## GPT4All\n",
    "* Demo, data and code to train an assistant-style large language model with ~800k GPT-3.5-Turbo Generations based on LLaMa.\n",
    "* https://github.com/nomic-ai/gpt4all\n",
    "\n",
    "## GPTQ-for-LLaMA\n",
    "* 4 bits quantization of LLaMA using GPTQ. GPTQ is SOTA one-shot weight quantization method.\n",
    "* https://github.com/qwopqwop200/GPTQ-for-LLaMa\n",
    "\n",
    "## Koala\n",
    "* Koala is a language model fine-tuned on top of LLaMA\n",
    "* Koala: A Dialogue Model for Academic Research\n",
    "* demo: https://chat.lmsys.org/?model=koala-13b\n",
    "* https://github.com/young-geng/EasyLM/blob/main/docs/koala.md\n",
    "\n",
    "## Pygmalion-7b\n",
    "* https://huggingface.co/PygmalionAI/pygmalion-7b\n",
    "* a dialogue model based on Metaâ€™s LLaMA-7B. This is version 1. It has been fine-tuned using a subset of the data from Pygmalion-6B-v8-pt4, for those of you familiar with the project.\n",
    "\n",
    "## [Vicuna (FastChat)](https://github.com/lm-sys/FastChat)  ***** \n",
    "* An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality.\n",
    "* including finetune, serving with web gui, api, evaluation\n",
    "* released FastChat-T5, supported models: lmsys/fastchat-t5-3b-v1.0\n",
    "BlinkDL/RWKV-4-Raven\n",
    "databricks/dolly-v2-12b\n",
    "OpenAssistant/oasst-sft-1-pythia-12b\n",
    "project-baize/baize-lora-7B\n",
    "StabilityAI/stablelm-tuned-alpha-7b\n",
    "THUDM/chatglm-6b\n",
    "* Vicuna-7B: needs around 30 GB of CPU RAM, Vicuna-13B needs 60 GB of CPU RAM. requires around 28GB of GPU memory for Vicuna-13B and 14GB of GPU memory for Vicuna-7B. Vicuna-7B can run on a 32GB M1 Macbook with 1 - 2 words / second.\n",
    "* https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/huggingface_api.py\n",
    "* blog introduction: https://lmsys.org/blog/2023-03-30-vicuna/\n",
    "* online-demo: https://chat.lmsys.org/\n",
    "* model comparison online: https://chat.lmsys.org/?arena\n",
    "\n",
    "## BLOOM (BigScience) ****\n",
    "* BigScience Large Open-science Open-access Multilingual Language Model.\n",
    "* https://huggingface.co/bigscience/bloom\n",
    "* demo: https://huggingface.co/spaces/huggingface/bloom_demo\n",
    "\n",
    "\n",
    "## More\n",
    "* https://medium.com/geekculture/list-of-open-sourced-fine-tuned-large-language-models-llm-8d95a2e0dc76\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tools\n",
    "\n",
    "## ChatGPT-Next-Web\n",
    "\n",
    "* https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web 58.5k, 2024-01-19\n",
    "* \n",
    "\n",
    "## ChatLM-mini-Chinese\n",
    "\n",
    "* T5 åŸºç¡€ä¸Šå¾—åˆ°ï¼Œæä¾›æ•°æ®ï¼Œè®­ç»ƒï¼Œå¾®è°ƒæ•´å¥—\n",
    "* 0.2B å°æ¨¡å‹ã€‚\n",
    "* å¦å¤–ä¸€ä¸ªå°æ¨¡å‹ï¼š https://github.com/charent/Phi2-mini-Chinese \n",
    "\n",
    "## Dify\n",
    "\n",
    "## ragflow\n",
    "\n",
    "* ä¸»æ‰“rag æ™ºèƒ½æ–‡æ¡£å¤„ç†åŠå…¶å¯è§†åŒ–å’Œå¯è§£é‡Šæ€§\n",
    "\n",
    "## embedchain\n",
    "\n",
    "* https://docs.embedchain.ai/components/evaluation\n",
    "* ä¸»æ‰“ç®€å•ï¼ŒåŠŸèƒ½ä¸°å¯Œã€‚æ”¯æŒmysql\n",
    "\n",
    "## GoMate\n",
    "\n",
    "* https://github.com/gomate-community/GoMate\n",
    "* å¯é çš„è¾“å…¥ï¼Œå¯ä¿¡çš„è¾“å‡º: ä½†æ˜¯æ€ä¹ˆä½“ç°å¯ä¿¡ï¼Œå¯é çš„ï¼Ÿ\n",
    "* 231 stars.\n",
    "\n",
    "## QAnything\n",
    "\n",
    "* https://github.com/netease-youdao/QAnything\n",
    "* Netease Youdao's open-source embedding and reranker models for RAG products.\n",
    "* [BCEmbedding: Bilingual and Crosslingual Embedding for RAG](https://github.com/netease-youdao/BCEmbedding)\n",
    "\n",
    "## Langchain-Chatchat\n",
    "\n",
    "* https://github.com/chatchat-space/Langchain-Chatchat\n",
    "  * 20.6k stars\n",
    "  * æ”¯æŒopenai, æœ¬åœ°æ¨¡å‹[FastChatæ”¯æŒ]\n",
    "\n",
    "## anything-llm\n",
    "\n",
    "* https://github.com/Mintplex-Labs/anything-llm   14.6k stars\n",
    "* The all-in-one Desktop & Docker AI application with full RAG and AI Agent capabilities.\n",
    "\n",
    "## phidata\n",
    "\n",
    "* function calling is all you need.\n",
    "* https://github.com/phidatahq/phidata\n",
    "\n",
    "## FastChat 29.2k 23-11-16\n",
    "\n",
    "* https://github.com/BlinkDL/FastChat\n",
    "* æ”¯æŒå¤šç§å¤§æ¨¡å‹ï¼ŒåŒ…æ‹¬ChatGLM, QWen, æ”¯æŒfinetuneï¼Œæ”¯æŒservingï¼ˆä¸openaiä¸€æ ·çš„apiï¼‰\n",
    "* [Docs](https://projects.laion.ai/Open-Assistant/docs/intro)\n",
    "* [Supported Models](https://github.com/lm-sys/FastChat/blob/main/docs/model_support.md)\n",
    "\n",
    "## FastGPT\n",
    "\n",
    "* FastGPT is a knowledge-based platform built on the LLM, offers out-of-the-box data processing and model invocation capabilities, allows for workflow orchestration through Flow visualization!\n",
    "\n",
    "\n",
    "## AutoGen\n",
    "\n",
    "## langchain\n",
    "* https://github.com/hwchase17/langchain\n",
    "  * https://github.com/hwchase17/chat-langchain \n",
    "    * running example: This repo is an implementation of a locally hosted chatbot specifically focused on question answering over the LangChain documentation. Built with LangChain and FastAPI.\n",
    "  * https://huggingface.co/spaces/JavaFXpert/Chat-GPT-LangChain\n",
    "* Building applications with LLMs through composability\n",
    "* do the dirty work when building apps with LLMs.\n",
    "\n",
    "## Lamda-Index\n",
    "* https://github.com/huggingface/lamda-index\n",
    "* Lamda-Index is a tool to index and search large language models. It is designed to be used with the Hugging Face Hub.\n",
    "* https://huggingface.co/spaces/huggingface/lamda-index\n",
    "\n",
    "## open assistant 35.7k stars 23-11-16\n",
    "* https://github.com/LAION-AI/Open-Assistant\n",
    "* have several plugins: web retrieval plugin.\n",
    "\n",
    "## Luotuo-Chinese-LLM\n",
    "\n",
    "## RWKV\n",
    "* https://github.com/BlinkDL/RWKV-LM\n",
    "\n",
    "## FlagOpen\n",
    "\n",
    "* https://github.com/FlagOpen\n",
    "* 2022å¹´2æœˆ28æ—¥ï¼Œæ™ºæºç ”ç©¶é™¢æœ€æ–°å‘å¸ƒäº†FlagOpené£æ™ºå¤§æ¨¡å‹æŠ€æœ¯å¼€æºä½“ç³»\n",
    "* åŒ…æ‹¬æ•°æ®æ ‡æ³¨éƒ¨åˆ†ã€‚FlagData\n",
    "\n",
    "## OpenChatKit\n",
    "* https://github.com/togethercomputer/OpenChatKit\n",
    "* provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. The kit includes an instruction-tuned language models, a moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories. OpenChatKit models were trained on the OIG-43M training dataset, which was a collaboration between Together, LAION, and Ontocord.ai.\n",
    "* Augmenting the model with additional context from a retrieval index\n",
    "\n",
    "## ChatRWKV\n",
    "\n",
    "## xTuring\n",
    "* https://xturing.stochastic.ai/finetune/guide/\n",
    "* Easily build, customize and control your own LLMs\n",
    "\n",
    "## MLC LLM\n",
    "* Enable everyone to develop, optimize and deploy AI models natively on everyone's devices.\n",
    "* https://github.com/mlc-ai/mlc-llm\n",
    "\n",
    "\n",
    "## others\n",
    "\n",
    "### awesome\n",
    "\n",
    "* https://github.com/nichtdax/awesome-totally-open-chatgpt\n",
    "* [IDPChat](https://github.com/BaihaiAI/IDPChat)    \n",
    "  * IDPChatæ˜¯å¼€æ”¾çš„ä¸­æ–‡å¤šæ¨¡æ€æ¨¡å‹ï¼ŒåŸºäºé¢„è®­ç»ƒå¤§é¢„è¨€æ¨¡å‹ LLaMA-13B å’Œå¼€æºæ–‡ç”Ÿå›¾é¢„è®­ç»ƒæ¨¡å‹Stable Diffusionæ„å»ºã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Tools\n",
    "\n",
    "## txtai\n",
    "\n",
    "* https://github.com/neuml/txtai\n",
    "* All-in-one open-source embeddings database for semantic search, LLM orchestration and language model workflows\n",
    "\n",
    "## Question: is there any reason to use txtai instead of llama_index?\n",
    "\n",
    "## FlagEmbedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API tools via llms\n",
    "\n",
    "## Gorilla\n",
    "\n",
    "* https://github.com/ShishirPatil/gorilla\n",
    "* Gorilla: An API store for LLMs\n",
    "* ä½¿ç”¨external apis\n",
    "\n",
    "\n",
    "## One-api\n",
    "\n",
    "* https://github.com/songquanpeng/one-api\n",
    "* OpenAI æ¥å£ç®¡ç† & åˆ†å‘ç³»ç»Ÿï¼Œæ”¯æŒ Azureã€Anthropic Claudeã€Google PaLM 2 & Geminiã€æ™ºè°± ChatGLMã€ç™¾åº¦æ–‡å¿ƒä¸€è¨€ã€è®¯é£æ˜Ÿç«è®¤çŸ¥ã€é˜¿é‡Œé€šä¹‰åƒé—®ã€360 æ™ºè„‘ä»¥åŠè…¾è®¯æ··å…ƒï¼Œ\n",
    "\n",
    "\n",
    "## serving API\n",
    "\n",
    "\n",
    "### api-for-open-llm\n",
    "\n",
    "* https://github.com/xusenlinzy/api-for-open-llm/tree/master\n",
    "* æ”¯æŒç»å¤§å¤šæ•°local llmsï¼šLLaMA, LLaMA-2, BLOOM, Falcon, Baichuan, Qwen, Xverse, SqlCoder, CodeLLaMA, ChatGLM, ChatGLM2, ChatGLM3\n",
    "\n",
    "### litellm \n",
    "\n",
    "* https://docs.litellm.ai/docs/proxy/quick_start\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function calling : Agent tools\n",
    "\n",
    "## Byzer-LLM\n",
    "\n",
    "* tutorial: https://mp.weixin.qq.com/s/GTVCYUhR_atYMX9ymp0eCg\n",
    "* https://mp.weixin.qq.com/s?__biz=MzIyNzQyNzgxNQ==&mid=2247484107&idx=1&sn=7966ce057bc495bb55d583f39c9f11a5&chksm=e8601643df179f55291794dddd9f98b992aff2c85f51d04eb86ae76d8f1c38f9f4abde5e1a96&scene=132&exptype=timeline_recommend_article_extendread_samebiz#wechat_redirect\n",
    "* https://github.com/allwefantasy/byzer-agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-processing tools for llm\n",
    "\n",
    "## PyMuPDF4LLM\n",
    "\n",
    "* https://artifex.com/blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf\n",
    "\n",
    "## Stirling-PDF\n",
    "\n",
    "* https://github.com/Stirling-Tools/Stirling-PDF\n",
    "* å¯¹PDFæ–‡ä»¶è¿›è¡Œå„ç§æ“ä½œï¼ŒåŒ…æ‹¬æ‹†åˆ†ã€åˆå¹¶ã€è½¬æ¢ã€é‡æ–°ç»„ç»‡ã€æ·»åŠ å›¾åƒã€æ—‹è½¬ã€å‹ç¼©ç­‰.\n",
    "* å¯ä»¥ç›´æ¥éƒ¨ç½²ã€‚\n",
    "\n",
    "## pdfminer\n",
    "\n",
    "* https://github.com/euske/pdfminer\n",
    "* ä¸“æ³¨äºä» PDF æ–‡ä»¶ä¸­æå–æ–‡æœ¬å’Œä¿¡æ¯ https://mp.weixin.qq.com/s/W01r8BhHWoQsSBQqfan-cQ\n",
    "\n",
    "## omniparse\n",
    "\n",
    "* https://github.com/adithya-s-k/omniparse\n",
    "* æ˜“ç”¨ï¼Œç›´æ¥è¾“å‡ºxml ç»“æ„åŒ–æ•°æ®ã€‚\n",
    "\n",
    "## PDF-Extract-Kit\n",
    "\n",
    "* https://github.com/opendatalab/PDF-Extract-Kit\n",
    "* é«˜è´¨é‡PDFå†…å®¹æå–çš„ç»¼åˆå·¥å…·åŒ…ï¼Œé›†æˆå¤šç§æ¨¡å‹å®ç°PDFå¸ƒå±€æ£€æµ‹ã€å…¬å¼æ£€æµ‹ä¸è¯†åˆ«ã€å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼Œé€‚ç”¨äºå­¦æœ¯è®ºæ–‡ã€æ•™ç§‘ä¹¦ã€ç ”ç©¶æŠ¥å‘Šå’Œè´¢åŠ¡æŠ¥è¡¨ç­‰å¤šç§æ–‡æ¡£ç±»å‹ï¼Œå³ä½¿åœ¨æ‰«ææ¨¡ç³Šæˆ–æœ‰æ°´å°çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¿æŒé«˜é²æ£’æ€§ã€‚\n",
    "\n",
    "## MinerU\n",
    "\n",
    "* https://github.com/opendatalab/MinerU\n",
    "* A one-stop, open-source, high-quality data extraction tool, supports PDF/webpage/e-book extraction.ä¸€ç«™å¼å¼€æºé«˜è´¨é‡æ•°æ®æå–å·¥å…·ï¼Œæ”¯æŒPDF/ç½‘é¡µ/å¤šæ ¼å¼ç”µå­ä¹¦æå–ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM TOD tools\n",
    "\n",
    "## OpsPilot\n",
    "\n",
    "* https://github.com/WeOps-Lab/OpsPilot\n",
    "* OpsPilotæ˜¯ä¸€ä¸ªåŸºäºRasaå’ŒLLMæŠ€æœ¯çš„ChatBotï¼Œä¸ºè¿ç»´ç³»ç»Ÿæä¾›ChatOps/LMOpsçš„èƒ½åŠ›\n",
    "\n",
    "## RasaGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "\n",
    "## mem0\n",
    "\n",
    "* https://github.com/mem0ai/mem0 åˆ†å…è´¹ç‰ˆå’Œä¼ä¸šç‰ˆ\n",
    "* The memory layer for Personalized AI\n",
    "* Core features:\n",
    "    * Multi-Level Memory: User, Session, and AI Agent memory retention\n",
    "    * Adaptive Personalization: Continuous improvement based on interactions\n",
    "    * Developer-Friendly API: Simple integration into various applications\n",
    "    * Cross-Platform Consistency: Uniform behavior across devices\n",
    "    * Managed Service: Hassle-free hosted solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text2video å¼€æº\n",
    "\n",
    "## text embedding models\n",
    "* Taiyi-CLIP-Roberta-102M-Chinese: IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese. ä¸­æ–‡text2embedding æ¨¡å‹ï¼Œå¯ä»¥è·Ÿ\"openai/clip-vit-base-patch32\"ç›¸åº”è®¡ç®—ã€‚\n",
    "  * https://huggingface.co/IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q transformers accelerate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigscience/bloomz-7b1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "inputs = tokenizer.encode(\"Translate to English: Je tâ€™aime.\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetune and train\n",
    "\n",
    "## Llama Factory\n",
    "\n",
    "* Unify Efficient Fine-Tuning of 100+ LLMs, 17.2k stars\n",
    "* https://github.com/hiyouga/LLaMA-Factory\n",
    "* ä½¿ç”¨ Web UI è¿›è¡Œç›‘ç£å¾®è°ƒ\n",
    "\n",
    "## [PEFT](https://huggingface.co/docs/peft/index) \n",
    "* Parameter-Efficient Fine-Tuning (PEFT)\n",
    "* supported models\n",
    "  * [LoRA](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "  * [Prefix Tuning](https://aclanthology.org/2021.acl-long.353/)\n",
    "  * [P-Tuning](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "  * [Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "  * [AdaLoRA](https://arxiv.org/abs/2303.10512)\n",
    "  * [LLaMA-Adapter](https://github.com/ZrrSkywalker/LLaMA-Adapter)\n",
    "\n",
    "\n",
    "## [deepspeech](https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload) \n",
    "* designed for speed and scale for distributed training of large models with billions of parameters\n",
    "* coupled with PEFT\n",
    "* At its core is the Zero Redundancy Optimizer (ZeRO) that shards optimizer states (ZeRO-1), gradients (ZeRO-2), and parameters (ZeRO-3) across data parallel processes.\n",
    "* ZeRO-Offload reduces GPU compute and memory by leveraging CPU resources during optimization.\n",
    "\n",
    "## LLMTune\n",
    "* 48Gæ˜¾å­˜çš„æ˜¾å¡ä¸Šå¾®è°ƒ4bitçš„650äº¿å‚æ•°çš„LLaMAæ¨¡å‹\n",
    "\n",
    "## Training Tools\n",
    "\n",
    "### Firefly\n",
    "\n",
    "* Firefly: ä¸€ç«™å¼å¤§æ¨¡å‹è®­ç»ƒå·¥å…· https://github.com/yangjianxin1/Firefly\n",
    "* æ”¯æŒå¯¹ä¸»æµçš„å¤§æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒå’ŒDPOï¼ŒåŒ…æ‹¬ä½†ä¸é™äºGemmaã€Qwen1.5ã€MiniCPMã€Llamaã€InternLMã€Baichuanã€ChatGLMã€Yiã€Deepseekã€Qwenã€Orionã€Ziyaã€Xverseã€Mistralã€Mixtral-8x7Bã€Zephyrã€Vicunaã€Bloomç­‰ã€‚ æœ¬é¡¹ç›®æ”¯æŒå…¨é‡å‚æ•°è®­ç»ƒã€LoRAã€QLoRAé«˜æ•ˆè®­ç»ƒï¼Œæ”¯æŒé¢„è®­ç»ƒã€SFTã€DPOã€‚\n",
    "* æ•´ç†å¹¶å¼€æºæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¤§æ¨¡å‹æœªæ¥æ–¹å‘\n",
    "\n",
    "* æ›´é•¿çš„context\n",
    "* æœ¬åœ°åŒ–éƒ¨ç½²ï¼Œä¸€èˆ¬GPUå¦‚3090ä¸Šè¿è¡Œ\n",
    "* è”ç½‘ï¼Œä¸æœ¬åœ°åŒ–çŸ¥è¯†åº“ç›¸ç»“åˆ\n",
    "* æ›´å¿«çš„æ¨ç†é€Ÿåº¦\n",
    "* prompt tuning for applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.29.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25b04a004a14659905edfbff416084d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb5d1ae56ae4530aa0afa07b80325a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e283a73526814870b46a0f29b22c5fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)l-00001-of-00008.bin:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccad1d5c99414efeb40ba05a74845dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)l-00002-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab07900c3d445819783788a5e93930e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)l-00003-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0bc799259e4e09b748d621618347af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)l-00004-of-00008.bin:   0%|          | 0.00/1.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c889c2d622424feeb38bdeffa73dd564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)l-00005-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cae8c46f8704a23aebcdfc9e0958ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)l-00006-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20de7999f526495ba031c7bff8bdde52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)l-00007-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6eaa8c636ab409a8c58a2f070cf3272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)l-00008-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4125fdde9c4dc784bccaf3a098146c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n",
      "æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©äººæ„Ÿåˆ°ä¸å®‰å’Œç„¦è™‘ï¼Œä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½æœ‰ç”¨çš„æŠ€å·§ï¼Œå¸®åŠ©æ›´å¥½åœ°å…¥ç¡ï¼š\n",
      "\n",
      "1. ä¿æŒæ”¾æ¾ï¼šè¯•ç€è¿›è¡Œæ·±å‘¼å¸ã€æ¸è¿›æ€§è‚Œè‚‰æ”¾æ¾æˆ–å†¥æƒ³ç­‰æ”¾æ¾æŠ€å·§ï¼Œå‡è½»èº«ä½“å’Œå¿ƒç†ä¸Šçš„å‹åŠ›ã€‚\n",
      "\n",
      "2. åˆ›é€ èˆ’é€‚çš„ç¡çœ ç¯å¢ƒï¼šä¿æŒæˆ¿é—´å®‰é™ã€å‡‰çˆ½ã€é»‘æš—å’Œèˆ’é€‚ï¼Œç¡®ä¿åºŠå«å’Œæ•å¤´æŸ”è½¯èˆ’é€‚ã€‚\n",
      "\n",
      "3. é¿å…åˆºæ¿€æ€§ç‰©è´¨ï¼šé¿å…é¥®ç”¨å«æœ‰å’–å•¡å› ã€é…’ç²¾æˆ–å°¼å¤ä¸çš„é¥®æ–™ï¼Œä»¥åŠåƒè¿‡å¤šæ²¹è…»æˆ–åˆºæ¿€æ€§çš„é£Ÿç‰©ã€‚\n",
      "\n",
      "4. å»ºç«‹è§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨ï¼šå°½å¯èƒ½æ¯å¤©åœ¨ç›¸åŒçš„æ—¶é—´ä¸ŠåºŠç¡è§‰ï¼Œå¹¶å°è¯•ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨ï¼Œå¸®åŠ©èº«ä½“é€‚åº”å’Œè°ƒæ•´ç”Ÿç‰©é’Ÿã€‚\n",
      "\n",
      "5. è¿›è¡Œè½»æ¾çš„æ´»åŠ¨ï¼šåœ¨ç¡è§‰å‰åšäº›è½»æ¾çš„æ´»åŠ¨ï¼Œå¦‚è¯»ä¹¦ã€æ´—æ¾¡ã€å¬è½»æŸ”çš„éŸ³ä¹ç­‰ï¼Œæœ‰åŠ©äºæ”¾æ¾èº«å¿ƒã€‚\n",
      "\n",
      "6. å°è¯•ä¸€äº›æ”¾æ¾æŠ€å·§ï¼šå¦‚æœæ— æ³•å…¥ç¡ï¼Œå¯ä»¥å°è¯•ä¸€äº›æ”¾æ¾æŠ€å·§ï¼Œå¦‚å†¥æƒ³ã€æ¸è¿›æ€§è‚Œè‚‰æ”¾æ¾ã€æ·±å‘¼å¸ç­‰ã€‚\n",
      "\n",
      "å¦‚æœè¿™äº›æ–¹æ³•ä¸èƒ½å¸®åŠ©å…¥ç¡ï¼Œä½ å¯èƒ½éœ€è¦å’¨è¯¢åŒ»ç”Ÿæˆ–å¿ƒç†åŒ»ç”Ÿï¼Œä»¥è·å¾—æ›´æ·±å…¥çš„å»ºè®®å’Œæ²»ç–—ã€‚\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import transformers\n",
    "\n",
    "print(transformers.__version__)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()\n",
    "response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\n",
    "print(response)\n",
    "response, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åœ¨åŒ—äº¬å†¬å¥¥ä¼šè‡ªç”±å¼æ»‘é›ªå¥³å­å¡é¢éšœç¢æŠ€å·§å†³èµ›ä¸­ï¼Œä¸­å›½é€‰æ‰‹è°·çˆ±å‡Œå¤ºå¾—é“¶ç‰Œã€‚åœ¨å†³èµ›ä¸­ï¼Œè°·çˆ±å‡Œè¡¨ç°ä¼˜å¼‚ï¼Œè·å¾—69.90åˆ†ï¼Œåœ¨12ä½é€‰æ‰‹ä¸­æ’åç¬¬ä¸‰ã€‚ä½†åœ¨ç¬¬äºŒè½®ä¸­ï¼Œè°·çˆ±å‡Œåœ¨é“å…·åŒºç¬¬ä¸‰ä¸ªéšœç¢å¤„å¤±è¯¯ï¼Œè·å¾—16.98åˆ†ã€‚æ®å¾®ä¿¡å…¬ä¼—å·â€œç•Œé¢â€æŠ¥é“ï¼Œä¸­å›½å‘æ”¹å§”åå„æ–­è°ƒæŸ¥å°ç»„çªå‡»æŸ¥è®¿å¥”é©°ä¸Šæµ·åŠäº‹å¤„ï¼Œè°ƒå–æ•°æ®ææ–™ï¼Œå¹¶å¯¹å¤šåå¥”é©°é«˜ç®¡è¿›è¡Œäº†çº¦è°ˆã€‚\n"
     ]
    }
   ],
   "source": [
    "chunk = \"\"\"æ‘˜è¦ï¼š\n",
    "è´¢è”ç¤¾5æœˆ22æ—¥è®¯ï¼Œæ®å¹³å®‰åŒ…å¤´å¾®ä¿¡å…¬ä¼—å·æ¶ˆæ¯ï¼Œè¿‘æ—¥ï¼ŒåŒ…å¤´è­¦æ–¹å‘å¸ƒä¸€èµ·åˆ©ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å®æ–½ç”µä¿¡è¯ˆéª—çš„å…¸å‹æ¡ˆä¾‹ï¼Œç¦å·å¸‚æŸç§‘æŠ€å…¬å¸æ³•äººä»£è¡¨éƒ­å…ˆç”Ÿ10åˆ†é’Ÿå†…è¢«éª—430ä¸‡å…ƒã€‚4æœˆ20æ—¥ä¸­åˆï¼Œéƒ­å…ˆç”Ÿçš„å¥½å‹çªç„¶é€šè¿‡å¾®ä¿¡è§†é¢‘è”ç³»ä»–ï¼Œè‡ªå·±çš„æœ‹å‹åœ¨å¤–åœ°ç«æ ‡ï¼Œéœ€è¦430ä¸‡ä¿è¯é‡‘ï¼Œä¸”éœ€è¦å…¬å¯¹å…¬è´¦æˆ·è¿‡è´¦ï¼Œæƒ³è¦å€Ÿéƒ­å…ˆç”Ÿå…¬å¸çš„è´¦æˆ·èµ°è´¦ã€‚åŸºäºå¯¹å¥½å‹çš„ä¿¡ä»»ï¼ŒåŠ ä¸Šå·²ç»è§†é¢‘èŠå¤©æ ¸å®äº†èº«ä»½ï¼Œéƒ­å…ˆç”Ÿæ²¡æœ‰æ ¸å®é’±æ¬¾æ˜¯å¦åˆ°è´¦ï¼Œå°±åˆ†ä¸¤ç¬”æŠŠ430ä¸‡è½¬åˆ°äº†å¥½å‹æœ‹å‹çš„é“¶è¡Œå¡ä¸Šã€‚éƒ­å…ˆç”Ÿæ‹¨æ‰“å¥½å‹ç”µè¯ï¼Œæ‰çŸ¥é“è¢«éª—ã€‚éª—å­é€šè¿‡æ™ºèƒ½AIæ¢è„¸å’Œæ‹Ÿå£°æŠ€æœ¯ï¼Œä½¯è£…å¥½å‹å¯¹ä»–å®æ–½äº†è¯ˆéª—ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œéª—å­å¹¶æ²¡æœ‰ä½¿ç”¨ä¸€ä¸ªä»¿çœŸçš„å¥½å‹å¾®ä¿¡æ·»åŠ éƒ­å…ˆç”Ÿä¸ºå¥½å‹ï¼Œè€Œæ˜¯ç›´æ¥ç”¨å¥½å‹å¾®ä¿¡å‘èµ·è§†é¢‘èŠå¤©ï¼Œè¿™ä¹Ÿæ˜¯éƒ­å…ˆç”Ÿè¢«éª—çš„åŸå› ä¹‹ä¸€ã€‚éª—å­ææœ‰å¯èƒ½é€šè¿‡æŠ€æœ¯æ‰‹æ®µç›—ç”¨äº†éƒ­å…ˆç”Ÿå¥½å‹çš„å¾®ä¿¡ã€‚å¹¸è¿çš„æ˜¯ï¼Œæ¥åˆ°æŠ¥è­¦åï¼Œç¦å·ã€åŒ…å¤´ä¸¤åœ°è­¦é“¶è¿…é€Ÿå¯åŠ¨æ­¢ä»˜æœºåˆ¶ï¼ŒæˆåŠŸæ­¢ä»˜æ‹¦æˆª336.84ä¸‡å…ƒï¼Œä½†ä»æœ‰93.16ä¸‡å…ƒè¢«è½¬ç§»ï¼Œç›®å‰æ­£åœ¨å…¨åŠ›è¿½ç¼´ä¸­ã€‚\n",
    "\"\"\"\n",
    "\n",
    "chunk = \"æ‘˜è¦æˆ200å­—å·¦å³ï¼šåœ¨åŒ—äº¬å†¬å¥¥ä¼šè‡ªç”±å¼æ»‘é›ªå¥³å­å¡é¢éšœç¢æŠ€å·§å†³èµ›ä¸­ï¼Œä¸­å›½é€‰æ‰‹è°·çˆ±å‡Œå¤ºå¾—é“¶ç‰Œã€‚ç¥è´ºè°·çˆ±å‡Œï¼ä»Šå¤©ä¸Šåˆï¼Œè‡ªç”±å¼æ»‘é›ªå¥³å­å¡é¢éšœç¢æŠ€å·§å†³èµ›ä¸¾è¡Œã€‚å†³èµ›åˆ†ä¸‰è½®è¿›è¡Œï¼Œå–é€‰æ‰‹æœ€ä½³æˆç»©æ’åå†³å‡ºå¥–ç‰Œã€‚ç¬¬ä¸€è·³ï¼Œä¸­å›½é€‰æ‰‹è°·çˆ±å‡Œè·å¾—69.90åˆ†ã€‚åœ¨12ä½é€‰æ‰‹ä¸­æ’åç¬¬ä¸‰ã€‚å®ŒæˆåŠ¨ä½œåï¼Œè°·çˆ±å‡Œåˆæ‰®äº†ä¸ªé¬¼è„¸ï¼Œç”šæ˜¯å¯çˆ±ã€‚ç¬¬äºŒè½®ä¸­ï¼Œè°·çˆ±å‡Œåœ¨é“å…·åŒºç¬¬ä¸‰ä¸ªéšœç¢å¤„å¤±è¯¯ï¼Œè½åœ°æ—¶æ‘”å€’ã€‚è·å¾—16.98åˆ†ã€‚\"\n",
    "\n",
    "text = \"æ®å¾®ä¿¡å…¬ä¼—å·â€œç•Œé¢â€æŠ¥é“ï¼Œ4æ—¥ä¸Šåˆ10ç‚¹å·¦å³ï¼Œä¸­å›½å‘æ”¹å§”åå„æ–­è°ƒæŸ¥å°ç»„çªå‡»æŸ¥è®¿å¥”é©°ä¸Šæµ·åŠäº‹å¤„ï¼Œè°ƒå–æ•°æ®ææ–™ï¼Œå¹¶å¯¹å¤šåå¥”é©°é«˜ç®¡è¿›è¡Œäº†çº¦è°ˆã€‚æˆªæ­¢æ˜¨æ—¥æ™š9ç‚¹ï¼ŒåŒ…æ‹¬åŒ—äº¬æ¢…èµ›å¾·æ–¯-å¥”é©°é”€å”®æœåŠ¡æœ‰é™å…¬å¸ä¸œåŒºæ€»ç»ç†åœ¨å†…çš„å¤šåç®¡ç†äººå‘˜ä»ç•™åœ¨ä¸Šæµ·åŠå…¬å®¤å†…\"\n",
    "\n",
    "\n",
    "response, history = model.chat(tokenizer, chunk + text, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 159\n"
     ]
    }
   ],
   "source": [
    "print(len(chunk + text), len(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering proxy context http://192.168.1.45:10809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : C:\\Users\\73915\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int8\\22906aeb32fd7952ce323dc9d25e01693b270da6\\quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 C:\\Users\\73915\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int8\\22906aeb32fd7952ce323dc9d25e01693b270da6\\quantization_kernels_parallel.c -shared -o C:\\Users\\73915\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int8\\22906aeb32fd7952ce323dc9d25e01693b270da6\\quantization_kernels_parallel.so\n",
      "Compile default cpu kernel failed, using default cpu kernel code.\n",
      "Compiling gcc -O3 -fPIC -std=c99 C:\\Users\\73915\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int8\\22906aeb32fd7952ce323dc9d25e01693b270da6\\quantization_kernels.c -shared -o C:\\Users\\73915\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int8\\22906aeb32fd7952ce323dc9d25e01693b270da6\\quantization_kernels.so\n",
      "Compile default cpu kernel failed.\n",
      "Failed to load kernel.\n",
      "Cannot load cpu kernel, don't use quantized model on cpu.\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n",
      "exit proxy context http://192.168.1.45:10809\n",
      "<class 'transformers_modules.THUDM.chatglm-6b-int8.22906aeb32fd7952ce323dc9d25e01693b270da6.modeling_chatglm.ChatGLMForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from yelib.utils.network_util import ProxyContext\n",
    "\n",
    "with ProxyContext():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b-int8\", trust_remote_code=True)\n",
    "    model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int8\", trust_remote_code=True, local_files_only=True).half().cuda()\n",
    "print(type(model))\n",
    "# response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\n",
    "# print(response)\n",
    "# response, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history)\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¦å·ç§‘æŠ€å…¬å¸æ³•äººä»£è¡¨éƒ­å…ˆç”Ÿäº10åˆ†é’Ÿå†…è¢«éª—430ä¸‡å…ƒï¼Œéª—å­é€šè¿‡äººå·¥æ™ºèƒ½æŠ€æœ¯å®æ–½ç”µä¿¡è¯ˆéª—ã€‚æ­¤å¤–ï¼Œå¥”é©°å…¬å¸ä¹Ÿè¢«ä¸­å›½å‘æ”¹å§”åå„æ–­è°ƒæŸ¥å°ç»„çªå‡»æŸ¥è®¿ï¼Œå¹¶å¯¹å¤šåé«˜ç®¡è¿›è¡Œçº¦è°ˆã€‚\n"
     ]
    }
   ],
   "source": [
    "chunk = \"\"\"æ‘˜è¦ï¼š\n",
    "è´¢è”ç¤¾5æœˆ22æ—¥è®¯ï¼Œæ®å¹³å®‰åŒ…å¤´å¾®ä¿¡å…¬ä¼—å·æ¶ˆæ¯ï¼Œè¿‘æ—¥ï¼ŒåŒ…å¤´è­¦æ–¹å‘å¸ƒä¸€èµ·åˆ©ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å®æ–½ç”µä¿¡è¯ˆéª—çš„å…¸å‹æ¡ˆä¾‹ï¼Œç¦å·å¸‚æŸç§‘æŠ€å…¬å¸æ³•äººä»£è¡¨éƒ­å…ˆç”Ÿ10åˆ†é’Ÿå†…è¢«éª—430ä¸‡å…ƒã€‚4æœˆ20æ—¥ä¸­åˆï¼Œéƒ­å…ˆç”Ÿçš„å¥½å‹çªç„¶é€šè¿‡å¾®ä¿¡è§†é¢‘è”ç³»ä»–ï¼Œè‡ªå·±çš„æœ‹å‹åœ¨å¤–åœ°ç«æ ‡ï¼Œéœ€è¦430ä¸‡ä¿è¯é‡‘ï¼Œä¸”éœ€è¦å…¬å¯¹å…¬è´¦æˆ·è¿‡è´¦ï¼Œæƒ³è¦å€Ÿéƒ­å…ˆç”Ÿå…¬å¸çš„è´¦æˆ·èµ°è´¦ã€‚åŸºäºå¯¹å¥½å‹çš„ä¿¡ä»»ï¼ŒåŠ ä¸Šå·²ç»è§†é¢‘èŠå¤©æ ¸å®äº†èº«ä»½ï¼Œéƒ­å…ˆç”Ÿæ²¡æœ‰æ ¸å®é’±æ¬¾æ˜¯å¦åˆ°è´¦ï¼Œå°±åˆ†ä¸¤ç¬”æŠŠ430ä¸‡è½¬åˆ°äº†å¥½å‹æœ‹å‹çš„é“¶è¡Œå¡ä¸Šã€‚éƒ­å…ˆç”Ÿæ‹¨æ‰“å¥½å‹ç”µè¯ï¼Œæ‰çŸ¥é“è¢«éª—ã€‚éª—å­é€šè¿‡æ™ºèƒ½AIæ¢è„¸å’Œæ‹Ÿå£°æŠ€æœ¯ï¼Œä½¯è£…å¥½å‹å¯¹ä»–å®æ–½äº†è¯ˆéª—ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œéª—å­å¹¶æ²¡æœ‰ä½¿ç”¨ä¸€ä¸ªä»¿çœŸçš„å¥½å‹å¾®ä¿¡æ·»åŠ éƒ­å…ˆç”Ÿä¸ºå¥½å‹ï¼Œè€Œæ˜¯ç›´æ¥ç”¨å¥½å‹å¾®ä¿¡å‘èµ·è§†é¢‘èŠå¤©ï¼Œè¿™ä¹Ÿæ˜¯éƒ­å…ˆç”Ÿè¢«éª—çš„åŸå› ä¹‹ä¸€ã€‚éª—å­ææœ‰å¯èƒ½é€šè¿‡æŠ€æœ¯æ‰‹æ®µç›—ç”¨äº†éƒ­å…ˆç”Ÿå¥½å‹çš„å¾®ä¿¡ã€‚å¹¸è¿çš„æ˜¯ï¼Œæ¥åˆ°æŠ¥è­¦åï¼Œç¦å·ã€åŒ…å¤´ä¸¤åœ°è­¦é“¶è¿…é€Ÿå¯åŠ¨æ­¢ä»˜æœºåˆ¶ï¼ŒæˆåŠŸæ­¢ä»˜æ‹¦æˆª336.84ä¸‡å…ƒï¼Œä½†ä»æœ‰93.16ä¸‡å…ƒè¢«è½¬ç§»ï¼Œç›®å‰æ­£åœ¨å…¨åŠ›è¿½ç¼´ä¸­ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# chunk = \"æ‘˜è¦æˆ200å­—å·¦å³ï¼šåœ¨åŒ—äº¬å†¬å¥¥ä¼šè‡ªç”±å¼æ»‘é›ªå¥³å­å¡é¢éšœç¢æŠ€å·§å†³èµ›ä¸­ï¼Œä¸­å›½é€‰æ‰‹è°·çˆ±å‡Œå¤ºå¾—é“¶ç‰Œã€‚ç¥è´ºè°·çˆ±å‡Œï¼ä»Šå¤©ä¸Šåˆï¼Œè‡ªç”±å¼æ»‘é›ªå¥³å­å¡é¢éšœç¢æŠ€å·§å†³èµ›ä¸¾è¡Œã€‚å†³èµ›åˆ†ä¸‰è½®è¿›è¡Œï¼Œå–é€‰æ‰‹æœ€ä½³æˆç»©æ’åå†³å‡ºå¥–ç‰Œã€‚ç¬¬ä¸€è·³ï¼Œä¸­å›½é€‰æ‰‹è°·çˆ±å‡Œè·å¾—69.90åˆ†ã€‚åœ¨12ä½é€‰æ‰‹ä¸­æ’åç¬¬ä¸‰ã€‚å®ŒæˆåŠ¨ä½œåï¼Œè°·çˆ±å‡Œåˆæ‰®äº†ä¸ªé¬¼è„¸ï¼Œç”šæ˜¯å¯çˆ±ã€‚ç¬¬äºŒè½®ä¸­ï¼Œè°·çˆ±å‡Œåœ¨é“å…·åŒºç¬¬ä¸‰ä¸ªéšœç¢å¤„å¤±è¯¯ï¼Œè½åœ°æ—¶æ‘”å€’ã€‚è·å¾—16.98åˆ†ã€‚\"\n",
    "\n",
    "text = \"æ®å¾®ä¿¡å…¬ä¼—å·â€œç•Œé¢â€æŠ¥é“ï¼Œ4æ—¥ä¸Šåˆ10ç‚¹å·¦å³ï¼Œä¸­å›½å‘æ”¹å§”åå„æ–­è°ƒæŸ¥å°ç»„çªå‡»æŸ¥è®¿å¥”é©°ä¸Šæµ·åŠäº‹å¤„ï¼Œè°ƒå–æ•°æ®ææ–™ï¼Œå¹¶å¯¹å¤šåå¥”é©°é«˜ç®¡è¿›è¡Œäº†çº¦è°ˆã€‚æˆªæ­¢æ˜¨æ—¥æ™š9ç‚¹ï¼ŒåŒ…æ‹¬åŒ—äº¬æ¢…èµ›å¾·æ–¯-å¥”é©°é”€å”®æœåŠ¡æœ‰é™å…¬å¸ä¸œåŒºæ€»ç»ç†åœ¨å†…çš„å¤šåç®¡ç†äººå‘˜ä»ç•™åœ¨ä¸Šæµ·åŠå…¬å®¤å†…\"\n",
    "\n",
    "\n",
    "response, history = model.chat(tokenizer, chunk + text, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generation config docs\n",
    "\n",
    "* https://huggingface.co/docs/transformers/main/en/main_classes/text_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese-langchain\n",
    "\n",
    "## install\n",
    "\n",
    "* running build_ext error: [WinError 2] ç³»ç»Ÿæ‰¾ä¸åˆ°æŒ‡å®šçš„æ–‡ä»¶\n",
    "* segmental faulté—®é¢˜ï¼špip install boost cmakeï¼Œ å¯èƒ½æ˜¯cmakeæ²¡æœ‰æ­£ç¡®å®‰è£…ã€‚\n",
    "* Chinese-langchain \n",
    "\n",
    "\n",
    "### unstructured å®‰è£…ï¼šä¼šå¯¼è‡´ç³»ç»Ÿå¡æ­»\n",
    "* [Installation with conda on Windows](https://github.com/Unstructured-IO/unstructured/blob/main/docs/source/installing.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache\n",
    "\n",
    "## [CodeFuse-ModelCache](https://github.com/codefuse-ai/CodeFuse-ModelCache)\n",
    "\n",
    "## [GPTCache](https://github.com/zilliztech/GPTCache) 6k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å›½å†…åŠ é€Ÿä¸‹è½½\n",
    "\n",
    "* https://mp.weixin.qq.com/s/Fx6nfFt_RPwDHZ3V73PD1Q\n",
    "* git clone https://github.com/LetheSec/HuggingFace-Download-Accelerator.git\n",
    "* export HF_ENDPOINT=\"https://hf-mirror.com\"\n",
    "  * https://padeoe.com/huggingface-large-models-downloader/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval æ•°æ®\n",
    "\n",
    "## z-bench\n",
    "\n",
    "* https://github.com/zhenbench/z-bench?tab=readme-ov-file\n",
    "* åŒ…å«ï¼šåŸºç¡€èƒ½åŠ›, è¿›é˜¶èƒ½åŠ›, å‚ç›´èƒ½åŠ›"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaderboard\n",
    "\n",
    "* https://hf.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
    "* https://chat.lmsys.org/?leaderboard\n",
    "* https://hf.co/spaces/mteb/leaderboard\n",
    "* https://github.com/jeinlee1991/chinese-llm-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trouble shooting\n",
    "\n",
    "## bitsandbytes cuda windows\n",
    "\n",
    "### LLaMA_etuning\n",
    "\n",
    "* ä»¥ä¸‹æ— æ³•æˆåŠŸï¼Œä½†æ˜¯å”¯ä¸€æ‰¾åˆ°æ–¹æ³•\n",
    "  * https://github.com/hiyouga/LLaMA-Efficient-Tuning\n",
    "  * pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.39.1-py3-none-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "\n",
    "model_path = \"LinkSoul/Chinese-Llama-2-7b\"\n",
    "from yelib.utils.network_util import ProxyContext\n",
    "\n",
    "with ProxyContext():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path).half().cuda()\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True) # type: ignore\n",
    "\n",
    "instruction = \"\"\"[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "            If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n{} [/INST]\"\"\"\n",
    "\n",
    "prompt = instruction.format(\"ç”¨è‹±æ–‡å›ç­”ï¼Œä»€ä¹ˆæ˜¯å¤«å¦»è‚ºç‰‡ï¼Ÿ\")\n",
    "generate_ids = model.generate(tokenizer(prompt, return_tensors='pt').input_ids.cuda(), max_new_tokens=4096, streamer=streamer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"anything\",\n",
    "    base_url=\"http://0.0.0.0:4000\"\n",
    ")\n",
    "\n",
    "# request sent to model set on litellm proxy, `litellm --model`\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"this is a test request, write a short poem\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "HuggingfaceException - response is not in expected format - [{'detail': 'Method Not Allowed'}]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHuggingfaceError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\main.py:1418\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   1417\u001b[0m custom_prompt_dict \u001b[38;5;241m=\u001b[39m custom_prompt_dict \u001b[38;5;129;01mor\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mcustom_prompt_dict\n\u001b[1;32m-> 1418\u001b[0m model_response \u001b[38;5;241m=\u001b[39m \u001b[43mhuggingface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhuggingface_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1434\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1436\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m optional_params\n\u001b[0;32m   1437\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m optional_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1438\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m acompletion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1439\u001b[0m ):\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;66;03m# don't try to access stream object,\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\llms\\huggingface_restapi.py:550\u001b[0m, in \u001b[0;36mHuggingface.completion\u001b[1;34m(self, model, messages, api_base, headers, model_response, print_verbose, timeout, encoding, api_key, logging_obj, custom_prompt_dict, acompletion, optional_params, litellm_params, logger_fn)\u001b[0m\n\u001b[0;32m    549\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 550\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\llms\\huggingface_restapi.py:539\u001b[0m, in \u001b[0;36mHuggingface.completion\u001b[1;34m(self, model, messages, api_base, headers, model_response, print_verbose, timeout, encoding, api_key, logging_obj, custom_prompt_dict, acompletion, optional_params, litellm_params, logger_fn)\u001b[0m\n\u001b[0;32m    535\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m HuggingfaceError(\n\u001b[0;32m    536\u001b[0m                 message\u001b[38;5;241m=\u001b[39mcompletion_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    537\u001b[0m                 status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[0;32m    538\u001b[0m             )\n\u001b[1;32m--> 539\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_model_response_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompletion_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HuggingfaceError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\llms\\huggingface_restapi.py:224\u001b[0m, in \u001b[0;36mHuggingface.convert_to_model_response_object\u001b[1;34m(self, completion_response, model_response, task, optional_params, encoding, input_text, model)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(completion_response, \u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(completion_response[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m completion_response[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    223\u001b[0m ):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HuggingfaceError(\n\u001b[0;32m    225\u001b[0m         status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m422\u001b[39m,\n\u001b[0;32m    226\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse is not in expected format - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompletion_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m     )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(completion_response[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mHuggingfaceError\u001b[0m: response is not in expected format - [{'detail': 'Method Not Allowed'}]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_65992\\2598652386.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m response = litellm.completion(\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2792\u001b[0m                     if (\n\u001b[0;32m   2793\u001b[0m                         \u001b[0mliteDebuggerClient\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mliteDebuggerClient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdashboard_url\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2794\u001b[0m                     ):  # make it easy to get to the debugger logs if you've initialized it\n\u001b[0;32m   2795\u001b[0m                         \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf\"\\n Check the log in your dashboard - {liteDebuggerClient.dashboard_url}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2796\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2792\u001b[0m                     if (\n\u001b[0;32m   2793\u001b[0m                         \u001b[0mliteDebuggerClient\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mliteDebuggerClient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdashboard_url\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2794\u001b[0m                     ):  # make it easy to get to the debugger logs if you've initialized it\n\u001b[0;32m   2795\u001b[0m                         \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf\"\\n Check the log in your dashboard - {liteDebuggerClient.dashboard_url}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2796\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\main.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   2089\u001b[0m             )\n\u001b[0;32m   2090\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2091\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2092\u001b[0m         \u001b[1;31m## Map to OpenAI Exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2093\u001b[1;33m         raise exception_type(\n\u001b[0m\u001b[0;32m   2094\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2095\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2096\u001b[0m             \u001b[0moriginal_exception\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[0;32m   8281\u001b[0m         \u001b[1;31m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8283\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8284\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8285\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[0;32m   8281\u001b[0m         \u001b[1;31m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8283\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8284\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8285\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAPIError\u001b[0m: HuggingfaceException - response is not in expected format - [{'detail': 'Method Not Allowed'}]"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "import os\n",
    "\n",
    "response = litellm.completion(\n",
    "    model=\"huggingface/bigcode/starcoder\",               # add `openai/` prefix to model so litellm knows to route to OpenAI\n",
    "    api_key=\"sk-1234\",                  # api key to your openai compatible endpoint\n",
    "    api_base=\"http://192.168.1.45:4000\",     # set API Base of your Custom OpenAI Endpoint\n",
    "    messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Hey, how's it going?\",\n",
    "                }\n",
    "    ],\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
