{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awesome LLM\n",
    "* https://github.com/Hannibal046/Awesome-LLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open source models\n",
    "\n",
    "## Character\n",
    "\n",
    "### ChatPLUG\n",
    "\n",
    "* https://github.com/X-PLUG/ChatPLUG\n",
    "\n",
    "## 医疗大模型\n",
    "\n",
    "* [复旦大学团队发布中文医疗健康个人助手，同时开源 47 万高质量数据集](https://www.toutiao.com/w/1781243874145280/)\n",
    "  * DISC-MedLLM 是一个专为医疗健康对话场景而打造的领域大模型，它可以满足您的各种医疗保健需求，包括疾病问诊和治疗方案咨询等，为您提供高质量的健康支持服务。\n",
    "* [华佗大模型](https://modelscope.cn/models/MaingBen/HuatuoGPT-reward-model-7B/summary)\n",
    "* DoctorGPT\n",
    "* Apollo：一个开源轻量级多语言医疗大型语言模型（0.5B、1.8B、2B、6B和7B）\n",
    "  * 通过代理调优（Proxy Tuning）来提升大型模型的多语言医疗能力\n",
    "\n",
    "\n",
    "## gpt4-pdf-chatbot-langchain\n",
    "* https://github.com/mayooear/gpt4-pdf-chatbot-langchain\n",
    "* 针对 PDF 文件构建的 GPT 机器人，上传你的 PDF 文件，使用的技术堆栈包括 LangChain、Pinecone、Typescript、Openai 和 Next.js。基于 Open AI 和 LangChain，可以分析 PDF 文档中的文字和内容，通过 embedding API 生成向量，然后存储到数据库中. 最后做成类似于 ChatGPT 的机器人，通过机器人快速的进行查询、输出答案。\n",
    "\n",
    "##  LLama\n",
    "* paper: https://arxiv.org/pdf/2302.13971.pdf\n",
    "  * data Wikipedia [4.5%]: which use either the Latin or Cyrillic scripts: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk.\n",
    "* https://github.com/facebookresearch/llama\n",
    "\n",
    "## 封神榜  IDEA 沈向洋 Ziya-LLaMA-7B-Reward\n",
    "* https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-7B-Reward\n",
    "* 自标注高质量偏好排序数据40190条。严格过滤的外部开源数据3600条，来源包括：OpenAssistant Conversations Dataset (OASST1)、Anthropic HH-RLHF、GPT-4-LLM和webgpt_comparisions。模型能够模拟中英双语生成的奖励环境，对LLM生成结果提供准确的奖励反馈。\n",
    "\n",
    "### Huatuo-Llama-Med-Chinese\n",
    "* https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese\n",
    "* \n",
    "\n",
    "## MOSS\n",
    "\n",
    "## ChatGLM\n",
    "* https://github.com/THUDM/ChatGLM-6B\n",
    "* An Open Bilingual Dialogue Language Model | 开源双语对话语言模型\n",
    "\n",
    "### 衍生项目\n",
    "* https://github.com/yanqiangmiffy/Chinese-LangChain\n",
    "* https://github.com/imClumsyPanda/langchain-ChatGLM\n",
    "* https://github.com/l15y/wenda\n",
    "* https://github.com/SCIR-HI/Med-ChatGLM\n",
    "* 更多：https://www.toutiao.com/article/7225991356471312948/?log_from=57e0e99656ef9_1683088485260\n",
    "\n",
    "## xmtf\n",
    "* Crosslingual Generalization through Multitask Finetuning\n",
    "* xP3 是 46 种语言的有监督数据集，带有英语和机器翻译的 prompts\n",
    "\n",
    "## stanford_alpaca\n",
    "* https://github.com/tatsu-lab/stanford_alpaca\n",
    "  * it was finetune on LLama 7B (affordable)\n",
    "* https://crfm.stanford.edu/2023/03/13/alpaca.html\n",
    "  * how the finetuning data is collected, and the evaluation results.\n",
    "\n",
    "## Baize\n",
    "* https://github.com/project-baize/baize\n",
    "* paper: https://arxiv.org/pdf/2304.01196.pdf\n",
    "* demo: https://huggingface.co/spaces/project-baize/Baize-7B\n",
    "* propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself.\n",
    "* based on LLaMA\n",
    "\n",
    "## [Cabrita](https://github.com/22-hours/cabrita)  ***** \n",
    "* try locally: https://github.com/22-hours/cabrita/blob/main/notebooks/cabrita-lora.ipynb\n",
    "* A portuguese finetuned instruction LLaMA. \n",
    "* just translated the alpaca_data.json to portuguese using ChatGPT, and then finetune.\n",
    "\n",
    "## Chinese-Vicuna\n",
    "* A Chinese Instruction-following LLaMA-based Model. 一个中文低资源的llama+lora方案，结构参考alpaca.\n",
    "* https://github.com/Facico/Chinese-Vicuna\n",
    "\n",
    "## GPT4-x-Alpaca\n",
    "* GPT4-x-Alpaca is a LLaMA 13B model fine-tuned with a collection of GPT4 conversations, GPTeacher. There’s not a lot of information on its training and performance.\n",
    "* https://huggingface.co/chavinlo/gpt4-x-alpaca\n",
    "\n",
    "## GPT4All\n",
    "* Demo, data and code to train an assistant-style large language model with ~800k GPT-3.5-Turbo Generations based on LLaMa.\n",
    "* https://github.com/nomic-ai/gpt4all\n",
    "\n",
    "## GPTQ-for-LLaMA\n",
    "* 4 bits quantization of LLaMA using GPTQ. GPTQ is SOTA one-shot weight quantization method.\n",
    "* https://github.com/qwopqwop200/GPTQ-for-LLaMa\n",
    "\n",
    "## Koala\n",
    "* Koala is a language model fine-tuned on top of LLaMA\n",
    "* Koala: A Dialogue Model for Academic Research\n",
    "* demo: https://chat.lmsys.org/?model=koala-13b\n",
    "* https://github.com/young-geng/EasyLM/blob/main/docs/koala.md\n",
    "\n",
    "## Pygmalion-7b\n",
    "* https://huggingface.co/PygmalionAI/pygmalion-7b\n",
    "* a dialogue model based on Meta’s LLaMA-7B. This is version 1. It has been fine-tuned using a subset of the data from Pygmalion-6B-v8-pt4, for those of you familiar with the project.\n",
    "\n",
    "## [Vicuna (FastChat)](https://github.com/lm-sys/FastChat)  ***** \n",
    "* An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality.\n",
    "* including finetune, serving with web gui, api, evaluation\n",
    "* released FastChat-T5, supported models: lmsys/fastchat-t5-3b-v1.0\n",
    "BlinkDL/RWKV-4-Raven\n",
    "databricks/dolly-v2-12b\n",
    "OpenAssistant/oasst-sft-1-pythia-12b\n",
    "project-baize/baize-lora-7B\n",
    "StabilityAI/stablelm-tuned-alpha-7b\n",
    "THUDM/chatglm-6b\n",
    "* Vicuna-7B: needs around 30 GB of CPU RAM, Vicuna-13B needs 60 GB of CPU RAM. requires around 28GB of GPU memory for Vicuna-13B and 14GB of GPU memory for Vicuna-7B. Vicuna-7B can run on a 32GB M1 Macbook with 1 - 2 words / second.\n",
    "* https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/huggingface_api.py\n",
    "* blog introduction: https://lmsys.org/blog/2023-03-30-vicuna/\n",
    "* online-demo: https://chat.lmsys.org/\n",
    "* model comparison online: https://chat.lmsys.org/?arena\n",
    "\n",
    "## BLOOM (BigScience) ****\n",
    "* BigScience Large Open-science Open-access Multilingual Language Model.\n",
    "* https://huggingface.co/bigscience/bloom\n",
    "* demo: https://huggingface.co/spaces/huggingface/bloom_demo\n",
    "\n",
    "\n",
    "## More\n",
    "* https://medium.com/geekculture/list-of-open-sourced-fine-tuned-large-language-models-llm-8d95a2e0dc76\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tools\n",
    "\n",
    "## ChatGPT-Next-Web\n",
    "\n",
    "* https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web 58.5k, 2024-01-19\n",
    "* \n",
    "\n",
    "## ChatLM-mini-Chinese\n",
    "\n",
    "* T5 基础上得到，提供数据，训练，微调整套\n",
    "* 0.2B 小模型。\n",
    "* 另外一个小模型： https://github.com/charent/Phi2-mini-Chinese \n",
    "\n",
    "## Dify\n",
    "\n",
    "## ragflow\n",
    "\n",
    "* 主打rag 智能文档处理及其可视化和可解释性\n",
    "\n",
    "## embedchain\n",
    "\n",
    "* https://docs.embedchain.ai/components/evaluation\n",
    "* 主打简单，功能丰富。支持mysql\n",
    "\n",
    "## GoMate\n",
    "\n",
    "* https://github.com/gomate-community/GoMate\n",
    "* 可靠的输入，可信的输出: 但是怎么体现可信，可靠的？\n",
    "* 231 stars.\n",
    "\n",
    "## QAnything\n",
    "\n",
    "* https://github.com/netease-youdao/QAnything\n",
    "* Netease Youdao's open-source embedding and reranker models for RAG products.\n",
    "* [BCEmbedding: Bilingual and Crosslingual Embedding for RAG](https://github.com/netease-youdao/BCEmbedding)\n",
    "\n",
    "## Langchain-Chatchat\n",
    "\n",
    "* https://github.com/chatchat-space/Langchain-Chatchat\n",
    "  * 20.6k stars\n",
    "  * 支持openai, 本地模型[FastChat支持]\n",
    "\n",
    "## anything-llm\n",
    "\n",
    "* https://github.com/Mintplex-Labs/anything-llm   14.6k stars\n",
    "* The all-in-one Desktop & Docker AI application with full RAG and AI Agent capabilities.\n",
    "\n",
    "## phidata\n",
    "\n",
    "* function calling is all you need.\n",
    "* https://github.com/phidatahq/phidata\n",
    "\n",
    "## FastChat 29.2k 23-11-16\n",
    "\n",
    "* https://github.com/BlinkDL/FastChat\n",
    "* 支持多种大模型，包括ChatGLM, QWen, 支持finetune，支持serving（与openai一样的api）\n",
    "* [Docs](https://projects.laion.ai/Open-Assistant/docs/intro)\n",
    "* [Supported Models](https://github.com/lm-sys/FastChat/blob/main/docs/model_support.md)\n",
    "\n",
    "## FastGPT\n",
    "\n",
    "* FastGPT is a knowledge-based platform built on the LLM, offers out-of-the-box data processing and model invocation capabilities, allows for workflow orchestration through Flow visualization!\n",
    "\n",
    "\n",
    "## AutoGen\n",
    "\n",
    "## langchain\n",
    "* https://github.com/hwchase17/langchain\n",
    "  * https://github.com/hwchase17/chat-langchain \n",
    "    * running example: This repo is an implementation of a locally hosted chatbot specifically focused on question answering over the LangChain documentation. Built with LangChain and FastAPI.\n",
    "  * https://huggingface.co/spaces/JavaFXpert/Chat-GPT-LangChain\n",
    "* Building applications with LLMs through composability\n",
    "* do the dirty work when building apps with LLMs.\n",
    "\n",
    "## Lamda-Index\n",
    "* https://github.com/huggingface/lamda-index\n",
    "* Lamda-Index is a tool to index and search large language models. It is designed to be used with the Hugging Face Hub.\n",
    "* https://huggingface.co/spaces/huggingface/lamda-index\n",
    "\n",
    "## open assistant 35.7k stars 23-11-16\n",
    "* https://github.com/LAION-AI/Open-Assistant\n",
    "* have several plugins: web retrieval plugin.\n",
    "\n",
    "## Luotuo-Chinese-LLM\n",
    "\n",
    "## RWKV\n",
    "* https://github.com/BlinkDL/RWKV-LM\n",
    "\n",
    "## FlagOpen\n",
    "\n",
    "* https://github.com/FlagOpen\n",
    "* 2022年2月28日，智源研究院最新发布了FlagOpen飞智大模型技术开源体系\n",
    "* 包括数据标注部分。FlagData\n",
    "\n",
    "## OpenChatKit\n",
    "* https://github.com/togethercomputer/OpenChatKit\n",
    "* provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. The kit includes an instruction-tuned language models, a moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories. OpenChatKit models were trained on the OIG-43M training dataset, which was a collaboration between Together, LAION, and Ontocord.ai.\n",
    "* Augmenting the model with additional context from a retrieval index\n",
    "\n",
    "## ChatRWKV\n",
    "\n",
    "## xTuring\n",
    "* https://xturing.stochastic.ai/finetune/guide/\n",
    "* Easily build, customize and control your own LLMs\n",
    "\n",
    "## MLC LLM\n",
    "* Enable everyone to develop, optimize and deploy AI models natively on everyone's devices.\n",
    "* https://github.com/mlc-ai/mlc-llm\n",
    "\n",
    "\n",
    "## others\n",
    "\n",
    "### awesome\n",
    "\n",
    "* https://github.com/nichtdax/awesome-totally-open-chatgpt\n",
    "* [IDPChat](https://github.com/BaihaiAI/IDPChat)    \n",
    "  * IDPChat是开放的中文多模态模型，基于预训练大预言模型 LLaMA-13B 和开源文生图预训练模型Stable Diffusion构建。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Tools\n",
    "\n",
    "## txtai\n",
    "\n",
    "* https://github.com/neuml/txtai\n",
    "* All-in-one open-source embeddings database for semantic search, LLM orchestration and language model workflows\n",
    "\n",
    "## Question: is there any reason to use txtai instead of llama_index?\n",
    "\n",
    "## FlagEmbedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API tools via llms\n",
    "\n",
    "## Gorilla\n",
    "\n",
    "* https://github.com/ShishirPatil/gorilla\n",
    "* Gorilla: An API store for LLMs\n",
    "* 使用external apis\n",
    "\n",
    "\n",
    "## One-api\n",
    "\n",
    "* https://github.com/songquanpeng/one-api\n",
    "* OpenAI 接口管理 & 分发系统，支持 Azure、Anthropic Claude、Google PaLM 2 & Gemini、智谱 ChatGLM、百度文心一言、讯飞星火认知、阿里通义千问、360 智脑以及腾讯混元，\n",
    "\n",
    "\n",
    "## serving API\n",
    "\n",
    "\n",
    "### api-for-open-llm\n",
    "\n",
    "* https://github.com/xusenlinzy/api-for-open-llm/tree/master\n",
    "* 支持绝大多数local llms：LLaMA, LLaMA-2, BLOOM, Falcon, Baichuan, Qwen, Xverse, SqlCoder, CodeLLaMA, ChatGLM, ChatGLM2, ChatGLM3\n",
    "\n",
    "### litellm \n",
    "\n",
    "* https://docs.litellm.ai/docs/proxy/quick_start\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function calling : Agent tools\n",
    "\n",
    "## Byzer-LLM\n",
    "\n",
    "* tutorial: https://mp.weixin.qq.com/s/GTVCYUhR_atYMX9ymp0eCg\n",
    "* https://mp.weixin.qq.com/s?__biz=MzIyNzQyNzgxNQ==&mid=2247484107&idx=1&sn=7966ce057bc495bb55d583f39c9f11a5&chksm=e8601643df179f55291794dddd9f98b992aff2c85f51d04eb86ae76d8f1c38f9f4abde5e1a96&scene=132&exptype=timeline_recommend_article_extendread_samebiz#wechat_redirect\n",
    "* https://github.com/allwefantasy/byzer-agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-processing tools for llm\n",
    "\n",
    "## PyMuPDF4LLM\n",
    "\n",
    "* https://artifex.com/blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf\n",
    "\n",
    "## Stirling-PDF\n",
    "\n",
    "* https://github.com/Stirling-Tools/Stirling-PDF\n",
    "* 对PDF文件进行各种操作，包括拆分、合并、转换、重新组织、添加图像、旋转、压缩等.\n",
    "* 可以直接部署。\n",
    "\n",
    "## pdfminer\n",
    "\n",
    "* https://github.com/euske/pdfminer\n",
    "* 专注于从 PDF 文件中提取文本和信息 https://mp.weixin.qq.com/s/W01r8BhHWoQsSBQqfan-cQ\n",
    "\n",
    "## omniparse\n",
    "\n",
    "* https://github.com/adithya-s-k/omniparse\n",
    "* 易用，直接输出xml 结构化数据。\n",
    "\n",
    "## PDF-Extract-Kit\n",
    "\n",
    "* https://github.com/opendatalab/PDF-Extract-Kit\n",
    "* 高质量PDF内容提取的综合工具包，集成多种模型实现PDF布局检测、公式检测与识别、光学字符识别，适用于学术论文、教科书、研究报告和财务报表等多种文档类型，即使在扫描模糊或有水印的情况下也能保持高鲁棒性。\n",
    "\n",
    "## MinerU\n",
    "\n",
    "* https://github.com/opendatalab/MinerU\n",
    "* A one-stop, open-source, high-quality data extraction tool, supports PDF/webpage/e-book extraction.一站式开源高质量数据提取工具，支持PDF/网页/多格式电子书提取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM TOD tools\n",
    "\n",
    "## OpsPilot\n",
    "\n",
    "* https://github.com/WeOps-Lab/OpsPilot\n",
    "* OpsPilot是一个基于Rasa和LLM技术的ChatBot，为运维系统提供ChatOps/LMOps的能力\n",
    "\n",
    "## RasaGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "\n",
    "## mem0\n",
    "\n",
    "* https://github.com/mem0ai/mem0 分免费版和企业版\n",
    "* The memory layer for Personalized AI\n",
    "* Core features:\n",
    "    * Multi-Level Memory: User, Session, and AI Agent memory retention\n",
    "    * Adaptive Personalization: Continuous improvement based on interactions\n",
    "    * Developer-Friendly API: Simple integration into various applications\n",
    "    * Cross-Platform Consistency: Uniform behavior across devices\n",
    "    * Managed Service: Hassle-free hosted solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text2video 开源\n",
    "\n",
    "## text embedding models\n",
    "* Taiyi-CLIP-Roberta-102M-Chinese: IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese. 中文text2embedding 模型，可以跟\"openai/clip-vit-base-patch32\"相应计算。\n",
    "  * https://huggingface.co/IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q transformers accelerate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigscience/bloomz-7b1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "inputs = tokenizer.encode(\"Translate to English: Je t’aime.\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetune and train\n",
    "\n",
    "## Llama Factory\n",
    "\n",
    "* Unify Efficient Fine-Tuning of 100+ LLMs, 17.2k stars\n",
    "* https://github.com/hiyouga/LLaMA-Factory\n",
    "* 使用 Web UI 进行监督微调\n",
    "\n",
    "## [PEFT](https://huggingface.co/docs/peft/index) \n",
    "* Parameter-Efficient Fine-Tuning (PEFT)\n",
    "* supported models\n",
    "  * [LoRA](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "  * [Prefix Tuning](https://aclanthology.org/2021.acl-long.353/)\n",
    "  * [P-Tuning](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "  * [Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "  * [AdaLoRA](https://arxiv.org/abs/2303.10512)\n",
    "  * [LLaMA-Adapter](https://github.com/ZrrSkywalker/LLaMA-Adapter)\n",
    "\n",
    "\n",
    "## [deepspeech](https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload) \n",
    "* designed for speed and scale for distributed training of large models with billions of parameters\n",
    "* coupled with PEFT\n",
    "* At its core is the Zero Redundancy Optimizer (ZeRO) that shards optimizer states (ZeRO-1), gradients (ZeRO-2), and parameters (ZeRO-3) across data parallel processes.\n",
    "* ZeRO-Offload reduces GPU compute and memory by leveraging CPU resources during optimization.\n",
    "\n",
    "## LLMTune\n",
    "* 48G显存的显卡上微调4bit的650亿参数的LLaMA模型\n",
    "\n",
    "## Training Tools\n",
    "\n",
    "### Firefly\n",
    "\n",
    "* Firefly: 一站式大模型训练工具 https://github.com/yangjianxin1/Firefly\n",
    "* 支持对主流的大模型进行预训练、指令微调和DPO，包括但不限于Gemma、Qwen1.5、MiniCPM、Llama、InternLM、Baichuan、ChatGLM、Yi、Deepseek、Qwen、Orion、Ziya、Xverse、Mistral、Mixtral-8x7B、Zephyr、Vicuna、Bloom等。 本项目支持全量参数训练、LoRA、QLoRA高效训练，支持预训练、SFT、DPO。\n",
    "* 整理并开源指令微调数据集："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大模型未来方向\n",
    "\n",
    "* 更长的context\n",
    "* 本地化部署，一般GPU如3090上运行\n",
    "* 联网，与本地化知识库相结合\n",
    "* 更快的推理速度\n",
    "* prompt tuning for applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.29.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25b04a004a14659905edfbff416084d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb5d1ae56ae4530aa0afa07b80325a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e283a73526814870b46a0f29b22c5fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00008.bin:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccad1d5c99414efeb40ba05a74845dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab07900c3d445819783788a5e93930e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0bc799259e4e09b748d621618347af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00008.bin:   0%|          | 0.00/1.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c889c2d622424feeb38bdeffa73dd564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00005-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cae8c46f8704a23aebcdfc9e0958ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00006-of-00008.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20de7999f526495ba031c7bff8bdde52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00007-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6eaa8c636ab409a8c58a2f070cf3272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00008-of-00008.bin:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4125fdde9c4dc784bccaf3a098146c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。\n",
      "晚上睡不着可能会让人感到不安和焦虑，但以下是一些可能有用的技巧，帮助更好地入睡：\n",
      "\n",
      "1. 保持放松：试着进行深呼吸、渐进性肌肉放松或冥想等放松技巧，减轻身体和心理上的压力。\n",
      "\n",
      "2. 创造舒适的睡眠环境：保持房间安静、凉爽、黑暗和舒适，确保床垫和枕头柔软舒适。\n",
      "\n",
      "3. 避免刺激性物质：避免饮用含有咖啡因、酒精或尼古丁的饮料，以及吃过多油腻或刺激性的食物。\n",
      "\n",
      "4. 建立规律的睡眠时间表：尽可能每天在相同的时间上床睡觉，并尝试保持规律的睡眠时间表，帮助身体适应和调整生物钟。\n",
      "\n",
      "5. 进行轻松的活动：在睡觉前做些轻松的活动，如读书、洗澡、听轻柔的音乐等，有助于放松身心。\n",
      "\n",
      "6. 尝试一些放松技巧：如果无法入睡，可以尝试一些放松技巧，如冥想、渐进性肌肉放松、深呼吸等。\n",
      "\n",
      "如果这些方法不能帮助入睡，你可能需要咨询医生或心理医生，以获得更深入的建议和治疗。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import transformers\n",
    "\n",
    "print(transformers.__version__)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()\n",
    "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "print(response)\n",
    "response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在北京冬奥会自由式滑雪女子坡面障碍技巧决赛中，中国选手谷爱凌夺得银牌。在决赛中，谷爱凌表现优异，获得69.90分，在12位选手中排名第三。但在第二轮中，谷爱凌在道具区第三个障碍处失误，获得16.98分。据微信公众号“界面”报道，中国发改委反垄断调查小组突击查访奔驰上海办事处，调取数据材料，并对多名奔驰高管进行了约谈。\n"
     ]
    }
   ],
   "source": [
    "chunk = \"\"\"摘要：\n",
    "财联社5月22日讯，据平安包头微信公众号消息，近日，包头警方发布一起利用人工智能（AI）实施电信诈骗的典型案例，福州市某科技公司法人代表郭先生10分钟内被骗430万元。4月20日中午，郭先生的好友突然通过微信视频联系他，自己的朋友在外地竞标，需要430万保证金，且需要公对公账户过账，想要借郭先生公司的账户走账。基于对好友的信任，加上已经视频聊天核实了身份，郭先生没有核实钱款是否到账，就分两笔把430万转到了好友朋友的银行卡上。郭先生拨打好友电话，才知道被骗。骗子通过智能AI换脸和拟声技术，佯装好友对他实施了诈骗。值得注意的是，骗子并没有使用一个仿真的好友微信添加郭先生为好友，而是直接用好友微信发起视频聊天，这也是郭先生被骗的原因之一。骗子极有可能通过技术手段盗用了郭先生好友的微信。幸运的是，接到报警后，福州、包头两地警银迅速启动止付机制，成功止付拦截336.84万元，但仍有93.16万元被转移，目前正在全力追缴中。\n",
    "\"\"\"\n",
    "\n",
    "chunk = \"摘要成200字左右：在北京冬奥会自由式滑雪女子坡面障碍技巧决赛中，中国选手谷爱凌夺得银牌。祝贺谷爱凌！今天上午，自由式滑雪女子坡面障碍技巧决赛举行。决赛分三轮进行，取选手最佳成绩排名决出奖牌。第一跳，中国选手谷爱凌获得69.90分。在12位选手中排名第三。完成动作后，谷爱凌又扮了个鬼脸，甚是可爱。第二轮中，谷爱凌在道具区第三个障碍处失误，落地时摔倒。获得16.98分。\"\n",
    "\n",
    "text = \"据微信公众号“界面”报道，4日上午10点左右，中国发改委反垄断调查小组突击查访奔驰上海办事处，调取数据材料，并对多名奔驰高管进行了约谈。截止昨日晚9点，包括北京梅赛德斯-奔驰销售服务有限公司东区总经理在内的多名管理人员仍留在上海办公室内\"\n",
    "\n",
    "\n",
    "response, history = model.chat(tokenizer, chunk + text, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 159\n"
     ]
    }
   ],
   "source": [
    "print(len(chunk + text), len(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering proxy context http://192.168.1.45:10809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : C:\\Users\\73915\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int8\\22906aeb32fd7952ce323dc9d25e01693b270da6\\quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 C:\\Users\\73915\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int8\\22906aeb32fd7952ce323dc9d25e01693b270da6\\quantization_kernels_parallel.c -shared -o C:\\Users\\73915\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int8\\22906aeb32fd7952ce323dc9d25e01693b270da6\\quantization_kernels_parallel.so\n",
      "Compile default cpu kernel failed, using default cpu kernel code.\n",
      "Compiling gcc -O3 -fPIC -std=c99 C:\\Users\\73915\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int8\\22906aeb32fd7952ce323dc9d25e01693b270da6\\quantization_kernels.c -shared -o C:\\Users\\73915\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int8\\22906aeb32fd7952ce323dc9d25e01693b270da6\\quantization_kernels.so\n",
      "Compile default cpu kernel failed.\n",
      "Failed to load kernel.\n",
      "Cannot load cpu kernel, don't use quantized model on cpu.\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n",
      "exit proxy context http://192.168.1.45:10809\n",
      "<class 'transformers_modules.THUDM.chatglm-6b-int8.22906aeb32fd7952ce323dc9d25e01693b270da6.modeling_chatglm.ChatGLMForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from yelib.utils.network_util import ProxyContext\n",
    "\n",
    "with ProxyContext():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b-int8\", trust_remote_code=True)\n",
    "    model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int8\", trust_remote_code=True, local_files_only=True).half().cuda()\n",
    "print(type(model))\n",
    "# response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "# print(response)\n",
    "# response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "福州科技公司法人代表郭先生于10分钟内被骗430万元，骗子通过人工智能技术实施电信诈骗。此外，奔驰公司也被中国发改委反垄断调查小组突击查访，并对多名高管进行约谈。\n"
     ]
    }
   ],
   "source": [
    "chunk = \"\"\"摘要：\n",
    "财联社5月22日讯，据平安包头微信公众号消息，近日，包头警方发布一起利用人工智能（AI）实施电信诈骗的典型案例，福州市某科技公司法人代表郭先生10分钟内被骗430万元。4月20日中午，郭先生的好友突然通过微信视频联系他，自己的朋友在外地竞标，需要430万保证金，且需要公对公账户过账，想要借郭先生公司的账户走账。基于对好友的信任，加上已经视频聊天核实了身份，郭先生没有核实钱款是否到账，就分两笔把430万转到了好友朋友的银行卡上。郭先生拨打好友电话，才知道被骗。骗子通过智能AI换脸和拟声技术，佯装好友对他实施了诈骗。值得注意的是，骗子并没有使用一个仿真的好友微信添加郭先生为好友，而是直接用好友微信发起视频聊天，这也是郭先生被骗的原因之一。骗子极有可能通过技术手段盗用了郭先生好友的微信。幸运的是，接到报警后，福州、包头两地警银迅速启动止付机制，成功止付拦截336.84万元，但仍有93.16万元被转移，目前正在全力追缴中。\n",
    "\"\"\"\n",
    "\n",
    "# chunk = \"摘要成200字左右：在北京冬奥会自由式滑雪女子坡面障碍技巧决赛中，中国选手谷爱凌夺得银牌。祝贺谷爱凌！今天上午，自由式滑雪女子坡面障碍技巧决赛举行。决赛分三轮进行，取选手最佳成绩排名决出奖牌。第一跳，中国选手谷爱凌获得69.90分。在12位选手中排名第三。完成动作后，谷爱凌又扮了个鬼脸，甚是可爱。第二轮中，谷爱凌在道具区第三个障碍处失误，落地时摔倒。获得16.98分。\"\n",
    "\n",
    "text = \"据微信公众号“界面”报道，4日上午10点左右，中国发改委反垄断调查小组突击查访奔驰上海办事处，调取数据材料，并对多名奔驰高管进行了约谈。截止昨日晚9点，包括北京梅赛德斯-奔驰销售服务有限公司东区总经理在内的多名管理人员仍留在上海办公室内\"\n",
    "\n",
    "\n",
    "response, history = model.chat(tokenizer, chunk + text, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generation config docs\n",
    "\n",
    "* https://huggingface.co/docs/transformers/main/en/main_classes/text_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese-langchain\n",
    "\n",
    "## install\n",
    "\n",
    "* running build_ext error: [WinError 2] 系统找不到指定的文件\n",
    "* segmental fault问题：pip install boost cmake， 可能是cmake没有正确安装。\n",
    "* Chinese-langchain \n",
    "\n",
    "\n",
    "### unstructured 安装：会导致系统卡死\n",
    "* [Installation with conda on Windows](https://github.com/Unstructured-IO/unstructured/blob/main/docs/source/installing.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache\n",
    "\n",
    "## [CodeFuse-ModelCache](https://github.com/codefuse-ai/CodeFuse-ModelCache)\n",
    "\n",
    "## [GPTCache](https://github.com/zilliztech/GPTCache) 6k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 国内加速下载\n",
    "\n",
    "* https://mp.weixin.qq.com/s/Fx6nfFt_RPwDHZ3V73PD1Q\n",
    "* git clone https://github.com/LetheSec/HuggingFace-Download-Accelerator.git\n",
    "* export HF_ENDPOINT=\"https://hf-mirror.com\"\n",
    "  * https://padeoe.com/huggingface-large-models-downloader/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval 数据\n",
    "\n",
    "## z-bench\n",
    "\n",
    "* https://github.com/zhenbench/z-bench?tab=readme-ov-file\n",
    "* 包含：基础能力, 进阶能力, 垂直能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaderboard\n",
    "\n",
    "* https://hf.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
    "* https://chat.lmsys.org/?leaderboard\n",
    "* https://hf.co/spaces/mteb/leaderboard\n",
    "* https://github.com/jeinlee1991/chinese-llm-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trouble shooting\n",
    "\n",
    "## bitsandbytes cuda windows\n",
    "\n",
    "### LLaMA_etuning\n",
    "\n",
    "* 以下无法成功，但是唯一找到方法\n",
    "  * https://github.com/hiyouga/LLaMA-Efficient-Tuning\n",
    "  * pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.39.1-py3-none-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "\n",
    "model_path = \"LinkSoul/Chinese-Llama-2-7b\"\n",
    "from yelib.utils.network_util import ProxyContext\n",
    "\n",
    "with ProxyContext():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path).half().cuda()\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True) # type: ignore\n",
    "\n",
    "instruction = \"\"\"[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "            If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n{} [/INST]\"\"\"\n",
    "\n",
    "prompt = instruction.format(\"用英文回答，什么是夫妻肺片？\")\n",
    "generate_ids = model.generate(tokenizer(prompt, return_tensors='pt').input_ids.cuda(), max_new_tokens=4096, streamer=streamer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"anything\",\n",
    "    base_url=\"http://0.0.0.0:4000\"\n",
    ")\n",
    "\n",
    "# request sent to model set on litellm proxy, `litellm --model`\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"this is a test request, write a short poem\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "HuggingfaceException - response is not in expected format - [{'detail': 'Method Not Allowed'}]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHuggingfaceError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\main.py:1418\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   1417\u001b[0m custom_prompt_dict \u001b[38;5;241m=\u001b[39m custom_prompt_dict \u001b[38;5;129;01mor\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mcustom_prompt_dict\n\u001b[1;32m-> 1418\u001b[0m model_response \u001b[38;5;241m=\u001b[39m \u001b[43mhuggingface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhuggingface_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1434\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1436\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m optional_params\n\u001b[0;32m   1437\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m optional_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1438\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m acompletion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1439\u001b[0m ):\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;66;03m# don't try to access stream object,\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\llms\\huggingface_restapi.py:550\u001b[0m, in \u001b[0;36mHuggingface.completion\u001b[1;34m(self, model, messages, api_base, headers, model_response, print_verbose, timeout, encoding, api_key, logging_obj, custom_prompt_dict, acompletion, optional_params, litellm_params, logger_fn)\u001b[0m\n\u001b[0;32m    549\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 550\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\llms\\huggingface_restapi.py:539\u001b[0m, in \u001b[0;36mHuggingface.completion\u001b[1;34m(self, model, messages, api_base, headers, model_response, print_verbose, timeout, encoding, api_key, logging_obj, custom_prompt_dict, acompletion, optional_params, litellm_params, logger_fn)\u001b[0m\n\u001b[0;32m    535\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m HuggingfaceError(\n\u001b[0;32m    536\u001b[0m                 message\u001b[38;5;241m=\u001b[39mcompletion_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    537\u001b[0m                 status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[0;32m    538\u001b[0m             )\n\u001b[1;32m--> 539\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_model_response_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompletion_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HuggingfaceError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\llms\\huggingface_restapi.py:224\u001b[0m, in \u001b[0;36mHuggingface.convert_to_model_response_object\u001b[1;34m(self, completion_response, model_response, task, optional_params, encoding, input_text, model)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(completion_response, \u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(completion_response[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m completion_response[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    223\u001b[0m ):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HuggingfaceError(\n\u001b[0;32m    225\u001b[0m         status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m422\u001b[39m,\n\u001b[0;32m    226\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse is not in expected format - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompletion_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m     )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(completion_response[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mHuggingfaceError\u001b[0m: response is not in expected format - [{'detail': 'Method Not Allowed'}]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_65992\\2598652386.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m response = litellm.completion(\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2792\u001b[0m                     if (\n\u001b[0;32m   2793\u001b[0m                         \u001b[0mliteDebuggerClient\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mliteDebuggerClient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdashboard_url\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2794\u001b[0m                     ):  # make it easy to get to the debugger logs if you've initialized it\n\u001b[0;32m   2795\u001b[0m                         \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf\"\\n Check the log in your dashboard - {liteDebuggerClient.dashboard_url}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2796\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2792\u001b[0m                     if (\n\u001b[0;32m   2793\u001b[0m                         \u001b[0mliteDebuggerClient\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mliteDebuggerClient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdashboard_url\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2794\u001b[0m                     ):  # make it easy to get to the debugger logs if you've initialized it\n\u001b[0;32m   2795\u001b[0m                         \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf\"\\n Check the log in your dashboard - {liteDebuggerClient.dashboard_url}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2796\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\main.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   2089\u001b[0m             )\n\u001b[0;32m   2090\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2091\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2092\u001b[0m         \u001b[1;31m## Map to OpenAI Exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2093\u001b[1;33m         raise exception_type(\n\u001b[0m\u001b[0;32m   2094\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2095\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2096\u001b[0m             \u001b[0moriginal_exception\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[0;32m   8281\u001b[0m         \u001b[1;31m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8283\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8284\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8285\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\litellm\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[0;32m   8281\u001b[0m         \u001b[1;31m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8283\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8284\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8285\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAPIError\u001b[0m: HuggingfaceException - response is not in expected format - [{'detail': 'Method Not Allowed'}]"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "import os\n",
    "\n",
    "response = litellm.completion(\n",
    "    model=\"huggingface/bigcode/starcoder\",               # add `openai/` prefix to model so litellm knows to route to OpenAI\n",
    "    api_key=\"sk-1234\",                  # api key to your openai compatible endpoint\n",
    "    api_base=\"http://192.168.1.45:4000\",     # set API Base of your Custom OpenAI Endpoint\n",
    "    messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Hey, how's it going?\",\n",
    "                }\n",
    "    ],\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
