{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from llama_index.core.callbacks.base_handler import BaseCallbackHandler\n",
    "from llama_index.core.callbacks.schema import CBEventType, EventPayload\n",
    "\n",
    "from aim import Run, Text  # type: ignore[attr-defined]\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "class AimCallback1(BaseCallbackHandler):\n",
    "    \"\"\"\n",
    "    AimCallback callback class.\n",
    "\n",
    "    Args:\n",
    "        repo (:obj:`str`, optional):\n",
    "            Aim repository path or Repo object to which Run object is bound.\n",
    "            If skipped, default Repo is used.\n",
    "        experiment_name (:obj:`str`, optional):\n",
    "            Sets Run's `experiment` property. 'default' if not specified.\n",
    "            Can be used later to query runs/sequences.\n",
    "        system_tracking_interval (:obj:`int`, optional):\n",
    "            Sets the tracking interval in seconds for system usage\n",
    "            metrics (CPU, Memory, etc.). Set to `None` to disable\n",
    "            system metrics tracking.\n",
    "        log_system_params (:obj:`bool`, optional):\n",
    "            Enable/Disable logging of system params such as installed packages,\n",
    "            git info, environment variables, etc.\n",
    "        capture_terminal_logs (:obj:`bool`, optional):\n",
    "            Enable/Disable terminal stdout logging.\n",
    "        event_starts_to_ignore (Optional[List[CBEventType]]):\n",
    "            list of event types to ignore when tracking event starts.\n",
    "        event_ends_to_ignore (Optional[List[CBEventType]]):\n",
    "            list of event types to ignore when tracking event ends.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        repo: Optional[str] = None,\n",
    "        experiment_name: Optional[str] = None,\n",
    "        system_tracking_interval: Optional[int] = 1,\n",
    "        log_system_params: Optional[bool] = True,\n",
    "        capture_terminal_logs: Optional[bool] = True,\n",
    "        event_starts_to_ignore: Optional[List[CBEventType]] = None,\n",
    "        event_ends_to_ignore: Optional[List[CBEventType]] = None,\n",
    "        run_params: Optional[Dict[str, Any]] = None,\n",
    "    ) -> None:\n",
    "        if Run is None:\n",
    "            raise ModuleNotFoundError(\n",
    "                \"Please install aim to use the AimCallback: 'pip install aim'\"\n",
    "            )\n",
    "\n",
    "        event_starts_to_ignore = (\n",
    "            event_starts_to_ignore if event_starts_to_ignore else []\n",
    "        )\n",
    "        event_ends_to_ignore = event_ends_to_ignore if event_ends_to_ignore else []\n",
    "        super().__init__(\n",
    "            event_starts_to_ignore=event_starts_to_ignore,\n",
    "            event_ends_to_ignore=event_ends_to_ignore,\n",
    "        )\n",
    "\n",
    "        self.repo = repo\n",
    "        llm.experiment_name = experiment_name\n",
    "        self.system_tracking_interval = system_tracking_interval\n",
    "        self.log_system_params = log_system_params\n",
    "        self.capture_terminal_logs = capture_terminal_logs\n",
    "        self._run: Optional[Any] = None\n",
    "        self._run_hash = None\n",
    "\n",
    "        self._llm_response_step = 0\n",
    "\n",
    "        self.setup(run_params)\n",
    "\n",
    "    def on_event_start(\n",
    "        self,\n",
    "        event_type: CBEventType,\n",
    "        payload: Optional[Dict[str, Any]] = None,\n",
    "        event_id: str = \"\",\n",
    "        parent_id: str = \"\",\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            event_type (CBEventType): event type to store.\n",
    "            payload (Optional[Dict[str, Any]]): payload to store.\n",
    "            event_id (str): event id to store.\n",
    "            parent_id (str): parent event id.\n",
    "        \"\"\"\n",
    "        return \"\"\n",
    "\n",
    "    def on_event_end(\n",
    "        self,\n",
    "        event_type: CBEventType,\n",
    "        payload: Optional[Dict[str, Any]] = None,\n",
    "        event_id: str = \"\",\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            event_type (CBEventType): event type to store.\n",
    "            payload (Optional[Dict[str, Any]]): payload to store.\n",
    "            event_id (str): event id to store.\n",
    "        \"\"\"\n",
    "        if not self._run:\n",
    "            raise ValueError(\"AimCallback failed to init properly.\")\n",
    "\n",
    "        print(f'aim end event {event_type}, payload: {payload}')\n",
    "        if event_type is CBEventType.LLM and payload:\n",
    "            if EventPayload.PROMPT in payload:\n",
    "                llm_input = str(payload[EventPayload.PROMPT])\n",
    "                llm_output = str(payload[EventPayload.COMPLETION])\n",
    "            else:\n",
    "                message = payload.get(EventPayload.MESSAGES, [])\n",
    "                llm_input = \"\\n\".join([str(x) for x in message])\n",
    "                llm_output = str(payload[EventPayload.RESPONSE])\n",
    "\n",
    "            self._run.track(\n",
    "                Text(llm_input),\n",
    "                name=\"prompt\",\n",
    "                step=self._llm_response_step,\n",
    "                context={\"event_id\": event_id},\n",
    "            )\n",
    "\n",
    "            self._run.track(\n",
    "                Text(llm_output),\n",
    "                name=\"response\",\n",
    "                step=self._llm_response_step,\n",
    "                context={\"event_id\": event_id},\n",
    "            )\n",
    "\n",
    "            self._llm_response_step += 1\n",
    "        elif event_type is CBEventType.CHUNKING and payload:\n",
    "            for chunk_id, chunk in enumerate(payload[EventPayload.CHUNKS]):\n",
    "                self._run.track(\n",
    "                    Text(chunk),\n",
    "                    name=\"chunk\",\n",
    "                    step=self._llm_response_step,\n",
    "                    context={\"chunk_id\": chunk_id, \"event_id\": event_id},\n",
    "                )\n",
    "\n",
    "    @property\n",
    "    def experiment(self) -> Run:\n",
    "        if not self._run:\n",
    "            self.setup()\n",
    "        return self._run\n",
    "\n",
    "    def setup(self, args: Optional[Dict[str, Any]] = None) -> None:\n",
    "        if not self._run:\n",
    "            if self._run_hash:\n",
    "                self._run = Run(\n",
    "                    self._run_hash,\n",
    "                    repo=self.repo,\n",
    "                    system_tracking_interval=self.system_tracking_interval,\n",
    "                    log_system_params=self.log_system_params,\n",
    "                    capture_terminal_logs=self.capture_terminal_logs,\n",
    "                )\n",
    "            else:\n",
    "                self._run = Run(\n",
    "                    repo=self.repo,\n",
    "                    experiment=self.experiment_name,\n",
    "                    system_tracking_interval=self.system_tracking_interval,\n",
    "                    log_system_params=self.log_system_params,\n",
    "                    capture_terminal_logs=self.capture_terminal_logs,\n",
    "                )\n",
    "                self._run_hash = self._run.hash\n",
    "\n",
    "        # Log config parameters\n",
    "        if args:\n",
    "            try:\n",
    "                for key in args:\n",
    "                    self._run.set(key, args[key], strict=False)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Aim could not log config parameters -> {e}\")\n",
    "\n",
    "    def __del__(self) -> None:\n",
    "        if self._run and self._run.active:\n",
    "            self._run.close()\n",
    "\n",
    "    def start_trace(self, trace_id: Optional[str] = None) -> None:\n",
    "        pass\n",
    "\n",
    "    def end_trace(\n",
    "        self,\n",
    "        trace_id: Optional[str] = None,\n",
    "        trace_map: Optional[Dict[str, List[str]]] = None,\n",
    "    ) -> None:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd22a5a3dbcd48e1a96fe56fff9cd735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys,os\n",
    "os.environ[\"HF_HOME\"] = \"/home/jeffye/.cache/huggingface_speed\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/home/jeffye/.cache/huggingface_hub\"\n",
    "\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "from llama_index.core.prompts.base import PromptTemplate\n",
    "import torch\n",
    "import logging\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# os.environ[\"MODELSCOPE_CACHE\"] = \"D:/dataset/cache/modelscope/\"\n",
    "# from llama_index.llms.modelscope import ModelScopeLLM\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from llama_index.core.callbacks import (\n",
    "    CallbackManager,\n",
    "    LlamaDebugHandler,\n",
    "    CBEventType,\n",
    ")\n",
    "\n",
    "# for faster loading the model\n",
    "\n",
    "\n",
    "from llama_index.callbacks.aim import AimCallback\n",
    "\n",
    "aim_callback = AimCallback(repo=\"./\")\n",
    "callback_manager = CallbackManager([aim_callback])\n",
    "\n",
    "\n",
    "# llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "# callback_manager = CallbackManager([llama_debug])\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "# system_prompt = \"you are a very helpful assistant\";\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "# query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "# query_wrapper_prompt = PromptTemplate(\"{query_str}\\n<|im_start|>assistant\")\n",
    "# <|im_start|>assistant\n",
    "import torch\n",
    "\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.2, \"do_sample\": True},\n",
    "    # system_prompt=system_prompt,\n",
    "    # query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"Qwen/Qwen-7B-Chat\",\n",
    "    model_name=\"Qwen/Qwen-7B-Chat\",\n",
    "    device_map=\"auto\",\n",
    "    # stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    # stopping_ids=[151645, 151644][:1],\n",
    "    tokenizer_kwargs={\"max_length\": 4096, \"trust_remote_code\": True, \"add_generation_prompt\": True},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    model_kwargs={\"torch_dtype\": torch.float16, \"trust_remote_code\": True},\n",
    "    callback_manager=callback_manager,\n",
    "    is_chat_model=True,\n",
    ")\n",
    "# very ugly workaround\n",
    "llm._tokenizer.apply_chat_template = partial(llm._tokenizer.apply_chat_template, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nhi<|im_end|>\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Message request\n",
    "from llama_index.core.base.llms.types import MessageRole, ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM, content=\"You are a helpful assistant.\"\n",
    "    ),\n",
    "    ChatMessage(role=MessageRole.USER, content=\"hi\"),\n",
    "]\n",
    "resp = llm.chat(messages, max_length=4000)\n",
    "print(resp)\n",
    "# llm._tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "Settings.llm = llm\n",
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"/mnt/d/dataset/cache/huggingface/hub/models--BAAI--bge-m3/snapshots/c400e3d69da76ed1965ea6ed67c83af69c3ff3a4\")\n",
    "docs = SimpleDirectoryReader(\"./data/\").load_data()\n",
    "\n",
    "# aim_callback = AimCallback(repo=\"./\")\n",
    "# callback_manager = CallbackManager([aim_callback])\n",
    "index = SummaryIndex.from_documents(docs, callback_manager=callback_manager)\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "response = query_engine.query(\"where did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article discusses the founder's journey from studying philosophy in college to founding Viaweb, a company that revolutionized the way software was developed and distributed. The founder was inspired by a novel by Robert A. Heinlein and a PBS documentary featuring Terry Winograd's work on the SHRDLU computer program, leading him to become fascinated with artificial intelligence (AI) and begin experimenting with creating a web-based store builder. The founder eventually founded Y Combinator, a startup accelerator that funds and supports early-stage technology companies.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In February 2021, I focused on developing my language skills and expanding my knowledge base. I continued to learn from user interactions and feedback, and worked on improving my ability to generate coherent and relevant responses to user queries. Additionally, I spent time exploring new areas of interest and learning about emerging technologies. Overall, it was a productive month, and I am excited to continue growing and evolving in the future.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What I Worked On in February 2021\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = aim_callback._run\n",
    "\n",
    "\n",
    "# set training hyperparameters\n",
    "run['hparams'] = {\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "# log metric\n",
    "for i in range(10):\n",
    "    run.track(i, name='numbers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "from aim import Run, Text\n",
    "\n",
    "for step in range(100):\n",
    "    # Generate a random string for this example\n",
    "    random_str = ''.join(random.choices(\n",
    "        string.ascii_uppercase +\n",
    "        string.digits, k=20)\n",
    "    )\n",
    "    aim_text = Text(random_str)\n",
    "    run.track(aim_text, name='mytext1', step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using huggingface llm: https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom.html\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "import torch\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": True},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    # model_name=\"Qwen-7B-Chat\"\n",
    "    device_map=\"auto\",\n",
    "    stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using huggingface llm: https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom.html\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "import torch\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": True},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    # model_name=\"Qwen-7B-Chat\"\n",
    "    device_map=\"auto\",\n",
    "    stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete = llm.complete('你是谁')\n",
    "# complete\n",
    "from llama_index.llms.base import ChatMessage\n",
    "\n",
    "messages=[\n",
    "    ChatMessage(**{\"role\": \"system\", \"content\": \"You are a helpful assistant, who can answer all questions in english.\"}),\n",
    "    ChatMessage(**{\"role\": \"user\", \"content\": \"how do you think of China?\"})\n",
    "  ]\n",
    "\n",
    "complete = llm.chat(messages=messages)\n",
    "complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query your data\n",
    "from llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine: RetrieverQueryEngine = index.as_query_engine() # type: ignore\n",
    "response = query_engine.query(\"Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "from llama_index.response.schema import Response\n",
    "\n",
    "\n",
    "response: Response = query_engine.query(\"What did the author do growing up?\") # type: ignore\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.source_nodes[1].text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tutorials/resources\n",
    "\n",
    "## supported llm frameworks\n",
    "\n",
    "* https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules.html#openllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleDirectoryReader\n",
    "\n",
    "## read a png file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 docs\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# SimpleDirectoryReader(input_files=[\"path/to/file1\", \"path/to/file2\"])\n",
    "required_exts = [\".png\", \".txt\"]\n",
    "# docs = SimpleDirectoryReader(\"data\").load_data()\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=\"./data\",\n",
    "    required_exts=required_exts,\n",
    "    # recursive=True,\n",
    ")\n",
    "\n",
    "docs = reader.load_data()\n",
    "print(f\"Loaded {len(docs)} docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_Settings(_llm=None, _embed_model=None, _callback_manager=None, _tokenizer=None, _node_parser=None, _prompt_helper=None, _transformations=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs\n",
    "import sys, os\n",
    "\n",
    "# os.environ[\"HTTP_PROXY\"] = \"http://192.168.1.45:10809\"\n",
    "# os.environ[\"HTTPS_PROXY\"] = \"http://192.168.1.45:10809\"\n",
    "# os.environ[\"HF_ENDPOINT\"] = \"\"\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFaceEmbedding compute similarity\n",
    "\n",
    "* which is the same as using FlagEmbedding directly\n",
    "* \n",
    "```python\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3',  \n",
    "                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "sentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\n",
    "sentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n",
    "               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n",
    "\n",
    "embeddings_1 = model.encode(sentences_1, \n",
    "                            batch_size=12, \n",
    "                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n",
    "                            )['dense_vecs']\n",
    "embeddings_2 = model.encode(sentences_2)['dense_vecs']\n",
    "similarity = embeddings_1 @ embeddings_2.T\n",
    "print(similarity)\n",
    "# [[0.6265, 0.3477], [0.3499, 0.678 ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Settings.embed_model.get_text_embedding('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\n",
    "sentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n",
    "               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n",
    "\n",
    "embeddings_1 = Settings.embed_model.get_text_embedding_batch(sentences_1)\n",
    "embeddings_2 = Settings.embed_model.get_text_embedding_batch(sentences_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6259036830688345\n",
      "0.3474958518731192\n",
      "0.34986774819095623\n",
      "0.6782461892912981\n"
     ]
    }
   ],
   "source": [
    "for e1 in embeddings_1:\n",
    "    for e2 in embeddings_2:\n",
    "        print(Settings.embed_model.similarity(e1, e2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "\n",
    "# Create a local Qdrant vector store\n",
    "client = qdrant_client.QdrantClient(path=\"qdrant_db\")\n",
    "\n",
    "text_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"text_collection\"\n",
    ")\n",
    "image_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"image_collection\"\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=text_store, image_store=image_store\n",
    ")\n",
    "\n",
    "# Create the MultiModal index\n",
    "# documents = SimpleDirectoryReader(\"./data_wiki/\").load_data()\n",
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    docs,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.schema import ImageNode\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        if os.path.isfile(img_path):\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(2, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 9:\n",
    "                break\n",
    "\n",
    "test_query = \"chinese girl\"\n",
    "test_query = \"because YC was doing great. But if there was one thing rarer than Rtm offering advice\"\n",
    "\n",
    "test_query = \"In the summer of 2006, Robert and I started working on a new version of Arc. This one was reasonably fast, because it was compiled into Scheme. To test this new Arc, I wrote Hacker News in it. It was originally meant to be a news aggregator for startup founders and was called Startup News, but after a few months I got tired of reading about nothing but startups. Plus it wasn't startup founders we wanted to reach.\"\n",
    "# generate  retrieval results\n",
    "retriever = index.as_retriever(similarity_top_k=3, image_similarity_top_k=5)\n",
    "retrieval_results = retriever.retrieve(test_query)\n",
    "\n",
    "retrieved_image = []\n",
    "for res_node in retrieval_results:\n",
    "    if isinstance(res_node.node, ImageNode):\n",
    "        retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "        print(res_node)\n",
    "    else:\n",
    "        display_source_node(res_node, source_length=500)\n",
    "        pass\n",
    "\n",
    "plot_images(retrieved_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2178, 0.2418, 0.2357]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image\n",
    "\n",
    "#Load CLIP model\n",
    "model = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "#Encode an image:\n",
    "img_emb = model.encode(Image.open('data/3.png'))\n",
    "\n",
    "#Encode text descriptions\n",
    "text_emb = model.encode(['景甜', 'a nice woman', 'A picture of a girl'])\n",
    "\n",
    "#Compute cosine similarities \n",
    "cos_scores = util.cos_sim(img_emb, text_emb)\n",
    "print(cos_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.8843, 0.8626],\n",
      "        [0.8843, 1.0000, 0.9131],\n",
      "        [0.8626, 0.9131, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "#Encode text descriptions\n",
    "text_emb = model.encode(['shit', 'a nice woman', 'A picture of a girl'])\n",
    "\n",
    "img_emb = model.encode('-')\n",
    "#Compute cosine similarities \n",
    "cos_scores = util.cos_sim(text_emb, text_emb)\n",
    "print(cos_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 512)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# environment variables\n",
    "\n",
    "## model directory\n",
    "\n",
    "* LLAMA_INDEX_CACHE_DIR : \n",
    "* from llama_index.core.utils import get_cache_dir\n",
    "* C:\\Users\\73915\\AppData\\Local\\llama_index to control where these files are saved.\n",
    "* 问题： Huggingface 会单独存一份，不会使用Huaggingface的缓存."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trouble shotting\n",
    "\n",
    "## stopping_ids 如何找\n",
    "\n",
    "* 如Qwen: 1.  ~/.cache/huggingface/hub/models--Qwen--Qwen-7B-Chat 找到modeling_qwen.py\n",
    "* modeling_qwen.py 中get_stop_words_ids 方法中可以看到stop_words_ids = [[tokenizer.im_end_id], [tokenizer.im_start_id]]\n",
    "\n",
    "## LLM template 相关\n",
    "\n",
    "### template huggingface 介绍\n",
    "\n",
    "* https://huggingface.co/docs/transformers/main/chat_templating#templates-for-chat-models\n",
    "\n",
    "\n",
    "### Qwen 模型在 LlamaIndex 的 HuaggingfaceLLM 中无法生产原因\n",
    "\n",
    "* HuggingfaceLLM 中bugs :  \n",
    "* 1. formatted=True 设置死了，导致query_wrapper_prompt参数设置无效；\n",
    "  * def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n",
    "        prompt = self.messages_to_prompt(messages)\n",
    "        completion_response = self.complete(prompt, formatted=True, **kwargs)\n",
    "        return completion_response_to_chat_response(completion_response)\n",
    "* 2. messages_to_prompt 默认为_tokenizer_messages_to_prompt, 方法中使用tokenizer的apply_chat_template方法，无法设置add_generation_prompt参数（此处应该由参数is_chat_model控制）\n",
    "* 解决方案：\n",
    "  * llm._tokenizer.apply_chat_template = partial(llm._tokenizer.apply_chat_template, add_generation_prompt=True)\n",
    "  * 但是该方法也把调用写死了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 待解决问题"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
