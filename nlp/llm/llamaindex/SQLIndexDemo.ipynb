{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a558ee1",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/index_structs/struct_indices/SQLIndexDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e45f9b60-cd6b-4c15-958f-1feca5438128",
   "metadata": {},
   "source": [
    "# Text-to-SQL Guide (Query Engine + Retriever)\n",
    "\n",
    "This is a basic guide to LlamaIndex's Text-to-SQL capabilities. \n",
    "1. We first show how to perform text-to-SQL over a toy dataset: this will do \"retrieval\" (sql query over db) and \"synthesis\".\n",
    "2. We then show how to buid a TableIndex over the schema to dynamically retrieve relevant tables during query-time.\n",
    "3. We finally show you how to define a text-to-SQL retriever on its own. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3f3baa",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ü¶ô."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69be8d-99ae-4d9f-91e4-b90fc62bcf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ca96e-f98f-4a72-9fe3-a372dbd08a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-..\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "107396a9-4aa7-49b3-9f0f-a755726c19ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "461438c8-302d-45c5-8e69-16ad604686d1",
   "metadata": {},
   "source": [
    "### Create Database Schema\n",
    "\n",
    "We use `sqlalchemy`, a popular SQL database toolkit, to create an empty `city_stats` Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a370b266-66f5-4624-bbf9-2ad57f0511f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    MetaData,\n",
    "    Table,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    "    select,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea24f794-f10b-42e6-922d-9258b7167405",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata_obj = MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4154b29-7e23-4c26-a507-370a66186ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create city SQL table\n",
    "table_name = \"city_stats\"\n",
    "city_stats_table = Table(\n",
    "    table_name,\n",
    "    metadata_obj,\n",
    "    Column(\"city_name\", String(16), primary_key=True),\n",
    "    Column(\"population\", Integer),\n",
    "    Column(\"country\", String(16), nullable=False),\n",
    ")\n",
    "metadata_obj.create_all(engine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c09089a-6bcd-48db-8120-a84c8da3f82e",
   "metadata": {},
   "source": [
    "### Define SQL Database\n",
    "\n",
    "We first define our `SQLDatabase` abstraction (a light wrapper around SQLAlchemy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768d1581-b482-4c73-9963-5ffd68a2aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SQLDatabase\n",
    "# from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bffabba0-8e54-4f24-ad14-2c8979c582a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c495024cd1fd4900a586adf9ffbdc7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "os.environ[\"HF_HOME\"] = \"/home/jeffye/.cache/huggingface_speed\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "# os.environ['HTTP_PROXY'] = \"192.168.1.45:10809\"\n",
    "# os.environ['HTTPS_PROXY'] = \"192.168.1.45:10809\"\n",
    "# os.environ[\"HF_HUB_CACHE\"] = \"/home/jeffye/.cache/huggingface_hub\"\n",
    "\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "from llama_index.core.prompts.base import PromptTemplate\n",
    "import torch\n",
    "import logging\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# os.environ[\"MODELSCOPE_CACHE\"] = \"D:/dataset/cache/modelscope/\"\n",
    "# from llama_index.llms.modelscope import ModelScopeLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from llama_index.core.callbacks import (\n",
    "    CallbackManager,\n",
    "    LlamaDebugHandler,\n",
    "    CBEventType,\n",
    ")\n",
    "\n",
    "from llama_index.callbacks.aim import AimCallback\n",
    "\n",
    "aim_callback = AimCallback(repo=\"./\")\n",
    "# callback_manager = CallbackManager([aim_callback])\n",
    "\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug, aim_callback])\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "# system_prompt = \"you are a very helpful assistant\";\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "# query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "# query_wrapper_prompt = PromptTemplate(\"{query_str}\\n<|im_start|>assistant\")\n",
    "# <|im_start|>assistant\n",
    "\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.2, \"do_sample\": True},\n",
    "    # system_prompt=system_prompt,\n",
    "    # query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"Qwen/Qwen1.5-7B-Chat\",\n",
    "    model_name=\"Qwen/Qwen1.5-7B-Chat\",\n",
    "    device_map=\"auto\",\n",
    "    # stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    # stopping_ids=[151645, 151644][:1],\n",
    "    tokenizer_kwargs={\"max_length\": 4096, \"trust_remote_code\": True},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    model_kwargs={\"torch_dtype\": torch.float16, \"trust_remote_code\": True},\n",
    "    callback_manager=callback_manager,\n",
    "    is_chat_model=True,\n",
    ")\n",
    "\n",
    "# very ugly workaround for a bug\n",
    "llm._tokenizer.apply_chat_template = partial(llm._tokenizer.apply_chat_template, add_generation_prompt=True)\n",
    "Settings.llm = llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "188239e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a88eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Message request\n",
    "from llama_index.core.base.llms.types import MessageRole, ChatMessage\n",
    "import os\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM, content=\"You are a helpful assistant.\"\n",
    "    ),\n",
    "    ChatMessage(role=MessageRole.USER, content=\"who are you?\"),\n",
    "]\n",
    "resp = llm.chat(messages, max_length=4000)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d44c289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_debug.get_event_pairs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bad7ffbe",
   "metadata": {},
   "source": [
    "We add some testing data to our SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95043e10-6cdf-4f66-96bd-ce307ea7df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n",
    "from sqlalchemy import insert\n",
    "\n",
    "rows = [\n",
    "    {\"city_name\": \"Toronto\", \"population\": 2930000, \"country\": \"Canada\"},\n",
    "    {\"city_name\": \"Tokyo\", \"population\": 13960000, \"country\": \"Japan\"},\n",
    "    {\n",
    "        \"city_name\": \"Chicago\",\n",
    "        \"population\": 2679000,\n",
    "        \"country\": \"United States\",\n",
    "    },\n",
    "    {\"city_name\": \"Seoul\", \"population\": 9776000, \"country\": \"South Korea\"},\n",
    "]\n",
    "for row in rows:\n",
    "    stmt = insert(city_stats_table).values(**row)\n",
    "    with engine.begin() as connection:\n",
    "        cursor = connection.execute(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b315b8ff-7dd7-4e7d-ac47-8c5a0c3e7ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Toronto', 2930000, 'Canada'), ('Tokyo', 13960000, 'Japan'), ('Chicago', 2679000, 'United States'), ('Seoul', 9776000, 'South Korea')]\n"
     ]
    }
   ],
   "source": [
    "# view current table\n",
    "stmt = select(\n",
    "    city_stats_table.c.city_name,\n",
    "    city_stats_table.c.population,\n",
    "    city_stats_table.c.country,\n",
    ").select_from(city_stats_table)\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    results = connection.execute(stmt).fetchall()\n",
    "    print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "051a171f-8c97-40ed-ae17-4e3fa3785487",
   "metadata": {},
   "source": [
    "### Query Index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6a2303f-3bae-4fa2-8750-03f9af747848",
   "metadata": {},
   "source": [
    "We first show how we can execute a raw SQL query, which directly executes over the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eddd3608-31ff-4591-a02a-90987e312669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Chicago',)\n",
      "('Seoul',)\n",
      "('Tokyo',)\n",
      "('Toronto',)\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "with engine.connect() as con:\n",
    "    rows = con.execute(text(\"SELECT city_name from city_stats\"))\n",
    "    for row in rows:\n",
    "        print(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e72b931",
   "metadata": {},
   "source": [
    "## Part 1: Text-to-SQL Query Engine\n",
    "Once we have constructed our SQL database, we can use the NLSQLTableQueryEngine to\n",
    "construct natural language queries that are synthesized into SQL queries.\n",
    "\n",
    "Note that we need to specify the tables we want to use with this query engine.\n",
    "If we don't the query engine will pull all the schema context, which could\n",
    "overflow the context window of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d992fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
    "\n",
    "query_engine = NLSQLTableQueryEngine(\n",
    "    sql_database=sql_database, tables=[\"city_stats\"], llm=llm, verbose=True, callback_manager=callback_manager\n",
    ")\n",
    "query_str = \"Which city has the highest population?\"\n",
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0dfe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "print(response)\n",
    "# query_engine._sql_retriever"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "298b4ca2",
   "metadata": {},
   "source": [
    "This query engine should be used in any case where you can specify the tables you want\n",
    "to query over beforehand, or the total size of all the table schema plus the rest of\n",
    "the prompt fits your context window."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dee4d251",
   "metadata": {},
   "source": [
    "## Part 2: Query-Time Retrieval of Tables for Text-to-SQL\n",
    "If we don't know ahead of time which table we would like to use, and the total size of\n",
    "the table schema overflows your context window size, we should store the table schema \n",
    "in an index so that during query time we can retrieve the right schema.\n",
    "\n",
    "The way we can do this is using the SQLTableNodeMapping object, which takes in a \n",
    "SQLDatabase and produces a Node object for each SQLTableSchema object passed \n",
    "into the ObjectIndex constructor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71045c0-7a96-4e86-b38c-c378b7759aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.struct_store.sql_query import (\n",
    "    SQLTableRetrieverQueryEngine,\n",
    ")\n",
    "\n",
    "\n",
    "from llama_index.core.objects import (\n",
    "    SQLTableNodeMapping,\n",
    "    ObjectIndex,\n",
    "    SQLTableSchema,\n",
    ")\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# set Logging to DEBUG for more detailed outputs\n",
    "table_node_mapping = SQLTableNodeMapping(sql_database)\n",
    "table_schema_objs = [\n",
    "    (SQLTableSchema(table_name=\"city_stats\"))\n",
    "]  # add a SQLTableSchema for each table\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    table_schema_objs,\n",
    "    table_node_mapping,\n",
    "    VectorStoreIndex,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6156caf",
   "metadata": {},
   "source": [
    "Now we can take our SQLTableRetrieverQueryEngine and query it for our response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7bb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_engine = SQLTableRetrieverQueryEngine(\n",
    "    sql_database, obj_index.as_retriever(similarity_top_k=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802da9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"ÈÇ£‰∏™ÂüéÂ∏Ç‰∫∫Âè£ÊúÄÂ§ö?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d19b9cd",
   "metadata": {},
   "source": [
    "You can also add additional context information for each table schema you define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a87651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually set context text\n",
    "city_stats_text = (\n",
    "    \"This table gives information regarding the population and country of a\"\n",
    "    \" given city.\\nThe user will query with codewords, where 'foo' corresponds\"\n",
    "    \" to population and 'bar'corresponds to city.\"\n",
    ")\n",
    "\n",
    "table_node_mapping = SQLTableNodeMapping(sql_database)\n",
    "table_schema_objs = [\n",
    "    (SQLTableSchema(table_name=\"city_stats\", context_str=city_stats_text))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef57746-7cbf-48cd-8781-9c048ce3d3c7",
   "metadata": {},
   "source": [
    "## Part 3: Text-to-SQL Retriever\n",
    "\n",
    "So far our text-to-SQL capability is packaged in a query engine and consists of both retrieval and synthesis.\n",
    "\n",
    "You can use the SQL retriever on its own. We show you some different parameters you can try, and also show how to plug it into our `RetrieverQueryEngine` to get roughly the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99128819-6acf-4717-b703-9d1ca41190a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import NLSQLRetriever\n",
    "\n",
    "# default retrieval (return_raw=True)\n",
    "nl_sql_retriever = NLSQLRetriever(\n",
    "    sql_database, tables=[\"city_stats\"], return_raw=True, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc6858-7e3b-4cf1-806f-0d6d63a55d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = nl_sql_retriever.retrieve(\n",
    "    \"Return the top 5 cities (along with their populations) with the highest population.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576523f-0b5c-4c40-8b2a-d734bdfbde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "for n in results:\n",
    "    display_source_node(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfdc307-65e1-4a0e-8098-f30c0db60c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default retrieval (return_raw=False)\n",
    "nl_sql_retriever = NLSQLRetriever(\n",
    "    sql_database, tables=[\"city_stats\"], return_raw=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ccfa42-51e0-49e7-afac-6a0d011af4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = nl_sql_retriever.retrieve(\n",
    "    \"Return the top 5 cities (along with their populations) with the highest population.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd14d5e-7a5c-4898-820a-469b80cee5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: all the content is in the metadata\n",
    "for n in results:\n",
    "    display_source_node(n, show_source_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392889f6-babe-4bce-b802-4379a9b3ca49",
   "metadata": {},
   "source": [
    "### Plug into our `RetrieverQueryEngine`\n",
    "\n",
    "We compose our SQL Retriever with our standard `RetrieverQueryEngine` to synthesize a response. The result is roughly similar to our packaged `Text-to-SQL` query engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85e7a24-bbd0-4439-9da4-26a7a9e43b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(nl_sql_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f2dec0-8700-49d3-81b7-69c21518ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"Return the top 5 cities (along with their populations) with the highest population.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb7e5a-a23b-47ab-a154-98ce4c0d1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
