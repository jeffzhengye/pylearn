{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.elastic_vector_search import ElasticKnnSearch\n",
    "from langchain.embeddings import ElasticsearchEmbeddings\n",
    "import elasticsearch\n",
    "from langchain.embeddings import ElasticsearchEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "\n",
    "class ProxyContext(object):\n",
    "    def __init__(self, proxy='http://192.168.1.45:10809'):\n",
    "        self.proxy = proxy\n",
    "        import os\n",
    "    \n",
    "    def __enter__(self):\n",
    "        os.environ['HTTP_PROXY'] = self.proxy\n",
    "        os.environ['HTTPS_PROXY'] = self.proxy\n",
    "        print('entering proxy context', self.proxy)\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb)-> None:\n",
    "        os.environ['HTTP_PROXY'] = ''\n",
    "        os.environ['HTTPS_PROXY'] = ''\n",
    "        print('exit proxy context', self.proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_search = ElasticKnnSearch(\n",
    "\tes_cloud_id=es_cloud_id, \n",
    "\tes_user=es_user, \n",
    "\tes_password=es_password, \n",
    "\tindex_name= test_index, \n",
    "\tembedding= embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "CONCISE SUMMARY IN CHINESE:\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "chunk = \"\"\"\n",
    "财联社5月22日讯，据平安包头微信公众号消息，近日，包头警方发布一起利用人工智能（AI）实施电信诈骗的典型案例，福州市某科技公司法人代表郭先生10分钟内被骗430万元。\n",
    "4月20日中午，郭先生的好友突然通过微信视频联系他，自己的朋友在外地竞标，需要430万保证金，且需要公对公账户过账，想要借郭先生公司的账户走账。\n",
    "基于对好友的信任，加上已经视频聊天核实了身份，郭先生没有核实钱款是否到账，就分两笔把430万转到了好友朋友的银行卡上。郭先生拨打好友电话，才知道被骗。骗子通过智能AI换脸和拟声技术，佯装好友对他实施了诈骗。\n",
    "值得注意的是，骗子并没有使用一个仿真的好友微信添加郭先生为好友，而是直接用好友微信发起视频聊天，这也是郭先生被骗的原因之一。骗子极有可能通过技术手段盗用了郭先生好友的微信。幸运的是，接到报警后，福州、包头两地警银迅速启动止付机制，成功止付拦截336.84万元，但仍有93.16万元被转移，目前正在全力追缴中。\n",
    "\"\"\"\n",
    "\n",
    "llm = OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", openai_api_key='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')\n",
    "# llm = OpenAI(temperature=0, openai_api_key='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')\n",
    "\n",
    "text_splitter = CharacterTextSplitter()\n",
    "texts = text_splitter.split_text(chunk)\n",
    "docs = [Document(page_content=t) for t in texts]\n",
    "print(f\"origin len={len(chunk)}, docs={len(docs)}\")\n",
    "\n",
    "with ProxyContext(proxy='http://192.168.1.45:10809'):\n",
    "    chain = load_summarize_chain(llm, chain_type=\"map_reduce\", return_intermediate_steps=True, map_prompt=PROMPT, combine_prompt=PROMPT)\n",
    "    output = chain({\"input_documents\": docs}, return_only_outputs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProxyContext(proxy='http://192.168.1.45:10809'):\n",
    "    chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
    "    output = chain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise summary (around 40 character) of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "CONCISE SUMMARY IN CHINESE:\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary in Chinese\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_template,\n",
    ")\n",
    "with ProxyContext(proxy='http://192.168.1.45:10809'):\n",
    "    chain = load_summarize_chain(llm, chain_type=\"refine\", return_intermediate_steps=True, question_prompt=PROMPT, refine_prompt=refine_prompt)\n",
    "    refine_output = chain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "\n",
    "print(refine_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"973141a098d186b02e0d433340e502e480d4214abc18add43fe0f5d0c1ade3cf\"\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "llm = OpenAI(temperature=0, openai_api_key='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')\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "with ProxyContext(proxy='http://192.168.1.45:10809'):\n",
    "    agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baidu Qianfan\n",
    "\n",
    "* https://docs.llamaindex.ai/en/stable/examples/llm/langchain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For basic init and call\"\"\"\n",
    "import os\n",
    "\n",
    "from langchain_community.llms import QianfanLLMEndpoint\n",
    "\n",
    "os.environ[\"QIANFAN_AK\"] = \"BbTpj7vuUUk7GOt6t6X8XjtO\"\n",
    "os.environ[\"QIANFAN_SK\"] = \"LzTSlYubQ3bapZkfLmqzYvWmw644WpUR\"\n",
    "\n",
    "llm = QianfanLLMEndpoint(model=\"ERNIE-4.0-8K\", streaming=True)\n",
    "# res = llm(\"hi\")\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QianfanLLMEndpoint(client=<qianfan.resources.llm.completion.Completion object at 0x000002AA9A7A1330>, qianfan_ak=SecretStr('**********'), qianfan_sk=SecretStr('**********'), streaming=True, model='ERNIE-4.0-8K')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] [04-26 09:08:09] base.py:406 [t:31004]: retry is not available when stream is enabled\n",
      "[WARNING] [04-26 09:08:09] base.py:621 [t:31004]: This key `messages` does not seem to be a parameter that the model `ERNIE-4.0-8K` will accept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```sql\n",
      "SELECT city_name, population\n",
      "FROM city_stats\n",
      "ORDER BY population DESC\n",
      "LIMIT 1;\n",
      "```\n",
      "SQLResult:\n",
      "```diff\n",
      "+------------+------------+\n",
      "| city_name  | population |\n",
      "+------------+------------+\n",
      "| Metropolis |    1000000 |\n",
      "+------------+------------+\n",
      "```\n",
      "Answer:\n",
      "Metropolis\n"
     ]
    }
   ],
   "source": [
    "query = \"Given an input question, first create a syntactically correct sqlite query to run, then look at the results of the query and return the answer. You can order the results by a relevant column to return the most interesting examples in the database.\\n\\nNever query for all the columns from a specific table, only ask for a few relevant columns given the question.\\n\\nPay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Pay attention to which column is in which table. Also, qualify column names with the table name when needed. You are required to use the following format (1. dont output markdown/html tags (e.g. <b>, </b>, <p>, </p>, ```sql``` etc.); 2. the sql part of your output should be on one line with line breaks, and 3. the answer part should be on one line) to be parsed easily by later parser components, each part taking one line as follows:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\nOnly use tables listed below.\\nTable 'city_stats' has columns: city_name (VARCHAR(16)), population (INTEGER), country (VARCHAR(16)), and foreign keys: .\\n\\nQuestion: Which city has the highest population?\\nSQLQuery: \"\n",
    "\n",
    "\n",
    "res = llm(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\73915\\.conda\\envs\\py10\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "[WARNING] [04-25 00:46:09] base.py:406 [t:27080]: retry is not available when stream is enabled\n",
      "[WARNING] [04-25 00:46:09] base.py:621 [t:27080]: This key `messages` does not seem to be a parameter that the model `ERNIE-4.0-8K` will accept\n",
      "[INFO] [04-25 00:46:09] oauth.py:222 [t:27080]: trying to refresh access_token for ak `BbTpj7***`\n",
      "[INFO] [04-25 00:46:09] oauth.py:237 [t:27080]: sucessfully refresh access_token\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.langchain import LangChainLLM\n",
    "\n",
    "llm = LangChainLLM(llm=llm)\n",
    "\n",
    "response_gen = llm.stream_complete(\"Hi this is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LLMChatStartEvent\nmessages\n  value is not a valid list (type=type_error.list)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the capital of France?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\py10\\lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:129\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[1;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wrapper_logic(_self) \u001b[38;5;28;01mas\u001b[39;00m callback_manager:\n\u001b[0;32m    127\u001b[0m     span_id \u001b[38;5;241m=\u001b[39m dispatcher\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;241m.\u001b[39mcurrent_span_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m     dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m--> 129\u001b[0m         \u001b[43mLLMChatStartEvent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_self\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43madditional_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspan_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspan_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    136\u001b[0m     event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[0;32m    137\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[0;32m    138\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m         },\n\u001b[0;32m    143\u001b[0m     )\n\u001b[0;32m    144\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m f(_self, messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\py10\\lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for LLMChatStartEvent\nmessages\n  value is not a valid list (type=type_error.list)"
     ]
    }
   ],
   "source": [
    "response = llm.chat(\"What is the capital of France?\")\n",
    "print(response)\n",
    "# for w in response_gen:\n",
    "#     print(w, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# awesome projects using langchain\n",
    "\n",
    "## Langchain-chatchat\n",
    "\n",
    "* https://github.com/chatchat-space/Langchain-Chatchat/wiki/\n",
    "* 优点：支持fastchat，文本格式多。支持agent，支持搜索引擎，中文支持好。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_editor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
