{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654b397f6cf74cabbcde450f35c87c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"golaxy/gogpt2-7b\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"golaxy/gogpt2-7b\")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"golaxy/gogpt2-7b\", trust_remote_code=True)\n",
    "model = LlamaForCausalLM.from_pretrained(\"golaxy/gogpt2-7b\",trust_remote_code=True).half().cuda()\n",
    "# model = LlamaForCausalLM.from_pretrained(\"golaxy/gogpt2-7b\",trust_remote_code=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\transformers\\generation\\utils.py:1291: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\code\\learn\\pylearn\\nlp\\llm\\llama.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/learn/pylearn/nlp/llm/llama.ipynb#W4sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m s \u001b[39m=\u001b[39m generation_output[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/learn/pylearn/nlp/llm/llama.ipynb#W4sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m output \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(s, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/code/learn/pylearn/nlp/llm/llama.ipynb#W4sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m response \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m### Response:\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/learn/pylearn/nlp/llm/llama.ipynb#W4sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "prompt_input = (\n",
    "    \"Below is an instruction that describes a task. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n\\n{instruction}\\n\\n### Response:\\n\\n\"\n",
    ")\n",
    "\n",
    "prompt_input = (\n",
    "    \"Below is an instruction that describes a task. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n\\n{instruction}\\n\\n### Response:\\n\\n\"\n",
    ")\n",
    "\n",
    "def generate_prompt(instruction, input=None):\n",
    "    if input:\n",
    "        instruction = instruction + '\\n' + input\n",
    "    return prompt_input.format_map({'instruction': instruction})\n",
    "\n",
    "# raw_input_text = \"写一篇介绍python loguru包的文章，要求越详细越好、给出代码。\"\n",
    "raw_input_text = \"\"\"重写以下文章，要求篇幅更短、内容更吸引人眼球：\n",
    "\n",
    "财联社5月22日讯，据平安包头微信公众号消息，近日，包头警方发布一起利用人工智能（AI）实施电信诈骗的典型案例，福州市某科技公司法人代表郭先生10分钟内被骗430万元。\n",
    "4月20日中午，郭先生的好友突然通过微信视频联系他，自己的朋友在外地竞标，需要430万保证金，且需要公对公账户过账，想要借郭先生公司的账户走账。\n",
    "基于对好友的信任，加上已经视频聊天核实了身份，郭先生没有核实钱款是否到账，就分两笔把430万转到了好友朋友的银行卡上。郭先生拨打好友电话，才知道被骗。骗子通过智能AI换脸和拟声技术，佯装好友对他实施了诈骗。\n",
    "值得注意的是，骗子并没有使用一个仿真的好友微信添加郭先生为好友，而是直接用好友微信发起视频聊天，这也是郭先生被骗的原因之一。骗子极有可能通过技术手段盗用了郭先生好友的微信。幸运的是，接到报警后，福州、包头两地警银迅速启动止付机制，成功止付拦截336.84万元，但仍有93.16万元被转移，目前正在全力追缴中。\n",
    "\"\"\"\n",
    "\n",
    "raw_input_text = \"\"\"给以下文章写一个引人眼球标题：\n",
    "\n",
    "财联社5月22日讯，据平安包头微信公众号消息，近日，包头警方发布一起利用人工智能（AI）实施电信诈骗的典型案例，福州市某科技公司法人代表郭先生10分钟内被骗430万元。\n",
    "4月20日中午，郭先生的好友突然通过微信视频联系他，自己的朋友在外地竞标，需要430万保证金，且需要公对公账户过账，想要借郭先生公司的账户走账。\n",
    "基于对好友的信任，加上已经视频聊天核实了身份，郭先生没有核实钱款是否到账，就分两笔把430万转到了好友朋友的银行卡上。郭先生拨打好友电话，才知道被骗。骗子通过智能AI换脸和拟声技术，佯装好友对他实施了诈骗。\n",
    "值得注意的是，骗子并没有使用一个仿真的好友微信添加郭先生为好友，而是直接用好友微信发起视频聊天，这也是郭先生被骗的原因之一。骗子极有可能通过技术手段盗用了郭先生好友的微信。幸运的是，接到报警后，福州、包头两地警银迅速启动止付机制，成功止付拦截336.84万元，但仍有93.16万元被转移，目前正在全力追缴中。\n",
    "\"\"\"\n",
    "\n",
    "# input_text = generate_prompt(instruction=raw_input_text)\n",
    "input_text = raw_input_text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "# generation_output = model.generate(\n",
    "#     input_ids=inputs[\"input_ids\"].to('cuda'),\n",
    "#     max_new_tokens=2048,\n",
    "#     temperature=0.1,\n",
    "#     do_sample=True,\n",
    "#     top_p=1.0,\n",
    "#     top_k=0,\n",
    "#     repetition_penalty=1.1,\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "generate_kwargs = dict(\n",
    "    input_ids=inputs[\"input_ids\"].to('cuda'),\n",
    "    max_new_tokens=2048,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    top_p=1.0,\n",
    "    top_k=0,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "generation_output =model.generate(**generate_kwargs)\n",
    "s = generation_output[0]\n",
    "output = tokenizer.decode(s, skip_special_tokens=True)\n",
    "response = output.split(\"### Response:\")[1].strip()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'给以下文章写一个引人眼球标题：\\n\\n财联社5月22日讯，据平安包头微信公众号消息，近日，包头警方发布一起利用人工智能（AI）实施电信诈骗的典型案例，福州市某科技公司法人代表郭先生10分钟内被骗430万元。\\n4月20日中午，郭先生的好友突然通过微信视频联系他，自己的朋友在外地竞标，需要430万保证金，且需要公对公账户过账，想要借郭先生公司的账户走账。\\n基于对好友的信任，加上已经视频聊天核实了身份，郭先生没有核实钱款是否到账，就分两笔把430万转到了好友朋友的银行卡上。郭先生拨打好友电话，才知道被骗。骗子通过智能AI换脸和拟声技术，佯装好友对他实施了诈骗。\\n值得注意的是，骗子并没有使用一个仿真的好友微信添加郭先生为好友，而是直接用好友微信发起视频聊天，这也是郭先生被骗的原因之一。骗子极有可能通过技术手段盗用了郭先生好友的微信。幸运的是，接到报警后，福州、包头两地警银迅速启动止付机制，成功止付拦截336.84万元，但仍有93.16万元被转移，目前正在全力追缴中。\\n\\n标题建议：“利用AI技术实施电信诈骗：深圳市企业家被盗430万元”'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tiiuae/falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sequences = pipeline(\n",
    "   \"write me an article about python pathlib package? requirements: it should have codes for the usages. give me more details.:\\n\\n\",\n",
    "    max_length=1000,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: write me an article about python zhon package? requirements: it should have codes for the usages.:\n",
      "Here is an article about Python zhon package which includes information on how to install it, usage, and code examples. The zhon package is an open source software that can be used to simplify data processing, analysis, and visualization. It is written in Python and provides a wide range of functions for data manipulation and statistical computing. This package allows users to work with numerical data to perform complex calculations without the need to write code. With the zhon package, data can be processed and analyzed quickly and efficiently. \n",
      "\n",
      "To install the zhon package, you will need to first install Python on your computer. You can download the zhon package from the internet and save it to a directory on your computer. Once you have downloaded the package, you can install it using the command line interface with pip, the Python package installer. To get the package to run, you will need to create a Python script that imports the zhon package and runs the appropriate functions. Once you have the script set up, you will be able to use the many functions within the zhon package. The documentation provided with the package will show you how to use the functions and examples of how they can be used in practical applications. \n",
      "\n",
      "The zhon package provides many useful functions for working with numerical data, such as numerical methods, matrix calculations, and statistical analysis. It allows users to perform complex analysis and computations without writing code from scratch. With the zhon package, users can simplify data processing, analysis, and visualization. By using the many functions within the zhon package, users can work with numerical data more efficiently and effectively than with raw code. \n",
      "\n",
      "The requirements for using the zhon package include downloading the zhon code from the internet and having Python installed on your computer. The package can be installed using pip, the Python package installer, and the documentation will provide examples of how to use the functions and methods. With the zhon package, numerical data can be manipulated and analyzed quickly and efficiently.\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "   \"write me an article about python zhon package? requirements: it should have codes for the usages.:\",\n",
    "    max_length=4096,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baichuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "帮我写一篇介绍 python pathlib的文章? 要求: 必须包括代码样例和解释，越详细越好:->\n",
      "Python Pathlib\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"baichuan-inc/Baichuan-7B\", trust_remote_code=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"baichuan-inc/Baichuan-7B\", device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.bfloat16,\n",
    ")\n",
    "# inputs = tokenizer('登鹳雀楼->王之涣\\n夜雨寄北->', return_tensors='pt')\n",
    "inputs = tokenizer(\n",
    "    \"帮我写一篇介绍 python pathlib的文章? 要求: 必须包括代码样例和解释，越详细越好:->\", return_tensors=\"pt\"\n",
    ")\n",
    "inputs = inputs.to(\"cuda:1\")\n",
    "pred = model.generate(**inputs, max_new_tokens=640, repetition_penalty=1.1)\n",
    "print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "心情不好应该做什么？\n",
      "今天是2019年7月3日，天气晴朗。我是一个很喜欢记录生活的人，所以我会把每天发生的事情都写下来，这样可以让我在以后的日子里回忆起来的时候不会那么的迷茫和无助！ 今天的心情不是很好，因为昨天晚上没有睡好觉，早上起床之后感觉头昏脑胀，浑身无力，而且还伴随着恶心想吐的感觉，但是又不想吃东西，就一直躺着休息了半天才缓过来一点劲儿...... 我不知道为什么会变成这个样子，也不知道自己是怎么了，可能是因为最近太累了吧[流泪]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"摘要：\n",
    "\n",
    "中国人民银行 国家金融监督管理总局关于调整优化差别化住房信贷政策的通知\n",
    "中国人民银行上海总部，各省、自治区、直辖市、计划单列市分行；国家金融监督管理总局各监管局；各国有商业银行，中国邮政储蓄银行，各股份制商业银行：\n",
    "为落实党中央、国务院决策部署，坚持房子是用来住的、不是用来炒的定位，适应我国房地产市场供求关系发生重大变化的新形势，更好满足刚性和改善性住房需求，促进房地产市场平稳健康发展，中国人民银行、国家金融监督管理总局决定调整优化差别化住房信贷政策。现就有关事项通知如下：\n",
    "一、对于贷款购买商品住房的居民家庭，首套住房商业性个人住房贷款最低首付款比例统一为不低于20%，二套住房商业性个人住房贷款最低首付款比例统一为不低于30%。\n",
    "二、首套住房商业性个人住房贷款利率政策下限按现行规定执行，二套住房商业性个人住房贷款利率政策下限调整为不低于相应期限贷款市场报价利率加20个基点。\n",
    "三、中国人民银行、国家金融监督管理总局各派出机构按照因城施策原则，指导各省级市场利率定价自律机制，根据辖区内各城市房地产市场形势及当地政府调控要求，自主确定辖区内各城市首套和二套住房商业性个人住房贷款最低首付款比例和利率下限。\n",
    "四、银行业金融机构应根据各省级市场利率定价自律机制确定的最低首付款比例和利率下限，结合本机构经营状况、客户风险状况等因素，合理确定每笔贷款的具体首付款比例和利率水平。\n",
    "中国人民银行\n",
    "国家金融监督管理总局\n",
    "2023年8月31日\n",
    "\n",
    "\"\"\"\n",
    "text = \"心情不好应该做什么？\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "generate_kwargs = dict(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_new_tokens=4096,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    top_p=1.0,\n",
    "    top_k=0,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "pred = model.generate(**generate_kwargs)\n",
    "\n",
    "# pred = model.generate(**inputs, max_new_tokens=4096, repetition_penalty=1.1)\n",
    "print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "from yelib.utils.network_util import ProxyContext\n",
    "\n",
    "with ProxyContext():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\", use_fast=False, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\", device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\")\n",
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"世界上第二高的山峰是哪座\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欢迎来到ICONIP2023\n",
      "第30届国际神经信息处理大会(ICONIP2023)将于2023年11月20日至23日在中国长沙举行。ICONIP是亚太神经网络学会(APNNS)的年度旗舰会议。\n",
      "\n",
      "长沙，湖南省省会，是中国长株潭城市群的核心城市之一，也是中部地区第二大城市。长沙是一个以新一代信息技术为核心的经济中心。湖南以辣味著称，拥有丰富的旅游资源，如张家界、凤凰古城、衡山等。其中，张家界的山景尤为壮观，云雾缭绕中，尖峭的砂岩峰林拔地而起，绿意盎然，这正是詹姆斯·卡梅隆电影《阿凡达》的灵感来源。\n",
      "\n",
      "征稿启事(CFP.PDF)\n",
      "ICONIP2023旨在为全球科学家、研究人员、教育工作者、业界专业人士和学生提供一个高水平的国际论坛，展示神经信息处理领域的最新研究成果，探讨新的挑战和趋势。ICONIP2023将邀请世界著名的神经网络理论与应用、计算神经科学、机器学习等领域的专家学者进行演讲。除了常规的技术分会场以及口头和海报报告外，会议日程还包括关于当前感兴趣主题的特别分会场和教程。\n",
      "\n",
      "ICONIP2023设有全体/主旨发言和专题讨论环节，由世界领先的研究者主持;同时颁发优秀论文奖以表彰在本届会议上发表的重要论文。我们热烈欢迎有关聚焦主题创新研究成果的邀约分会场提案、新型新兴研究议题的培训课程及研讨会的建议。请于2023年4月10日前通过电子邮件iconip2023@126.com向会议秘书处提交邀约分会场、培训课程及研讨会提案。欲了解更多信息，请参阅会议网站获取详细资料。\n"
     ]
    }
   ],
   "source": [
    "content = \"\"\"translate the following article into Chinese:\\n\\n\n",
    "Welcome to ICONIP2023\n",
    "The 30th International Conference on Neural Information Processing (ICONIP2023) will be held in Changsha, China, November 20-23, 2023. ICONIP is the annual flagship conference of Asia Pacific Neural Network Society (APNNS).\n",
    "\n",
    "Changsha, the capital of Hunan province, which is the leading city of Changsha-Zhuzhou-Xiangtan city cluster and the second-largest city in central China. Changsha is new information-based economic center. Hunan is noted for its spicy food with copious use of chillies. There is plenty of tourist sites in Hunan, such as Wulingyuan Scenic Area (Zhangjiajie), Feng Huang City, Mount Heng, Shaoshan and so on. Among them, Zhangjiajie has a spectacular mountain view, with jagged sandstone pinnacles rising in the mist and green vegetation at the top, which was the inspiration for James Cameron's film Avatar.\n",
    "\n",
    "Call for Papers (CFP.PDF)\n",
    "ICONIP 2023 aims to provide a high-level international forum for scientists, researchers, educators, industrial professionals, and students worldwide to present state of research results, address new challenges, and discuss trends in neural information processing and applications. ICONIP 2023 will invite world-renowned scholars in the areas of neural network theory and applications, computational neuroscience, machine learning and others. In addition to regular technical sessions with oral and poster presentations, the conference program will include special sessions and tutorials on topics of current interest.\n",
    "\n",
    "ICONIP 2023 features plenary/keynote and panel discussion sessions by the world’s leading researchers as well as awards to honour outstanding papers presented at this conference. Welcomes proposals for invited sessions reporting innovative research results on focused topics and tutorials and workshops on novel emerging research topics. Please submit invited sessions and tutorial/workshop proposals to the conference Secretariat at iconip2023@126.com by April 10, 2023. For more information about the conference, please refer to the conference website for details.\n",
    "\"\"\"\n",
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": content})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 int8 量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 int8 量化 (To use int8 quantization):\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\", use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\", torch_dtype=torch.float16, trust_remote_code=True)\n",
    "model = model.quantize(8).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欢迎来到ICONIP2023年会\n",
      "第30届国际神经信息处理大会(ICONIP2023)将于2023年11月20日至23日在中国长沙举行。ICONIP是亚太神经网络学会(APNNS)的年度旗舰会议。\n",
      "\n",
      "长沙，湖南省省会，是中国长株潭城市群的领军城市和中部地区第二大城市。长沙是一个以新一代信息技术为核心的经济中心。湖南以其丰富的辣椒美食而闻名。湖南拥有许多旅游胜地，如张家界、凤凰古城、衡山等。其中，张家界的壮丽景色尤为引人入胜，悬崖峭壁在雾气中拔地而起，绿意盎然，这正是詹姆斯·卡梅隆电影《阿凡达》的灵感来源。\n",
      "\n",
      "征稿启事(CFP.PDF)\n",
      "ICONIP 2023旨在为全球科学家、研究人员、教育工作者、工业专业人士和学生提供一个高水平的国际论坛，展示神经信息处理领域的最新研究成果，探讨新的挑战和趋势。ICONIP 2023将邀请世界著名的学者，涵盖神经网络理论与应用、计算神经科学、机器学习等相关领域。除了常规的技术研讨会外，会议日程还包括关于当前感兴趣主题的特别研讨会和教程。\n",
      "\n",
      "ICONIP 2023设有由世界领先的研究人员主讲的大会报告/特邀演讲环节以及表彰在本次会议上提交的优秀论文的奖项。同时，也热烈欢迎有关聚焦主题创新研究成果的专题讨论会及新型新兴研究话题的培训班和工作坊的提案。请于2023年4月10日前向会议秘书处发送invited session、培训课程和工作坊的提案至iconip2023@126.com。欲了解更多信息，请参阅会议网站获取详细资料。\n"
     ]
    }
   ],
   "source": [
    "content = \"\"\"translate the following article into Chinese:\\n\\n\n",
    "Welcome to ICONIP2023\n",
    "The 30th International Conference on Neural Information Processing (ICONIP2023) will be held in Changsha, China, November 20-23, 2023. ICONIP is the annual flagship conference of Asia Pacific Neural Network Society (APNNS).\n",
    "\n",
    "Changsha, the capital of Hunan province, which is the leading city of Changsha-Zhuzhou-Xiangtan city cluster and the second-largest city in central China. Changsha is new information-based economic center. Hunan is noted for its spicy food with copious use of chillies. There is plenty of tourist sites in Hunan, such as Wulingyuan Scenic Area (Zhangjiajie), Feng Huang City, Mount Heng, Shaoshan and so on. Among them, Zhangjiajie has a spectacular mountain view, with jagged sandstone pinnacles rising in the mist and green vegetation at the top, which was the inspiration for James Cameron's film Avatar.\n",
    "\n",
    "Call for Papers (CFP.PDF)\n",
    "ICONIP 2023 aims to provide a high-level international forum for scientists, researchers, educators, industrial professionals, and students worldwide to present state of research results, address new challenges, and discuss trends in neural information processing and applications. ICONIP 2023 will invite world-renowned scholars in the areas of neural network theory and applications, computational neuroscience, machine learning and others. In addition to regular technical sessions with oral and poster presentations, the conference program will include special sessions and tutorials on topics of current interest.\n",
    "\n",
    "ICONIP 2023 features plenary/keynote and panel discussion sessions by the world’s leading researchers as well as awards to honour outstanding papers presented at this conference. Welcomes proposals for invited sessions reporting innovative research results on focused topics and tutorials and workshops on novel emerging research topics. Please submit invited sessions and tutorial/workshop proposals to the conference Secretariat at iconip2023@126.com by April 10, 2023. For more information about the conference, please refer to the conference website for details.\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-13B-Chat\", use_fast=False, trust_remote_code=True)\n",
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": content})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tigerBot\n",
    "\n",
    "* https://github.com/TigerResearch/TigerBot\n",
    "* TigerResearch/tigerbot-70b-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966f167c7a5942609c996a5dd7d00b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin c:\\Users\\73915\\.conda\\envs\\py310llama_etuning\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.7\\bin\\cudart64_110.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary c:\\Users\\73915\\.conda\\envs\\py310llama_etuning\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\73915\\.conda\\envs\\py310llama_etuning\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:156: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('C:/Users/73915/.conda/envs/py310llama_etuning/bin')}\n",
      "  warn(msg)\n",
      "c:\\Users\\73915\\.conda\\envs\\py310llama_etuning\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:156: UserWarning: C:\\Users\\73915\\.conda\\envs\\py310llama_etuning did not contain ['cudart64_110.dll', 'cudart64_120.dll', 'cudart64_12.dll'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db7a777f1024b01a8ed8302e0e36d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74990cea5af4407b566fd57d33eb422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e68454663145f092c45c62960ffcea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "We have no connection or you passed local_files_only, so force_download is not an accepted option.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# 下载过旧版的用户需要指定`force_download=True`避免使用旧版缓存\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model_sft \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mAutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mTigerResearch/tigerbot-13b-chat\u001b[39;49m\u001b[39m'\u001b[39;49m, force_download\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      5\u001b[0m model_base \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mAutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mTigerResearch/tigerbot-13b-chat\u001b[39m\u001b[39m'\u001b[39m, force_download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\py310llama_etuning\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:493\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    492\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    494\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    495\u001b[0m     )\n\u001b[0;32m    496\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    497\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    498\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    499\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\py310llama_etuning\\lib\\site-packages\\transformers\\modeling_utils.py:2610\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2607\u001b[0m \u001b[39m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[0;32m   2608\u001b[0m \u001b[39mif\u001b[39;00m is_sharded:\n\u001b[0;32m   2609\u001b[0m     \u001b[39m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[1;32m-> 2610\u001b[0m     resolved_archive_file, sharded_metadata \u001b[39m=\u001b[39m get_checkpoint_shard_files(\n\u001b[0;32m   2611\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   2612\u001b[0m         resolved_archive_file,\n\u001b[0;32m   2613\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   2614\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m   2615\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   2616\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m   2617\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m   2618\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m   2619\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m   2620\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m   2621\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m   2622\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m   2623\u001b[0m     )\n\u001b[0;32m   2625\u001b[0m \u001b[39m# load pt weights early so that we know which dtype to init the model under\u001b[39;00m\n\u001b[0;32m   2626\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\py310llama_etuning\\lib\\site-packages\\transformers\\utils\\hub.py:958\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[1;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, use_auth_token, user_agent, revision, subfolder, _commit_hash)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[39mfor\u001b[39;00m shard_filename \u001b[39min\u001b[39;00m tqdm(shard_filenames, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading shards\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m    956\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    957\u001b[0m         \u001b[39m# Load from URL\u001b[39;00m\n\u001b[1;32m--> 958\u001b[0m         cached_filename \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m    959\u001b[0m             pretrained_model_name_or_path,\n\u001b[0;32m    960\u001b[0m             shard_filename,\n\u001b[0;32m    961\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    962\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    963\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    964\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    965\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    966\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    967\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    968\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    969\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m    970\u001b[0m             _commit_hash\u001b[39m=\u001b[39;49m_commit_hash,\n\u001b[0;32m    971\u001b[0m         )\n\u001b[0;32m    972\u001b[0m     \u001b[39m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[0;32m    973\u001b[0m     \u001b[39m# we don't have to catch them here.\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\py310llama_etuning\\lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    414\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    418\u001b[0m         path_or_repo_id,\n\u001b[0;32m    419\u001b[0m         filename,\n\u001b[0;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    430\u001b[0m     )\n\u001b[0;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    434\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    435\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    436\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    438\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\py310llama_etuning\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\py310llama_etuning\\lib\\site-packages\\huggingface_hub\\file_download.py:1256\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[39mif\u001b[39;00m etag \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[39m# In those cases, we cannot force download.\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m     \u001b[39mif\u001b[39;00m force_download:\n\u001b[1;32m-> 1256\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1257\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mWe have no connection or you passed local_files_only, so force_download is not an accepted option.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1258\u001b[0m         )\n\u001b[0;32m   1260\u001b[0m     \u001b[39m# Try to get \"commit_hash\" from \"revision\"\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m     commit_hash \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: We have no connection or you passed local_files_only, so force_download is not an accepted option."
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "# 下载过旧版的用户需要指定`force_download=True`避免使用旧版缓存\n",
    "model_sft = transformers.AutoModelForCausalLM.from_pretrained('TigerResearch/tigerbot-13b-chat', force_download=True)\n",
    "model_base = transformers.AutoModelForCausalLM.from_pretrained('TigerResearch/tigerbot-13b-chat', force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_editor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
