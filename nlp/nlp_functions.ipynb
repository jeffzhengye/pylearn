{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 内容\n",
    "* 以功能索引记录NLP应用\n",
    "* \n",
    "\n",
    "# 关键词抽取\n",
    "\n",
    "* https://pynlpir.readthedocs.io/en/latest/\n",
    "\n",
    "## pynlpir\n",
    "\n",
    "### install\n",
    "* pip install pynlpir\n",
    "* pynlpir update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('镜像服务', 8.64),\n",
       " ('ChatGPT', 7.44),\n",
       " ('商家', 4.39),\n",
       " ('二道贩子', 4.39),\n",
       " ('提供', 4.39),\n",
       " ('用户', 4.39),\n",
       " ('信息', 4.39),\n",
       " ('镜像', 3.1),\n",
       " ('服务', 3.1),\n",
       " ('网民', 2.4),\n",
       " ('OpenAI', 2.4),\n",
       " ('人工智能', 2.0),\n",
       " ('全球', 2.0),\n",
       " ('范围', 2.0),\n",
       " ('开始', 2.0),\n",
       " ('方式', 2.0),\n",
       " ('借助', 2.0),\n",
       " ('牟利', 2.0),\n",
       " ('充当', 2.0),\n",
       " ('行为', 2.0),\n",
       " ('存在', 2.0),\n",
       " ('隐私', 2.0),\n",
       " ('泄露', 2.0),\n",
       " ('风险', 2.0),\n",
       " ('询问', 2.0),\n",
       " ('涉及', 2.0),\n",
       " ('个人', 2.0),\n",
       " ('企业', 2.0),\n",
       " ('暴露', 2.0),\n",
       " ('公司', 2.0),\n",
       " ('敏感', 1.0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pynlpir\n",
    "pynlpir.open()\n",
    "\n",
    "s = '随着ChatGPT人工智能在全球范围内爆红,一些商家开始以各种方式借助ChatGPT牟利。部分商家充当“二道贩子”为国内网民提供ChatGPT镜像服务,这种行为存在用户隐私泄露的风险。一旦用户通过这些镜像服务询问涉及个人或企业的敏感信息,这些信息将暴露给提供镜像服务的公司和OpenAI。'\n",
    "\n",
    "# pynlpir.segment(s) # 分词\n",
    "\n",
    "pynlpir.get_key_words(s, weighted=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## harvesttext\n",
    "* https://github.com/blmoistawinde/HarvestText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'上海上港足球队': '机构名', '武磊': '人名', '中国': '地名'}\n",
      "武磊威武，中超第一射手！\n",
      "武球王威武，中超最强前锋！\n",
      "\n",
      "Text summarization(避免重复)\n",
      "武磊威武，中超第一射手！\n",
      "郜林看来不行，已经到上限了。\n",
      "武磊和郜林，谁是中国最好的前锋？\n",
      "《关键词》里的关键词\n",
      "jieba_tfidf ['镜像', '服务', '商家', '用户', '爆红']\n",
      "textrank ['服务', '镜像', '商家', '提供', '用户']\n"
     ]
    }
   ],
   "source": [
    "from harvesttext import HarvestText\n",
    "ht = HarvestText()\n",
    "\n",
    "# 命名实体识别\n",
    "sent = \"上海上港足球队的武磊是中国最好的前锋。\"\n",
    "print(ht.named_entity_recognition(sent))\n",
    "\n",
    "# 文本摘要\n",
    "docs = [\"武磊威武，中超第一射手！\",\n",
    "        \"郜林看来不行，已经到上限了。\",\n",
    "        \"武球王威武，中超最强前锋！\",\n",
    "        \"武磊和郜林，谁是中国最好的前锋？\"]\n",
    "for doc in ht.get_summary(docs, topK=2):\n",
    "    print(doc)\n",
    "print(\"\\nText summarization(避免重复)\")\n",
    "for doc in ht.get_summary(docs, topK=3, avoid_repeat=True):\n",
    "    print(doc)\n",
    "\n",
    "# 关键词\n",
    "\n",
    "print(\"《关键词》里的关键词\")\n",
    "kwds = ht.extract_keywords(s, 5, method=\"jieba_tfidf\")\n",
    "print(\"jieba_tfidf\", kwds)\n",
    "kwds = ht.extract_keywords(s, 5, method=\"textrank\")\n",
    "print(\"textrank\", kwds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要\n",
    "\n",
    "## 传统方法\n",
    "\n",
    "### snownlp\n",
    "\n",
    "* https://github.com/isnowfy/snownlp\n",
    "* 可训练\n",
    "* 中文分词（Character-Based Generative Model）\n",
    "词性标注（TnT 3-gram 隐马）\n",
    "情感分析（现在训练数据主要是买卖东西时的评价，所以对其他的一些可能效果不是很好，待解决）\n",
    "文本分类（Naive Bayes）\n",
    "转换成拼音（Trie树实现的最大匹配）\n",
    "繁体转简体（Trie树实现的最大匹配）\n",
    "提取文本关键词（TextRank算法）\n",
    "提取文本摘要（TextRank算法）\n",
    "tf，idf\n",
    "Tokenization（分割成句子）\n",
    "文本相似（BM25）\n",
    "支持python3（感谢erning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e58c2b405b44f5b8ba1ef7076da0540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/477M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611d11dc2a344b0495acfd5c54bb49c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/374k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851d50c4fc5e4ae6850b41475d624f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'滑雪女子坡面障碍技巧决赛谷爱凌获银牌'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"D:/code/nlp/summary/Fengshenbang-LM/fengshen/examples/pegasus/\")\n",
    "sys.path.append(\"D:/code/nlp/summary/Fengshenbang-LM/\")\n",
    "\n",
    "from transformers import PegasusForConditionalGeneration\n",
    "# Need to download tokenizers_pegasus.py and other Python script from Fengshenbang-LM github repo in advance,\n",
    "# or you can download tokenizers_pegasus.py and data_utils.py in https://huggingface.co/IDEA-CCNL/Randeng_Pegasus_523M/tree/main\n",
    "# Strongly recommend you git clone the Fengshenbang-LM repo:\n",
    "# 1. git clone https://github.com/IDEA-CCNL/Fengshenbang-LM\n",
    "# 2. cd Fengshenbang-LM/fengshen/examples/pegasus/\n",
    "# and then you will see the tokenizers_pegasus.py and data_utils.py which are needed by pegasus model\n",
    "\n",
    "\n",
    "from tokenizers_pegasus import PegasusTokenizer\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese\")\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese\")\n",
    "\n",
    "text = \"在北京冬奥会自由式滑雪女子坡面障碍技巧决赛中，中国选手谷爱凌夺得银牌。祝贺谷爱凌！今天上午，自由式滑雪女子坡面障碍技巧决赛举行。决赛分三轮进行，取选手最佳成绩排名决出奖牌。第一跳，中国选手谷爱凌获得69.90分。在12位选手中排名第三。完成动作后，谷爱凌又扮了个鬼脸，甚是可爱。第二轮中，谷爱凌在道具区第三个障碍处失误，落地时摔倒。获得16.98分。网友：摔倒了也没关系，继续加油！在第二跳失误摔倒的情况下，谷爱凌顶住压力，第三跳稳稳发挥，流畅落地！获得86.23分！此轮比赛，共12位选手参赛，谷爱凌第10位出场。网友：看比赛时我比谷爱凌紧张，加油！\"\n",
    "inputs = tokenizer(text, max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "# model Output: 滑雪女子坡面障碍技巧决赛谷爱凌获银牌\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\73915\\.conda\\envs\\py38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2382: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\73915\\.conda\\envs\\py38\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (256) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'反垄断调查小组突击查访奔驰上海办事处，对多名奔驰高管进行约谈'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration\n",
    "# Need to download tokenizers_pegasus.py and other Python script from Fengshenbang-LM github repo in advance,\n",
    "# or you can download tokenizers_pegasus.py and data_utils.py in https://huggingface.co/IDEA-CCNL/Randeng_Pegasus_523M/tree/main\n",
    "# Strongly recommend you git clone the Fengshenbang-LM repo:\n",
    "# 1. git clone https://github.com/IDEA-CCNL/Fengshenbang-LM\n",
    "# 2. cd Fengshenbang-LM/fengshen/examples/pegasus/\n",
    "# and then you will see the tokenizers_pegasus.py and data_utils.py which are needed by pegasus model\n",
    "from tokenizers_pegasus import PegasusTokenizer\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese\")\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese\")\n",
    "\n",
    "text = \"据微信公众号“界面”报道，4日上午10点左右，中国发改委反垄断调查小组突击查访奔驰上海办事处，调取数据材料，并对多名奔驰高管进行了约谈。截止昨日晚9点，包括北京梅赛德斯-奔驰销售服务有限公司东区总经理在内的多名管理人员仍留在上海办公室内\"\n",
    "inputs = tokenizer(text, max_length=1024, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "# model Output: 反垄断调查小组突击查访奔驰上海办事处，对多名奔驰高管进行约谈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "财联社5月22日讯，据平安包头微信公众号消息，近日，包头警方发布一起利用人工智能（AI）实施电信诈骗的典型案例，福州市某科技公司法人代表郭先生10分钟内被骗430万元。\n",
      "4月20日中午，郭先生的好友突然通过微信视频联系他，自己的朋友在外地竞标，需要430万保证金，且需要公对公账户过账，想要借郭先生公司的账户走账。\n",
      "基于对好友的信任，加上已经视频聊天核实了身份，郭先生没有核实钱款是否到账，就分两笔把430万转到了好友朋友的银行卡上。\n",
      "郭先生拨打好友电话，才知道被骗。\n",
      "骗子通过智能AI换脸和拟声技术，佯装好友对他实施了诈骗。\n",
      "值得注意的是，骗子并没有使用一个仿真的好友微信添加郭先生为好友，而是直接用好友微信发起视频聊天，这也是郭先生被骗的原因之一。\n",
      "骗子极有可能通过技术手段盗用了郭先生好友的微信。\n",
      "幸运的是，接到报警后，福州、包头两地警银迅速启动止付机制，成功止付拦截336.84万元，但仍有93.16万元被转移，目前正在全力追缴中。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\73915\\.conda\\envs\\py38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2382: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\73915\\.conda\\envs\\py38\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (256) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['利用人工智能实施电信诈骗案例：郭先生10 分钟被骗430万',\n",
       " '郭先生被好友诈骗430万保证金要公对公账户过账',\n",
       " '银行卡里的钱都去哪儿了？',\n",
       " '郭先生拨打好友电话才知道被骗',\n",
       " '骗子通过智能ai换脸和拟声技术佯装好友诈骗',\n",
       " '骗子冒充微信好友发视频敲诈郭先生',\n",
       " '骗子可能通过技术手段盗用郭先生微信',\n",
       " '即时：福州包头警银成功止付拦截336.84 万元']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"在北京冬奥会自由式滑雪女子坡面障碍技巧决赛中，中国选手谷爱凌夺得银牌。祝贺谷爱凌！今天上午，自由式滑雪女子坡面障碍技巧决赛举行。决赛分三轮进行，取选手最佳成绩排名决出奖牌。第一跳，中国选手谷爱凌获得69.90分。在12位选手中排名第三。完成动作后，谷爱凌又扮了个鬼脸，甚是可爱。第二轮中，谷爱凌在道具区第三个障碍处失误，落地时摔倒。获得16.98分。\"\n",
    "\n",
    "text1 = \"网友：摔倒了也没关系，继续加油！在第二跳失误摔倒的情况下，谷爱凌顶住压力，第三跳稳稳发挥，流畅落地！获得86.23分！此轮比赛，共12位选手参赛，谷爱凌第10位出场。网友：看比赛时我比谷爱凌紧张，加油！\"\n",
    "\n",
    "chunk = \"\"\"\n",
    "财联社5月22日讯，据平安包头微信公众号消息，近日，包头警方发布一起利用人工智能（AI）实施电信诈骗的典型案例，福州市某科技公司法人代表郭先生10分钟内被骗430万元。\n",
    "4月20日中午，郭先生的好友突然通过微信视频联系他，自己的朋友在外地竞标，需要430万保证金，且需要公对公账户过账，想要借郭先生公司的账户走账。\n",
    "基于对好友的信任，加上已经视频聊天核实了身份，郭先生没有核实钱款是否到账，就分两笔把430万转到了好友朋友的银行卡上。郭先生拨打好友电话，才知道被骗。骗子通过智能AI换脸和拟声技术，佯装好友对他实施了诈骗。\n",
    "值得注意的是，骗子并没有使用一个仿真的好友微信添加郭先生为好友，而是直接用好友微信发起视频聊天，这也是郭先生被骗的原因之一。骗子极有可能通过技术手段盗用了郭先生好友的微信。幸运的是，接到报警后，福州、包头两地警银迅速启动止付机制，成功止付拦截336.84万元，但仍有93.16万元被转移，目前正在全力追缴中。\n",
    "\"\"\"\n",
    "\n",
    "from ltp import StnSplit\n",
    "sents_splitter = StnSplit()\n",
    "\n",
    "sentences = sents_splitter.split(chunk)\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "    \n",
    "print()\n",
    "\n",
    "inputs = tokenizer(sentences, max_length=1024, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# print(inputs)\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c9437c5ed04a57ab9a657d7b916d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/751 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6574d46996684a248ecc02f49ad7b277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79a59134d774daab50549651fec79c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e70e517b8e4bb282e1285b057cf4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7691a23afc0448cc9f85f9e316010580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "包头警方发布一起利用AI实施电信诈骗典型案例:法人代表10分钟内被骗430万元\n"
     ]
    }
   ],
   "source": [
    "# heack/HeackMT5-ZhSum100k\n",
    "\n",
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"heack/HeackMT5-ZhSum100k\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"heack/HeackMT5-ZhSum100k\")\n",
    "\n",
    "chunk = \"\"\"\n",
    "财联社5月22日讯，据平安包头微信公众号消息，近日，包头警方发布一起利用人工智能（AI）实施电信诈骗的典型案例，福州市某科技公司法人代表郭先生10分钟内被骗430万元。\n",
    "4月20日中午，郭先生的好友突然通过微信视频联系他，自己的朋友在外地竞标，需要430万保证金，且需要公对公账户过账，想要借郭先生公司的账户走账。\n",
    "基于对好友的信任，加上已经视频聊天核实了身份，郭先生没有核实钱款是否到账，就分两笔把430万转到了好友朋友的银行卡上。郭先生拨打好友电话，才知道被骗。骗子通过智能AI换脸和拟声技术，佯装好友对他实施了诈骗。\n",
    "值得注意的是，骗子并没有使用一个仿真的好友微信添加郭先生为好友，而是直接用好友微信发起视频聊天，这也是郭先生被骗的原因之一。骗子极有可能通过技术手段盗用了郭先生好友的微信。幸运的是，接到报警后，福州、包头两地警银迅速启动止付机制，成功止付拦截336.84万元，但仍有93.16万元被转移，目前正在全力追缴中。\n",
    "\"\"\"\n",
    "inputs = tokenizer.encode(\"summarize: \" + chunk, return_tensors='pt', max_length=512, truncation=True)\n",
    "summary_ids = model.generate(inputs, max_length=150, num_beams=4, length_penalty=1.5, no_repeat_ngram_size=2)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量化过程:从GP16到INT4 量化后的内存\n"
     ]
    }
   ],
   "source": [
    "chunk = \"\"\"\n",
    "模型量化会带来一定的性能损失，经过测试，ChatGLM-6B 在 4-bit 量化下仍然能够进行自然流畅的生成。使用 GPT-Q 等量化方案可以进一步压缩量化精度/提升相同量化精度下的模型性能，欢迎大家提出对应的 Pull Request。\n",
    "\"\"\"\n",
    "\n",
    "chunk1 = \"\"\"\n",
    "量化过程需要在内存中首先加载 FP16 格式的模型，消耗大概 13GB 的内存。如果你的内存不足的话，可以直接加载量化后的模型，INT4 量化后的模型仅需大概 5.2GB 的内存：\"\"\"\n",
    "\n",
    "# chunk1 = \"\"\"\n",
    "# 该塔高324米（1063英尺），与一幢81层的建筑物一样高，是巴黎最高的建筑物。 它的底座是方形的，每边长125米（410英尺）。 在建造过程中，艾菲尔铁塔超过了华盛顿纪念碑，成为世界上最高的人造结构，它保持了41年的头衔，直到1930年纽约市的克莱斯勒大楼竣工。这是第一个到达300米高度的结构。 由于1957年在塔顶增加了广播天线，因此它现在比克莱斯勒大厦高5.2米（17英尺）。 除发射器外，艾菲尔铁塔是法国第二高的独立式建筑，仅次于米劳高架桥。\"\"\"\n",
    "\n",
    "inputs = tokenizer.encode(\"summarize: \" + chunk1, return_tensors='pt', max_length=512, truncation=True)\n",
    "summary_ids = model.generate(inputs, max_length=250, num_beams=1, length_penalty=2, no_repeat_ngram_size=4)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modelscope Multilingual-GLM-Summarization-zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 09:58:42,489 - modelscope - INFO - Use user-specified model revision: v1.0.1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import modelscope.models.nlp.mglm.mglm_for_text_summarization because of the following error (look up to see its traceback):\nNo module named 'deepspeed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\modelscope\\utils\\import_utils.py:438\u001b[0m, in \u001b[0;36mLazyImportModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m    437\u001b[0m         requires(module_name_full, requirements)\n\u001b[1;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m    439\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:843\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\modelscope\\models\\nlp\\mglm\\mglm_for_text_summarization.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodelscope\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmegatron_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m init_megatron_util\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39marguments\u001b[39;00m \u001b[39mimport\u001b[39;00m get_args\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgeneration_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m BeamSearchScorer\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\modelscope\\models\\nlp\\mglm\\arguments.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdeepspeed\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'deepspeed'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\code\\learn\\pylearn\\nlp\\nlp_functions.ipynb Cell 12\u001b[0m line \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/code/learn/pylearn/nlp/nlp_functions.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mZhipuAI/Multilingual-GLM-Summarization-zh\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/code/learn/pylearn/nlp/nlp_functions.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m preprocessor \u001b[39m=\u001b[39m MGLMSummarizationPreprocessor()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/code/learn/pylearn/nlp/nlp_functions.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/code/learn/pylearn/nlp/nlp_functions.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     task\u001b[39m=\u001b[39;49mTasks\u001b[39m.\u001b[39;49mtext_summarization,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/learn/pylearn/nlp/nlp_functions.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/learn/pylearn/nlp/nlp_functions.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     preprocessor\u001b[39m=\u001b[39;49mpreprocessor,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/learn/pylearn/nlp/nlp_functions.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     model_revision\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mv1.0.1\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/learn/pylearn/nlp/nlp_functions.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/learn/pylearn/nlp/nlp_functions.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m result \u001b[39m=\u001b[39m pipe(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/learn/pylearn/nlp/nlp_functions.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m据中国载人航天工程办公室消息，北京时间2022年10月25日，梦天实验舱与长征五号B遥四运载火箭组合体已转运至发射区。后续将按计划开展发射前各项功能检查和联合测试等工作，计划于近日择机实施发射。目前，文昌航天发射场设施设备状态良好，参试各单位正在加紧开展任务准备，全力以赴确保空间站建造任务决战决胜。\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/learn/pylearn/nlp/nlp_functions.ipynb#X35sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/learn/pylearn/nlp/nlp_functions.ipynb#X35sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\modelscope\\pipelines\\builder.py:159\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, preprocessor, config_file, pipeline_name, framework, device, model_revision, ignore_file_pattern, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    157\u001b[0m     cfg\u001b[39m.\u001b[39mpreprocessor \u001b[39m=\u001b[39m preprocessor\n\u001b[1;32m--> 159\u001b[0m \u001b[39mreturn\u001b[39;00m build_pipeline(cfg, task_name\u001b[39m=\u001b[39;49mtask)\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\modelscope\\pipelines\\builder.py:65\u001b[0m, in \u001b[0;36mbuild_pipeline\u001b[1;34m(cfg, task_name, default_args)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_pipeline\u001b[39m(cfg: ConfigDict,\n\u001b[0;32m     55\u001b[0m                    task_name: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     56\u001b[0m                    default_args: \u001b[39mdict\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     57\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" build pipeline given model config dict.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[0;32m     59\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39m        default_args (dict, optional): Default initialization arguments.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m build_from_cfg(\n\u001b[0;32m     66\u001b[0m         cfg, PIPELINES, group_key\u001b[39m=\u001b[39;49mtask_name, default_args\u001b[39m=\u001b[39;49mdefault_args)\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\modelscope\\utils\\registry.py:184\u001b[0m, in \u001b[0;36mbuild_from_cfg\u001b[1;34m(cfg, registry, group_key, default_args)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodelscope\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimport_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyImportModule\n\u001b[0;32m    183\u001b[0m sig \u001b[39m=\u001b[39m (registry\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mupper(), group_key, cfg[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 184\u001b[0m LazyImportModule\u001b[39m.\u001b[39;49mimport_module(sig)\n\u001b[0;32m    186\u001b[0m args \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    187\u001b[0m \u001b[39mif\u001b[39;00m default_args \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\modelscope\\utils\\import_utils.py:462\u001b[0m, in \u001b[0;36mLazyImportModule.import_module\u001b[1;34m(signature)\u001b[0m\n\u001b[0;32m    459\u001b[0m         requirements \u001b[39m=\u001b[39m LazyImportModule\u001b[39m.\u001b[39mAST_INDEX[REQUIREMENT_KEY][\n\u001b[0;32m    460\u001b[0m             module_name]\n\u001b[0;32m    461\u001b[0m         requires(module_name, requirements)\n\u001b[1;32m--> 462\u001b[0m     importlib\u001b[39m.\u001b[39;49mimport_module(module_name)\n\u001b[0;32m    463\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msignature\u001b[39m}\u001b[39;00m\u001b[39m not found in ast index file\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:843\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\modelscope\\pipelines\\nlp\\mglm_text_summarization_pipeline.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodelscope\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetainfo\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipelines\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodelscope\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodelscope\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnlp\u001b[39;00m \u001b[39mimport\u001b[39;00m MGLMForTextSummarization\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodelscope\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipelines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipeline, Tensor\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodelscope\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipelines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbuilder\u001b[39;00m \u001b[39mimport\u001b[39;00m PIPELINES\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1039\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\modelscope\\utils\\import_utils.py:422\u001b[0m, in \u001b[0;36mLazyImportModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    421\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module[name])\n\u001b[1;32m--> 422\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(module, name)\n\u001b[0;32m    423\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\modelscope\\utils\\import_utils.py:421\u001b[0m, in \u001b[0;36mLazyImportModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    419\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[0;32m    420\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m--> 421\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[0;32m    422\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[0;32m    423\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\modelscope\\utils\\import_utils.py:440\u001b[0m, in \u001b[0;36mLazyImportModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    439\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    441\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    442\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(look up to see its traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import modelscope.models.nlp.mglm.mglm_for_text_summarization because of the following error (look up to see its traceback):\nNo module named 'deepspeed'"
     ]
    }
   ],
   "source": [
    "from modelscope.models import Model\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.preprocessors import MGLMSummarizationPreprocessor\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "model = 'ZhipuAI/Multilingual-GLM-Summarization-zh'\n",
    "preprocessor = MGLMSummarizationPreprocessor()\n",
    "pipe = pipeline(\n",
    "    task=Tasks.text_summarization,\n",
    "    model=model,\n",
    "    preprocessor=preprocessor,\n",
    "    model_revision='v1.0.1',\n",
    ")\n",
    "result = pipe(\n",
    "    '据中国载人航天工程办公室消息，北京时间2022年10月25日，梦天实验舱与长征五号B遥四运载火箭组合体已转运至发射区。后续将按计划开展发射前各项功能检查和联合测试等工作，计划于近日择机实施发射。目前，文昌航天发射场设施设备状态良好，参试各单位正在加紧开展任务准备，全力以赴确保空间站建造任务决战决胜。'\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 翻译\n",
    "\n",
    "## Helsinki-NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\73915\\.conda\\envs\\py38\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import (AutoModelForSeq2SeqLM, AutoTokenizer, pipeline)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "zh2en_pipe = pipeline(\"translation_zh_to_en\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Hello.'}, {'translation_text': 'Yeah, me too.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['你好', '我也好']\n",
    "\n",
    "# translated = zh2en_pipe(text, max_length=512)[0][\"translation_text\"]\n",
    "translated = zh2en_pipe(text, max_length=512)\n",
    "translated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词，分句\n",
    "\n",
    "## LTP\n",
    "\n",
    "* docs: http://ltp.ai/docs/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['他', '叫', '汤姆', '去', '拿', '外衣', '。']]\n",
      "[['r', 'v', 'nh', 'v', 'v', 'n', 'wp']]\n",
      "[{'head': [2, 0, 2, 2, 4, 5, 2], 'label': ['AGT', 'Root', 'DATV', 'eSUCC', 'eSUCC', 'PAT', 'mPUNC']}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73253ceee1f94f8db639ff2b24a379e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5314c32c1c5f499a92556f7cb1425d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading cws_model.bin:   0%|          | 0.00/32.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a8c3fde8624a0d8c6ff5bb7a8b88ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pos_model.bin:   0%|          | 0.00/76.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc032c23bb1f4ef2870726497ba20602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ner_model.bin:   0%|          | 0.00/5.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['他', '叫', '汤姆', '去', '拿', '外衣', '。']] [['r', 'v', 'nh', 'v', 'v', 'n', 'wp']] [[('Nh', '汤姆')]]\n"
     ]
    }
   ],
   "source": [
    "# 基础使用样例。\n",
    "import torch\n",
    "from ltp import LTP\n",
    "\n",
    "ltp = LTP(\"LTP/small\")  # 默认加载 Small 模型\n",
    "\n",
    "# 将模型移动到 GPU 上\n",
    "if torch.cuda.is_available():\n",
    "    # ltp.cuda()\n",
    "    ltp.to(\"cuda\")\n",
    "\n",
    "output = ltp.pipeline([\"他叫汤姆去拿外衣。\"], tasks=[\"cws\", \"pos\", \"ner\", \"srl\", \"dep\", \"sdp\"])\n",
    "# 使用字典格式作为返回结果\n",
    "print(output.cws)  # print(output[0]) / print(output['cws']) # 也可以使用下标访问\n",
    "print(output.pos)\n",
    "print(output.sdp)\n",
    "\n",
    "# 使用感知机算法实现的分词、词性和命名实体识别，速度比较快，但是精度略低\n",
    "ltp = LTP(\"LTP/legacy\")\n",
    "# cws, pos, ner = ltp.pipeline([\"他叫汤姆去拿外衣。\"], tasks=[\"cws\", \"ner\"]).to_tuple() # error: NER 需要 词性标注任务的结果\n",
    "cws, pos, ner = ltp.pipeline([\"他叫汤姆去拿外衣。\"], tasks=[\"cws\", \"pos\", \"ner\"]).to_tuple()  # to tuple 可以自动转换为元组格式\n",
    "# 使用元组格式作为返回结果\n",
    "print(cws, pos, ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences: \n",
      " ['汤姆生病了.', '他去了医院。', '特斯拉CEO马斯克发声：hello world.']\n",
      "LTPOutput(cws=[['汤姆', '生病', '了', '.'], ['他', '去', '了', '医院', '。'], ['特斯拉', 'CEO', '马斯克', '发声', '：', 'hello world', '.']], pos=None, ner=None, srl=None, dep=None, sdp=None, sdpg=None)\n"
     ]
    }
   ],
   "source": [
    "# 分句样例\n",
    "from ltp import LTP\n",
    "\n",
    "ltp = LTP()\n",
    "\n",
    "from ltp import StnSplit\n",
    "sents = StnSplit().split(\"汤姆生病了.他去了医院。特斯拉CEO马斯克发声：hello world。\")\n",
    "\n",
    "# sents = StnSplit().batch_split([\"他叫汤姆去拿外衣。\", \"汤姆生病了。他去了医院。\"])\n",
    "print('sentences: \\n', sents)\n",
    "\n",
    "# 问题：hello world无法分开，对英文无法分词\n",
    "words = ltp.pipeline(sents, tasks = [\"cws\"], return_dict=True)\n",
    "print(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paddle Mode: 汤姆/生病/了/.\n",
      "Paddle Mode: 他/去/了/医院/。\n",
      "Paddle Mode: 特斯拉CEO马斯克发声：hello world。\n",
      "['特斯拉', 'CEO', '马斯克', '发声', '：', 'hello', ' ', 'world', '。']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from ltp import StnSplit\n",
    "sents = StnSplit().split(\"汤姆生病了.他去了医院。特斯拉CEO马斯克发声：hello world。\")\n",
    "import paddle\n",
    "\n",
    "paddle.enable_static()\n",
    "jieba.enable_paddle()# 启动paddle模式。 0.40版之后开始支持，早期版本不支持\n",
    "\n",
    "for str in sents:\n",
    "    seg_list = jieba.cut(str, use_paddle=True) # 使用paddle模式\n",
    "    print(\"Paddle Mode: \" + '/'.join(list(seg_list)))\n",
    "\n",
    "seg_list = jieba.cut(\"特斯拉CEO马斯克发声：hello world。\", cut_all=False)\n",
    "# print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n",
    "print(list(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutword\n",
    "\n",
    "* 分词速度是jieba的两倍。基于Cython优化，运行速度快\n",
    "* https://github.com/liwenju0/cutword\n",
    "    * jieba不维护了，所以有了cutword。\n",
    "    * "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则分句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['汤姆生病了, 所以去医院. 他去了医院。', '特斯拉CEO马斯克发声：hello world。']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "resentencesp = re.compile('([﹒﹔﹖﹗．；。！？][\"’”」』]{0,2}|：(?=[\"‘“「『]{1,2}|$))')\n",
    "def splitsentence(sentence):\n",
    "    s = sentence\n",
    "    slist = []\n",
    "    for i in resentencesp.split(s):\n",
    "        if resentencesp.match(i) and slist:\n",
    "            slist[-1] += i\n",
    "        elif i:\n",
    "            slist.append(i)\n",
    "    return slist\n",
    "\n",
    "print(splitsentence(\"汤姆生病了, 所以去医院。他去了医院。特斯拉CEO马斯克发声：hello world。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['汤姆生病了, 所以去医院。', '他去了医院。', '特斯拉CEO马斯克发声：hello world。']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: DeprecationWarning: invalid escape sequence \\?\n",
      "<>:5: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:7: DeprecationWarning: invalid escape sequence \\?\n",
      "<>:4: DeprecationWarning: invalid escape sequence \\?\n",
      "<>:5: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:7: DeprecationWarning: invalid escape sequence \\?\n",
      "C:\\Users\\73915\\AppData\\Local\\Temp\\ipykernel_44104\\3978118029.py:4: DeprecationWarning: invalid escape sequence \\?\n",
      "  para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)  # 单字符断句符\n",
      "C:\\Users\\73915\\AppData\\Local\\Temp\\ipykernel_44104\\3978118029.py:5: DeprecationWarning: invalid escape sequence \\.\n",
      "  para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para)  # 英文省略号\n",
      "C:\\Users\\73915\\AppData\\Local\\Temp\\ipykernel_44104\\3978118029.py:7: DeprecationWarning: invalid escape sequence \\?\n",
      "  para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)\n"
     ]
    }
   ],
   "source": [
    "# 版本为python3，如果为python2需要在字符串前面加上u\n",
    "import re\n",
    "def cut_sent(para):\n",
    "    para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)  # 单字符断句符\n",
    "    para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para)  # 英文省略号\n",
    "    para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  # 中文省略号\n",
    "    para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)\n",
    "    # 如果双引号前有终止符，那么双引号才是句子的终点，\n",
    "    #把分句符\\n放到双引号后，注意前面的几句都小心保留了双引号\n",
    "    para = para.rstrip()  # 段尾如果有多余的\\n就去掉它\n",
    "    # 很多规则中会考虑分号;，但是这里我把它忽略不计，破折号、英文双引号等同样忽略，\n",
    "    #需要的再做些简单调整即可。\n",
    "    return para.split(\"\\n\")\n",
    "\n",
    "print(cut_sent(\"汤姆生病了, 所以去医院。他去了医院。特斯拉CEO马斯克发声：hello world。\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zhon 分句\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['妈妈做的菜，很好吃！']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zhon\n",
    "import re\n",
    "\n",
    "# 效果较差\n",
    "re.findall(zhon.hanzi.sentence, '我买了一辆车.妈妈做的菜，很好吃！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## en\n",
    "\n",
    "### nltk\n",
    "\n",
    "* from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# insert whitespace between CJK\n",
    "\n",
    "* Paranoid text spacing for good readability, to automatically insert whitespace between CJK (Chinese, Japanese, Korean) and half-width characters (alphabetical letters, numerical digits and symbols).\n",
    "\n",
    "* pip install pangu\n",
    "* java, go, python\n",
    "* https://pypi.org/project/pangu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Mr. 龍島主道：「Let's Party! 各位高明博雅君子！」\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pangu\n",
    "\n",
    "pangu.spacing('新八的構造成分有95%是眼鏡、3%是水、2%是垃圾')\n",
    "# output: u'新八的構造成分有 95% 是眼鏡、3% 是水、2% 是垃圾'\n",
    "\n",
    "pangu.spacing_text(\"Mr.龍島主道：「Let's Party!各位高明博雅君子！」\")\n",
    "# output: u\"Mr. 龍島主道：「Let's Party! 各位高明博雅君子！」\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fuzzy matching\n",
    "\n",
    "## rapidfuzz \n",
    "\n",
    "* https://github.com/maxbachmann/RapidFuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.55172413793103\n",
      "80.0\n",
      "50.0\n"
     ]
    }
   ],
   "source": [
    "from rapidfuzz import fuzz\n",
    "print(fuzz.ratio(\"this is a test\", \"this is a test!\"))\n",
    "\n",
    "print(fuzz.ratio(\"叶梓红\", \"叶红\"))\n",
    "\n",
    "print(fuzz.ratio(\"代办事项\", \"代办服务\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时间语义解析\n",
    "\n",
    "## JioNLP\n",
    "\n",
    "* JioNLP: https://github.com/dongrixinyu/JioNLP/wiki/%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E8%A7%A3%E6%9E%90-%E8%AF%B4%E6%98%8E%E6%96%87%E6%A1%A3#user-content-%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E8%A7%A3%E6%9E%90\n",
    "\n",
    "## duckling\n",
    "\n",
    "* \n",
    "\n",
    "### Duckling, int() argument must be a string, a bytes-like object or a number, not 'java.lang.String',\n",
    "\n",
    "* pip install --force-reinstall JPype1==0.6.3\n",
    "* https://stackoverflow.com/questions/64841426/duckling-int-argument-must-be-a-string-a-bytes-like-object-or-a-number-not\n",
    "\n",
    "### duckling chinese\n",
    "\n",
    "* https://duckling.leovan.tech/dimension/\n",
    "* pip install duckling-chinese \n",
    "* 中文 Duckling 利用 Jpype1 调用 duckling-fork-chinese，需要系统正确安装配置 JVM，再通过 pip 安装最新版本。\n",
    "\n",
    "## Chronyk\n",
    "\n",
    "* https://www.toutiao.com/article/7351818121822356018/?log_from=21f55ba53ae76_1711980587028\n",
    "\n",
    "## ply\n",
    "\n",
    "* 词法分析：支持定义正则表达式进行词法分析。语法分析：支持基于上下文无关文法的语法分析。\n",
    "* https://mp.weixin.qq.com/s/iOPKx5Q_dMROXjSD1JRzeA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=<class 'jpype._jclass.java.lang.Long'>, toString type=<class 'str'>, value=3\n",
      "type=<class 'jpype._jclass.java.lang.Long'>, toString type=<class 'str'>, value=3\n",
      "{'dim': 'number', 'text': '三', 'start': 4, 'end': 5, 'value': {'value': 3.0}}\n",
      "{'dim': 'temperature', 'text': '三', 'start': 4, 'end': 5, 'value': {'value': 3.0, 'unit': None}}\n",
      "{'dim': 'time', 'text': '明天下午三点', 'start': 0, 'end': 6, 'value': {'value': '2024-06-26T15:00:00.000+08:00', 'grain': 'hour', 'others': [{'grain': 'hour', 'value': '2024-06-26T15:00:00.000+08:00'}]}}\n",
      "type=<class 'jpype._jclass.java.lang.Long'>, toString type=<class 'str'>, value=25\n",
      "[{'dim': 'number', 'text': '25', 'start': 3, 'end': 5, 'value': {'value': 25.0}}]\n",
      "type=<class 'jpype._jclass.java.lang.Long'>, toString type=<class 'str'>, value=2\n",
      "type=<class 'jpype._jclass.java.lang.Integer'>, toString type=<class 'str'>, value=7200\n",
      "type=<class 'jpype._jclass.java.lang.Long'>, toString type=<class 'str'>, value=2\n",
      "type=<class 'jpype._jclass.java.lang.Integer'>, toString type=<class 'str'>, value=7200\n",
      "[{'dim': 'duration', 'text': '两个小时', 'start': 5, 'end': 9, 'value': {'value': 2.0, 'unit': 'hour', 'year': None, 'month': None, 'day': None, 'hour': 2, 'minute': None, 'second': None}}, {'dim': 'duration', 'text': '两个小时', 'start': 5, 'end': 9, 'value': {'value': 2.0, 'unit': 'hour', 'year': None, 'month': None, 'day': None, 'hour': 2, 'minute': None, 'second': None}}]\n"
     ]
    }
   ],
   "source": [
    "from duckling import DucklingWrapper, Language\n",
    "\n",
    "# 初始化Duckling实例\n",
    "duckling = DucklingWrapper(language=Language.CHINESE)\n",
    "\n",
    "# # 解析文本\n",
    "text = \"明天下午三点开会\"\n",
    "results = duckling.parse(text)\n",
    "\n",
    "# # 打印解析结果\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "print(duckling.parse_number(\"我今年25岁了\"))\n",
    "print(duckling.parse_duration(\"我今天跑了两个小时\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=<class 'jpype._jclass.java.lang.Long'>, toString type=<class 'str'>, value=2\n",
      "type=<class 'jpype._jclass.java.lang.Integer'>, toString type=<class 'str'>, value=7200\n",
      "type=<class 'jpype._jclass.java.lang.Long'>, toString type=<class 'str'>, value=2\n",
      "type=<class 'jpype._jclass.java.lang.Integer'>, toString type=<class 'str'>, value=7200\n",
      "[{'dim': 'duration', 'text': '两个小时', 'start': 5, 'end': 9, 'value': {'value': 2.0, 'unit': 'hour', 'year': None, 'month': None, 'day': None, 'hour': 2, 'minute': None, 'second': None}}, {'dim': 'duration', 'text': '两个小时', 'start': 5, 'end': 9, 'value': {'value': 2.0, 'unit': 'hour', 'year': None, 'month': None, 'day': None, 'hour': 2, 'minute': None, 'second': None}}]\n",
      "type=<class 'jpype._jclass.java.lang.Long'>, toString type=<class 'str'>, value=25\n",
      "[{'dim': 'number', 'text': '25', 'start': 1, 'end': 3, 'value': {'value': 25.0}}]\n",
      "[{'dim': 'ordinal', 'text': '第一', 'start': 2, 'end': 4, 'value': {'value': 1}}, {'dim': 'ordinal', 'text': '第二', 'start': 7, 'end': 9, 'value': {'value': 2}}]\n",
      "type=<class 'jpype._jclass.java.lang.Long'>, toString type=<class 'str'>, value=25\n",
      "[{'dim': 'temperature', 'text': '25摄氏度', 'start': 9, 'end': 14, 'value': {'value': 25.0, 'unit': 'celsius'}}]\n",
      "[{'dim': 'time', 'text': '十一点半', 'start': 2, 'end': 6, 'value': {'value': '2024-06-25T23:30:00.000+08:00', 'grain': 'minute', 'others': [{'grain': 'minute', 'value': '2024-06-25T23:30:00.000+08:00'}, {'grain': 'minute', 'value': '2024-06-26T11:30:00.000+08:00'}, {'grain': 'minute', 'value': '2024-06-26T23:30:00.000+08:00'}]}}]\n",
      "[{'dim': 'timezone', 'text': 'UTC', 'start': 9, 'end': 12, 'value': {'value': 'UTC'}}]\n"
     ]
    }
   ],
   "source": [
    "d = duckling\n",
    "print(d.parse_duration(\"我今天跑了两个小时\")) # 解析时长\n",
    "print(d.parse_number(\"我25岁了\")) # 解析数值\n",
    "print(d.parse_ordinal(\"我是第一，你是第二\"))  # 解析序数#解析温度\n",
    "print(d.parse_temperature(\"人体最适宜的温度是25摄氏度\"))\n",
    "print(d.parse_time(\"我们十一点半见\"))# 解析时间\n",
    "print(d.parse_timezone(\"中国统一用一个时区UTC\")) # 解析时区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckling_chinese import Duckling, DucklingDimension\n",
    "\n",
    "duckling = Duckling()\n",
    "context = duckling.gen_context()\n",
    "options = duckling.gen_options(\n",
    "    targets={\n",
    "        DucklingDimension.ACT\n",
    "    }\n",
    ")\n",
    "\n",
    "text = '倒数第一场'\n",
    "answers = duckling.analyze(text, context=context, options=options)\n",
    "entities = duckling.parse_entities(text, context=context, options=options)\n",
    "\n",
    "print(duckling.parse_number(\"我今年25岁了\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "text = '今天上午9点我得了第一名'\n",
    "answers = duckling.analyze(text, context=context, options=options)\n",
    "entities = duckling.parse_entities(text, context=context, options=options)\n",
    "\n",
    "print(answers)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编码修复ftfy\n",
    "\n",
    "  ```python\n",
    "from ftfy import fix_text\n",
    "\n",
    "text = \"ÃƒÂ¡\"  # 这可能是“á”经过多次错误编码的结果\n",
    "fixed_text = fix_text(text)\n",
    "\n",
    "print(fixed_text)  # 输出应为\"á\"\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 纠错\n",
    "\n",
    "## pycorrector\n",
    "\n",
    "## YoungCorrector\n",
    "\n",
    "* https://github.com/hiyoung123/YoungCorrector/\n",
    "* 基于规则的文本纠错系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# markdown to text\n",
    "\n",
    "## strip_markdown\n",
    "\n",
    "* pip install strip_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good man\n",
      "如果老年人想去北京旅游，以下是一些建议：\n",
      "\n",
      "确定旅游时间：考虑到老年人的身体状况，建议选择在气候宜人且交通方便的季节出行，如春季或秋季。2. 做好健康管理：在出行前，老年人应咨询医生，了解自己的身体状况能否适应长途旅行和高强度的旅游活动。同时，准备好常用药品，并注意饮食卫生。3. 选择合适的交通方式：如果老年人身体健康状况良好，可以选择乘坐飞机前往北京。如果身体状况稍差，可以考虑乘坐火车，途中可以欣赏沿途的风景，缓解旅途疲劳。4. 规划好行程：老年人应选择轻松、悠闲的行程，避免过于劳累。可以考虑参观故宫、天安门广场、颐和园等著名景点，同时也可以选择一些适合老年人的文化休闲活动，如品茶、赏花等。5. 注意安全：在旅游过程中，老年人应尽量选择有专人陪同的旅行团，以确保安全。同时，要注意遵守旅游景区的规定和安全提示。6. 备好应急用品：老年人应准备一些急救药品和常用药品，以及必要的急救用品，如创可贴、消毒液等。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import strip_markdown\n",
    "MD = \"# good man \\n\\n如果老年人想去北京旅游，以下是一些建议：\\n\\n 1. **确定旅游时间**：考虑到老年人的身体状况，建议选择在气候宜人且交通方便的季节出行，如春季或秋季。2. **做好健康管理**：在出行前，老年人应咨询医生，了解自己的身体状况能否适应长途旅行和高强度的旅游活动。同时，准备好常用药品，并注意饮食卫生。3. **选择合适的交通方式**：如果老年人身体健康状况良好，可以选择乘坐飞机前往北京。如果身体状况稍差，可以考虑乘坐火车，途中可以欣赏沿途的风景，缓解旅途疲劳。4. **规划好行程**：老年人应选择轻松、悠闲的行程，避免过于劳累。可以考虑参观故宫、天安门广场、颐和园等著名景点，同时也可以选择一些适合老年人的文化休闲活动，如品茶、赏花等。5. **注意安全**：在旅游过程中，老年人应尽量选择有专人陪同的旅行团，以确保安全。同时，要注意遵守旅游景区的规定和安全提示。6. **备好应急用品**：老年人应准备一些急救药品和常用药品，以及必要的急救用品，如创可贴、消毒液等。\"\n",
    "TXT: str = strip_markdown.strip_markdown(MD)\n",
    "print(TXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mistune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>good man</h1>\n",
      "<p>如果老年人想去北京旅游，以下是一些建议：</p>\n",
      "<ol>\n",
      "<li><strong>确定旅游时间</strong>：考虑到老年人的身体状况，建议选择在气候宜人且交通方便的季节出行，如春季或秋季。2. <strong>做好健康管理</strong>：在出行前，老年人应咨询医生，了解自己的身体状况能否适应长途旅行和高强度的旅游活动。同时，准备好常用药品，并注意饮食卫生。3. <strong>选择合适的交通方式</strong>：如果老年人身体健康状况良好，可以选择乘坐飞机前往北京。如果身体状况稍差，可以考虑乘坐火车，途中可以欣赏沿途的风景，缓解旅途疲劳。4. <strong>规划好行程</strong>：老年人应选择轻松、悠闲的行程，避免过于劳累。可以考虑参观故宫、天安门广场、颐和园等著名景点，同时也可以选择一些适合老年人的文化休闲活动，如品茶、赏花等。5. <strong>注意安全</strong>：在旅游过程中，老年人应尽量选择有专人陪同的旅行团，以确保安全。同时，要注意遵守旅游景区的规定和安全提示。6. <strong>备好应急用品</strong>：老年人应准备一些急救药品和常用药品，以及必要的急救用品，如创可贴、消毒液等。</li>\n",
      "</ol>\n",
      "\n",
      "<h1>good man</h1>\n",
      "<p>如果老年人想去北京旅游，以下是一些建议：</p>\n",
      "<ol>\n",
      "<li><strong>确定旅游时间</strong>：考虑到老年人的身体状况，建议选择在气候宜人且交通方便的季节出行，如春季或秋季。2. <strong>做好健康管理</strong>：在出行前，老年人应咨询医生，了解自己的身体状况能否适应长途旅行和高强度的旅游活动。同时，准备好常用药品，并注意饮食卫生。3. <strong>选择合适的交通方式</strong>：如果老年人身体健康状况良好，可以选择乘坐飞机前往北京。如果身体状况稍差，可以考虑乘坐火车，途中可以欣赏沿途的风景，缓解旅途疲劳。4. <strong>规划好行程</strong>：老年人应选择轻松、悠闲的行程，避免过于劳累。可以考虑参观故宫、天安门广场、颐和园等著名景点，同时也可以选择一些适合老年人的文化休闲活动，如品茶、赏花等。5. <strong>注意安全</strong>：在旅游过程中，老年人应尽量选择有专人陪同的旅行团，以确保安全。同时，要注意遵守旅游景区的规定和安全提示。6. <strong>备好应急用品</strong>：老年人应准备一些急救药品和常用药品，以及必要的急救用品，如创可贴、消毒液等。</li>\n",
      "</ol>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mistune\n",
    "\n",
    "def markdown_to_text(markdown_string):\n",
    "    # renderer = mistune.Renderer(escape=False)\n",
    "    return mistune.html(markdown_string)\n",
    "    # markdown_parser = mistune.Markdown(renderer=renderer)\n",
    "    # return markdown_parser(markdown_string)\n",
    "\n",
    "# # 读取Markdown文件\n",
    "# with open('input.md', 'r') as f:\n",
    "#     content = f.read()\n",
    "\n",
    "# # 将Markdown转换为纯文本\n",
    "# text_content = markdown_to_text(content)\n",
    "\n",
    "# 写入TXT文件\n",
    "# with open('output.txt', 'w') as f:\n",
    "#     f.write(text_content)\n",
    "\n",
    "\n",
    "print(markdown_to_text(MD))\n",
    "\n",
    "from mistune import HTMLRenderer\n",
    "\n",
    "markdown = mistune.create_markdown(renderer=HTMLRenderer())\n",
    "print(markdown(MD))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good man\n",
      "如果老年人想去北京旅游，以下是一些建议：\n",
      "\n",
      "确定旅游时间：考虑到老年人的身体状况，建议选择在气候宜人且交通方便的季节出行，如春季或秋季。2. 做好健康管理：在出行前，老年人应咨询医生，了解自己的身体状况能否适应长途旅行和高强度的旅游活动。同时，准备好常用药品，并注意饮食卫生。3. 选择合适的交通方式：如果老年人身体健康状况良好，可以选择乘坐飞机前往北京。如果身体状况稍差，可以考虑乘坐火车，途中可以欣赏沿途的风景，缓解旅途疲劳。4. 规划好行程：老年人应选择轻松、悠闲的行程，避免过于劳累。可以考虑参观故宫、天安门广场、颐和园等著名景点，同时也可以选择一些适合老年人的文化休闲活动，如品茶、赏花等。5. 注意安全：在旅游过程中，老年人应尽量选择有专人陪同的旅行团，以确保安全。同时，要注意遵守旅游景区的规定和安全提示。6. 备好应急用品：老年人应准备一些急救药品和常用药品，以及必要的急救用品，如创可贴、消毒液等。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(markdown_to_text(MD), \"html.parser\")\n",
    "# text = ''.join(soup.findAll(text=True))\n",
    "text = soup.get_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_editor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
