{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awesome LLM\n",
    "* https://github.com/Hannibal046/Awesome-LLM\n",
    "# opensourced models\n",
    "\n",
    "##  LLama\n",
    "* paper: https://arxiv.org/pdf/2302.13971.pdf\n",
    "  * data Wikipedia [4.5%]: which use either the Latin or Cyrillic scripts: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk.\n",
    "* https://github.com/facebookresearch/llama\n",
    "\n",
    "### Huatuo-Llama-Med-Chinese\n",
    "* https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese\n",
    "* \n",
    "\n",
    "## MOSS\n",
    "\n",
    "## ChatGLM\n",
    "* https://github.com/THUDM/ChatGLM-6B\n",
    "* An Open Bilingual Dialogue Language Model | 开源双语对话语言模型\n",
    "\n",
    "### 衍生项目\n",
    "* https://github.com/yanqiangmiffy/Chinese-LangChain\n",
    "* https://github.com/imClumsyPanda/langchain-ChatGLM\n",
    "* https://github.com/l15y/wenda\n",
    "* https://github.com/SCIR-HI/Med-ChatGLM\n",
    "* 更多：https://www.toutiao.com/article/7225991356471312948/?log_from=57e0e99656ef9_1683088485260\n",
    "\n",
    "## xmtf\n",
    "* Crosslingual Generalization through Multitask Finetuning\n",
    "* xP3 是 46 种语言的有监督数据集，带有英语和机器翻译的 prompts\n",
    "\n",
    "## stanford_alpaca\n",
    "* https://github.com/tatsu-lab/stanford_alpaca\n",
    "  * it was finetune on LLama 7B (affordable)\n",
    "* https://crfm.stanford.edu/2023/03/13/alpaca.html\n",
    "  * how the finetuning data is collected, and the evaluation results.\n",
    "\n",
    "## Baize\n",
    "* https://github.com/project-baize/baize\n",
    "* paper: https://arxiv.org/pdf/2304.01196.pdf\n",
    "* demo: https://huggingface.co/spaces/project-baize/Baize-7B\n",
    "* propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself.\n",
    "* based on LLaMA\n",
    "\n",
    "## [Cabrita](https://github.com/22-hours/cabrita)  *****\n",
    "* try locally: https://github.com/22-hours/cabrita/blob/main/notebooks/cabrita-lora.ipynb\n",
    "* A portuguese finetuned instruction LLaMA. \n",
    "* just translated the alpaca_data.json to portuguese using ChatGPT, and then finetune.\n",
    "\n",
    "## Chinese-Vicuna \n",
    "* A Chinese Instruction-following LLaMA-based Model. 一个中文低资源的llama+lora方案，结构参考alpaca.\n",
    "* https://github.com/Facico/Chinese-Vicuna\n",
    "\n",
    "## GPT4-x-Alpaca\n",
    "* GPT4-x-Alpaca is a LLaMA 13B model fine-tuned with a collection of GPT4 conversations, GPTeacher. There’s not a lot of information on its training and performance.\n",
    "* https://huggingface.co/chavinlo/gpt4-x-alpaca\n",
    "\n",
    "## GPT4All\n",
    "* Demo, data and code to train an assistant-style large language model with ~800k GPT-3.5-Turbo Generations based on LLaMa.\n",
    "* https://github.com/nomic-ai/gpt4all\n",
    "\n",
    "## GPTQ-for-LLaMA\n",
    "* 4 bits quantization of LLaMA using GPTQ. GPTQ is SOTA one-shot weight quantization method.\n",
    "* https://github.com/qwopqwop200/GPTQ-for-LLaMa\n",
    "\n",
    "## Koala\n",
    "* Koala is a language model fine-tuned on top of LLaMA\n",
    "* Koala: A Dialogue Model for Academic Research\n",
    "* demo: https://chat.lmsys.org/?model=koala-13b\n",
    "* https://github.com/young-geng/EasyLM/blob/main/docs/koala.md\n",
    "\n",
    "## Pygmalion-7b\n",
    "* https://huggingface.co/PygmalionAI/pygmalion-7b\n",
    "* a dialogue model based on Meta’s LLaMA-7B. This is version 1. It has been fine-tuned using a subset of the data from Pygmalion-6B-v8-pt4, for those of you familiar with the project.\n",
    "\n",
    "## [Vicuna (FastChat)](https://github.com/lm-sys/FastChat)  *****\n",
    "* An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality.\n",
    "* including finetune, serving with web gui, api, evaluation\n",
    "* released FastChat-T5, supported models: lmsys/fastchat-t5-3b-v1.0\n",
    "BlinkDL/RWKV-4-Raven\n",
    "databricks/dolly-v2-12b\n",
    "OpenAssistant/oasst-sft-1-pythia-12b\n",
    "project-baize/baize-lora-7B\n",
    "StabilityAI/stablelm-tuned-alpha-7b\n",
    "THUDM/chatglm-6b\n",
    "* Vicuna-7B: needs around 30 GB of CPU RAM, Vicuna-13B needs 60 GB of CPU RAM. requires around 28GB of GPU memory for Vicuna-13B and 14GB of GPU memory for Vicuna-7B. Vicuna-7B can run on a 32GB M1 Macbook with 1 - 2 words / second.\n",
    "* https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/huggingface_api.py\n",
    "* blog introduction: https://lmsys.org/blog/2023-03-30-vicuna/\n",
    "* online-demo: https://chat.lmsys.org/\n",
    "* model comparison online: https://chat.lmsys.org/?arena\n",
    "\n",
    "## BLOOM (BigScience) ****\n",
    "* BigScience Large Open-science Open-access Multilingual Language Model.\n",
    "* https://huggingface.co/bigscience/bloom\n",
    "* demo: https://huggingface.co/spaces/huggingface/bloom_demo\n",
    "\n",
    "\n",
    "## More\n",
    "* https://medium.com/geekculture/list-of-open-sourced-fine-tuned-large-language-models-llm-8d95a2e0dc76\n",
    "\n",
    "# tools\n",
    "\n",
    "## FlagOpen\n",
    "\n",
    "* https://github.com/FlagOpen\n",
    "* 2022年2月28日，智源研究院最新发布了FlagOpen飞智大模型技术开源体系\n",
    "* 包括数据标注部分。FlagData\n",
    "\n",
    "\n",
    "## open assistant\n",
    "* https://github.com/LAION-AI/Open-Assistant\n",
    "\n",
    "## OpenChatKit\n",
    "* https://github.com/togethercomputer/OpenChatKit\n",
    "* provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. The kit includes an instruction-tuned language models, a moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories. OpenChatKit models were trained on the OIG-43M training dataset, which was a collaboration between Together, LAION, and Ontocord.ai.\n",
    "* Augmenting the model with additional context from a retrieval index\n",
    "\n",
    "## ChatRWKV\n",
    "\n",
    "## xTuring\n",
    "* https://xturing.stochastic.ai/finetune/guide/\n",
    "* Easily build, customize and control your own LLMs\n",
    "\n",
    "## MLC LLM\n",
    "* Enable everyone to develop, optimize and deploy AI models natively on everyone's devices.\n",
    "* https://github.com/mlc-ai/mlc-llm\n",
    "\n",
    "\n",
    "## langchain\n",
    "* https://github.com/hwchase17/langchain\n",
    "  * https://github.com/hwchase17/chat-langchain \n",
    "    * running example: This repo is an implementation of a locally hosted chatbot specifically focused on question answering over the LangChain documentation. Built with LangChain and FastAPI.\n",
    "  * https://huggingface.co/spaces/JavaFXpert/Chat-GPT-LangChain\n",
    "* Building applications with LLMs through composability\n",
    "* do the dirty work when building apps with LLMs.\n",
    "\n",
    "## others\n",
    "* https://github.com/nichtdax/awesome-totally-open-chatgpt\n",
    "* [IDPChat](https://github.com/BaihaiAI/IDPChat) \n",
    "  * IDPChat是开放的中文多模态模型，基于预训练大预言模型 LLaMA-13B 和开源文生图预训练模型Stable Diffusion构建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q transformers accelerate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigscience/bloomz-7b1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "inputs = tokenizer.encode(\"Translate to English: Je t’aime.\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetune\n",
    "\n",
    "## [PEFT](https://huggingface.co/docs/peft/index)\n",
    "* Parameter-Efficient Fine-Tuning (PEFT)\n",
    "* supported models\n",
    "  * [LoRA](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "  * [Prefix Tuning](https://aclanthology.org/2021.acl-long.353/)\n",
    "  * [P-Tuning](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "  * [Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "  * [AdaLoRA](https://arxiv.org/abs/2303.10512)\n",
    "  * [LLaMA-Adapter](https://github.com/ZrrSkywalker/LLaMA-Adapter)\n",
    "\n",
    "# speedup training\n",
    "\n",
    "## [deepspeech](https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload)\n",
    "* designed for speed and scale for distributed training of large models with billions of parameters\n",
    "* coupled with PEFT\n",
    "* At its core is the Zero Redundancy Optimizer (ZeRO) that shards optimizer states (ZeRO-1), gradients (ZeRO-2), and parameters (ZeRO-3) across data parallel processes.\n",
    "* ZeRO-Offload reduces GPU compute and memory by leveraging CPU resources during optimization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大模型未来方向\n",
    "\n",
    "* 更长的context\n",
    "* 本地化部署，一般GPU如3090上运行\n",
    "* 联网，与本地化知识库相结合\n",
    "* 更快的推理速度\n",
    "* prompt tuning for applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
