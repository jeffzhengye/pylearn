{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaddleNlp\n",
    "\n",
    "## PaddlePaddle install \n",
    "\n",
    "* https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/pip/linux-pip.html\n",
    "\n",
    "## PaddleNlp docs.\n",
    "\n",
    "* https://paddlenlp.readthedocs.io/zh/latest -- 文档\n",
    "* api: https://paddlenlp.readthedocs.io/en/latest/source/paddlenlp.transformers.tokenizer_utils.html\n",
    "\n",
    "## source code \n",
    "\n",
    "### all supported transformer tokenizer\n",
    "\n",
    "* paddlenlp/transformers/auto/tokenizer.py\n",
    "* paddlenlp/transformers/ernie/tokenizer.py  # ernie相关的\n",
    "\n",
    "### transformer model configs\n",
    "\n",
    "* paddlenlp/transformers/ernie/modeling.py\n",
    "\n",
    "### transformer model zoom, and supported language.\n",
    "\n",
    "* https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html#transformer  -- bert  有Portuguese 支持\n",
    "* https://paddlenlp.readthedocs.io/zh/latest/model_zoo/transformers/GPT/contents.html -- GPT 有Portuguese 支持"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaddleNlp 数据集、模型下载到默认路径\n",
    "\n",
    "* A: 内置的数据集、模型默认会下载到$HOME/.paddlenlp/下，通过配置环境变量可下载到指定路径：\n",
    "\n",
    "    *（1）Linux下，设置 export PPNLP_HOME=\"xxxx\"，注意不要设置带有中文字符的路径。\n",
    "\n",
    "    *（2）Windows下，同样配置环境变量 PPNLP_HOME 到其他非中文字符路径，重启即可。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer 返回结果含义\n",
    "\n",
    "* model_outputs.py 中 BaseModelOutputWithPoolingAndCrossAttentions 有详细说明\n",
    "    * https://huggingface.co/transformers/v3.0.2/model_doc/bert.html 解释, BertModel\n",
    "* modeling.py 中有 ErnieForSequenceClassification， ErnieForQuestionAnswering， ErnieForTokenClassification， ErnieLMPredictionHead， ErnieForMultipleChoice， ErnieForMaskedLM\n",
    "\n",
    "### Transformers pytorch return\n",
    "\n",
    "* last_hidden_state (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size)):\n",
    "    * Sequence of hidden-states at the output of the last layer of the model.\n",
    "* pooler_output (torch.FloatTensor: of shape (batch_size, hidden_size)):\n",
    "    * Last layer hidden-state of the first token of the sequence (classification token) further processed by a Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence prediction (classification) objective during pre-training.\n",
    "\n",
    "    * This output is usually not a good summary of the semantic content of the input, you’re often better with averaging or pooling the sequence of hidden-states for the whole input sequence.\n",
    "\n",
    "* hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True):\n",
    "    * Tuple of torch.FloatTensor (one for the output of the embeddings + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).\n",
    "    * Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "\n",
    "* attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when config.output_attentions=True):\n",
    "    * Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).\n",
    "    * Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "\n",
    "### ernie 超详细中文预训练模型ERNIE使用指南\n",
    "* https://aistudio.baidu.com/paddle/forum/topic/show/954092\n",
    "\n",
    "### tokenizer 参数文档\n",
    "\n",
    "* https://github.com/PaddlePaddle/PaddleNLP/blob/2000ea2ea88f5ef7804d1ee31f6b57f9feb6f006/paddlenlp/transformers/tokenizer_utils.py#L622-L625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-10-16 23:04:31,673] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-medium-zh'.\u001b[0m\n",
      "\u001b[32m[2022-10-16 23:04:31,680] [    INFO]\u001b[0m - Already cached /mnt/d/dataset/nlp/paddle/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt\u001b[0m\n",
      "\u001b[32m[2022-10-16 23:04:31,736] [    INFO]\u001b[0m - tokenizer config file saved in /mnt/d/dataset/nlp/paddle/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-10-16 23:04:31,743] [    INFO]\u001b[0m - Special tokens file saved in /mnt/d/dataset/nlp/paddle/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 647, 358, 9, 340, 9, 635, 3016, 1764, 292, 210, 7, 314, 5, 138, 362, 563, 2, 9, 14, 3016, 1764, 292, 210, 7, 314, 5, 305, 563, 1114, 12045, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import AutoModel, AutoTokenizer\n",
    "from paddlenlp.transformers import ErnieForTokenClassification, ErnieTokenizer\n",
    "\n",
    "import paddle\n",
    "\n",
    "model_name = 'ernie-3.0-medium-zh'\n",
    "# model_name = \"ernie-3.0-xbase-zh\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = ErnieForTokenClassification.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 请问有没有像琉璃神社一样的资源站 有和琉璃神社一样的网站吗？\n",
    "\n",
    "# inputs = tokenizer(\"请问有没有像琉璃神社一样的资源站\",\n",
    "#                    \"有和琉璃神社一样的网站吗？\", max_seq_len=64, pad_to_max_seq_len=True)\n",
    "\n",
    "query = \"请问有没有像琉璃神社一样的资源站\"\n",
    "title = \"有和琉璃神社一样的网站吗？\"\n",
    "max_seq_length = 64\n",
    "pad_to_max = True\n",
    "\n",
    "inputs = tokenizer(query, title, max_seq_len=max_seq_length, pad_to_max_seq_len=pad_to_max, return_attention_mask=True)\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "print(len(inputs['input_ids']))\n",
    "# inputs = {k:paddle.to_tensor([v]) for (k, v) in inputs.items()}\n",
    "# # logits = model(**inputs, return_dict=True, output_hidden_states=True, output_attentions=True)\n",
    "# logits = model(**inputs)\n",
    "# print(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[1, 15, 768], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[[ 0.06013136,  0.08001949,  0.05949559, ...,  0.03834157,\n",
      "           0.16428161, -0.02553395],\n",
      "         [ 0.17710927,  1.07631350,  0.22704059, ..., -0.01483721,\n",
      "          -0.58641475, -0.37783957],\n",
      "         [ 0.63684541,  0.03024112,  0.39377603, ...,  0.12226901,\n",
      "           0.43927503,  1.50784373],\n",
      "         ...,\n",
      "         [ 1.05572450,  2.52401543, -0.73081750, ..., -0.19437474,\n",
      "           0.94409478,  1.50580680],\n",
      "         [-0.25393343,  0.31314465,  0.06259530, ..., -0.44231522,\n",
      "          -0.11833040, -0.35460761],\n",
      "         [ 0.34953618, -1.13955617,  0.45758203, ...,  0.39407942,\n",
      "           0.17412712, -0.47145003]]])\n"
     ]
    }
   ],
   "source": [
    "print(logits.hidden_states[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-10 15:14:33,981] [    INFO]\u001b[0m - Downloading tokenizer_config.json from https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/tokenizer_config.json\u001b[0m\n",
      "100%|██████████| 167/167 [00:00<00:00, 89.6kB/s]\n",
      "\u001b[32m[2022-08-10 15:14:34,215] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.bert.tokenizer.BertTokenizer'> to load 'neuralmind/bert-base-portuguese-cased'.\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:34,217] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/vocab.txt and saved to /home/jeffye/.paddlenlp/models/neuralmind/bert-base-portuguese-cased\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:34,218] [    INFO]\u001b[0m - Downloading vocab.txt from https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/vocab.txt\u001b[0m\n",
      "100%|██████████| 205k/205k [00:00<00:00, 956kB/s] \n",
      "\u001b[32m[2022-08-10 15:14:34,699] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/added_tokens.json and saved to /home/jeffye/.paddlenlp/models/neuralmind/bert-base-portuguese-cased\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:34,700] [    INFO]\u001b[0m - Downloading added_tokens.json from https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/added_tokens.json\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:34,879] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/special_tokens_map.json and saved to /home/jeffye/.paddlenlp/models/neuralmind/bert-base-portuguese-cased\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:34,881] [    INFO]\u001b[0m - Downloading special_tokens_map.json from https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:35,067] [    INFO]\u001b[0m - Already cached /home/jeffye/.paddlenlp/models/neuralmind/bert-base-portuguese-cased/tokenizer_config.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tokenizer_pt = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model = AutoModel.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "# del pretrained_model\n",
    "# pretrained_model = AutoModel.from_pretrained(\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\")\n",
    "# del pretrained_model\n",
    "pretrained_model = AutoModel.from_pretrained(\"bert-base-multilingual-uncased\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer api\n",
    "* 文档： https://paddlenlp.readthedocs.io/en/latest/source/paddlenlp.transformers.ernie.tokenizer.html\n",
    "* huggingface: https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-05-07 21:31:14,609] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-medium-zh'.\u001b[0m\n",
      "\u001b[32m[2023-05-07 21:31:14,610] [    INFO]\u001b[0m - Already cached C:\\Users\\73915\\.paddlenlp\\models\\ernie-3.0-medium-zh\\ernie_3.0_medium_zh_vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-05-07 21:31:14,644] [    INFO]\u001b[0m - tokenizer config file saved in C:\\Users\\73915\\.paddlenlp\\models\\ernie-3.0-medium-zh\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-05-07 21:31:14,646] [    INFO]\u001b[0m - Special tokens file saved in C:\\Users\\73915\\.paddlenlp\\models\\ernie-3.0-medium-zh\\special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import AutoModel, AutoTokenizer\n",
    "from paddlenlp.transformers import ErnieForTokenClassification, ErnieTokenizer\n",
    "\n",
    "import paddle\n",
    "# from paddlenlp.transformers import ErnieTinyTokenizer\n",
    "# tokenizer = ErnieTinyTokenizer.from_pretrained('ernie-tiny')\n",
    "\n",
    "# model_name = 'ernie-3.0-medium-zh'\n",
    "# # model_name = \"ernie-3.0-xbase-zh\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized: ['this', 'is', '我', '所', '要', '的']\n",
      "convert_tokens_to_ids: [3730, 2775, 75, 110, 41, 5]\n",
      "token to index [3730, 2775, 75, 110, 41, 5]\n",
      "token ids: {'input_ids': [1, 3730, 2775, 75, 110, 41, 5, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0]} 开始插入1[CLS], 结束加入[SEP]2\n",
      "id2token: ['[CLS]', 'this', 'is', '我', '所', '要', '的', '[SEP]']\n",
      "[MASK] id= [3]\n",
      "id=0, token= ['[PAD]']\n"
     ]
    }
   ],
   "source": [
    "vocabs = tokenizer.get_vocab()\n",
    "# 分词\n",
    "tokens = tokenizer.tokenize(\"this is我所要的\")\n",
    "\n",
    "# tokenizer.get_vocab()[token] is equivalent to tokenizer.convert_tokens_to_ids(token) when token is in the vocab.\n",
    "strings = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids_vocab2index = [vocabs[tok] for tok in tokens]\n",
    "# text to ids\n",
    "token_ids = tokenizer(\"this is我所要的\")\n",
    "\n",
    "# id to token\n",
    "id2tokens = tokenizer.convert_ids_to_tokens(token_ids['input_ids'])\n",
    "\n",
    "print('tokenized:', tokens)\n",
    "print('convert_tokens_to_ids:', strings)\n",
    "print('token to index', token_ids_vocab2index)\n",
    "print('token ids:', token_ids, '开始插入1[CLS], 结束加入[SEP]2')\n",
    "print('id2token:', id2tokens)\n",
    "print('[MASK] id=', tokenizer.convert_tokens_to_ids(['[MASK]']))\n",
    "print('id=0, token=', tokenizer.convert_ids_to_tokens([0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer (Ernie) modeling 说明\n",
    "* 源码位置： paddlenlp\\transformers\\ernie\\modeling.py\n",
    "* 可以查看ModelOutput，每种model 返回格式： paddlenlp\\transformers\\model_outputs.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAC\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'LAC是个优秀的分词工具',\n",
       "  'segs': ['LAC', '是', '个', '优秀', '的', '分词', '工具'],\n",
       "  'tags': ['nz', 'v', 'q', 'a', 'u', 'n', 'n']}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from paddlenlp import Taskflow\n",
    "\n",
    "lac = Taskflow(\"lexical_analysis\")\n",
    "lac(\"LAC是个优秀的分词工具\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 摘要\n",
    "\n",
    "## text2title 中文  \n",
    "* https://huggingface.co/PaddlePaddle/unimo-text-1.0-summary\n",
    "\n",
    "## text to shorter text\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\73915\\.conda\\envs\\py38\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\u001b[32m[2023-08-01 23:27:00,616] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/unified_transformer/plato-mini-vocab.txt and saved to C:\\Users\\73915\\.paddlenlp\\models\\plato-mini\u001b[0m\n",
      "\u001b[32m[2023-08-01 23:27:01,199] [    INFO]\u001b[0m - Downloading plato-mini-vocab.txt from https://bj.bcebos.com/paddlenlp/models/transformers/unified_transformer/plato-mini-vocab.txt\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3e28351ca24ce9900712bddfd70fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/410k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-08-01 23:27:02,362] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/unified_transformer/plato-mini-spm.model and saved to C:\\Users\\73915\\.paddlenlp\\models\\plato-mini\u001b[0m\n",
      "\u001b[32m[2023-08-01 23:27:02,872] [    INFO]\u001b[0m - Downloading plato-mini-spm.model from https://bj.bcebos.com/paddlenlp/models/transformers/unified_transformer/plato-mini-spm.model\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609e7111feab45bab3be9b4c90937dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-08-01 23:27:04,968] [    INFO]\u001b[0m - tokenizer config file saved in C:\\Users\\73915\\.paddlenlp\\models\\plato-mini\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-08-01 23:27:04,969] [    INFO]\u001b[0m - Special tokens file saved in C:\\Users\\73915\\.paddlenlp\\models\\plato-mini\\special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2023-08-01 23:27:04,975] [    INFO]\u001b[0m - Model config UnifiedTransformerConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"mask_token_id\": 30000,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"unified_transformer\",\n",
      "  \"normalize_before\": true,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"role_type_size\": null,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token_id\": 0,\n",
      "  \"vocab_size\": 30001\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[32m[2023-08-01 23:27:04,977] [    INFO]\u001b[0m - Configuration saved in C:\\Users\\73915\\.paddlenlp\\models\\plato-mini\\config.json\u001b[0m\n",
      "\u001b[32m[2023-08-01 23:27:04,980] [    INFO]\u001b[0m - Downloading plato-mini.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/unified_transformer/plato-mini.pdparams\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9113590d944811897b2942edf58031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/518M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-08-01 23:32:59,821] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing UnifiedTransformerLMHeadModel.\n",
      "\u001b[0m\n",
      "\u001b[32m[2023-08-01 23:32:59,822] [    INFO]\u001b[0m - All the weights of UnifiedTransformerLMHeadModel were initialized from the model checkpoint at plato-mini.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UnifiedTransformerLMHeadModel for predictions without further training.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import UnifiedTransformerTokenizer\n",
    "#设置想要使用的模型名称\n",
    "model_name = 'plato-mini'\n",
    "tokenizer = UnifiedTransformerTokenizer.from_pretrained(model_name)\n",
    "\n",
    "from paddlenlp.transformers import UnifiedTransformerLMHeadModel\n",
    "\n",
    "model = UnifiedTransformerLMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\73915\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.745 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "\u001b[33m[2023-08-02 10:24:18,413] [ WARNING]\u001b[0m - Accessing `bos_token_id` through `model.bos_token_id` will be deprecated after v2.6.0. Instead, do `model.config.bos_token_id`\u001b[0m\n",
      "\u001b[33m[2023-08-02 10:24:18,413] [ WARNING]\u001b[0m - Accessing `eos_token_id` through `model.eos_token_id` will be deprecated after v2.6.0. Instead, do `model.config.eos_token_id`\u001b[0m\n",
      "\u001b[33m[2023-08-02 10:24:18,414] [ WARNING]\u001b[0m - Accessing `pad_token_id` through `model.pad_token_id` will be deprecated after v2.6.0. Instead, do `model.config.pad_token_id`\u001b[0m\n",
      "\u001b[33m[2023-08-02 10:24:18,415] [ WARNING]\u001b[0m - Accessing `forced_bos_token_id` through `model.forced_bos_token_id` will be deprecated after v2.6.0. Instead, do `model.config.forced_bos_token_id`\u001b[0m\n",
      "\u001b[33m[2023-08-02 10:24:18,416] [ WARNING]\u001b[0m - Accessing `forced_eos_token_id` through `model.forced_eos_token_id` will be deprecated after v2.6.0. Instead, do `model.config.forced_eos_token_id`\u001b[0m\n",
      "\u001b[33m[2023-08-02 10:24:18,417] [ WARNING]\u001b[0m - Accessing `decoder_start_token_id` through `model.decoder_start_token_id` will be deprecated after v2.6.0. Instead, do `model.config.decoder_start_token_id`\u001b[0m\n",
      "\u001b[33m[2023-08-02 10:24:18,418] [ WARNING]\u001b[0m - Accessing `no_repeat_ngram_size` through `model.no_repeat_ngram_size` will be deprecated after v2.6.0. Instead, do `model.config.no_repeat_ngram_size`\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问： 你好。\n",
      "答： 你好,你是哪里的?\n",
      "问： 我来自月球。你呢\n",
      "答： 我来自火星,你在月球上干什么呢?\n",
      "问： 我来自火星\n",
      "答： 我来自火星\n",
      "问： 给我写一篇关于刘德华的祭文\n",
      "答： 写这个干嘛\n",
      "问： 写得完\n",
      "答： 为什么要写不完呢?\n",
      "问： 你真啰嗦\n",
      "答： 你怎么知道我啰嗦的,是不是你的性格就不啰嗦\n",
      "问： 我叫葛军\n",
      "答： 你怎么了?为什么起这个名字呀?\n",
      "问： 因为我是男生\n",
      "答： 那你有什么爱好吗?\n",
      "问： 不是这样的啊\n",
      "答： 你是什么样子的呀\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "line = input()\n",
    "while line:\n",
    "    user_input = [line]\n",
    "\n",
    "    encoded_input = tokenizer.dialogue_encode(\n",
    "        user_input,\n",
    "        add_start_token_as_response=True,\n",
    "        return_tensors = True,\n",
    "        is_split_into_words = False)\n",
    "\n",
    "    ids, scores = model.generate(\n",
    "        input_ids = encoded_input['input_ids'],\n",
    "        token_type_ids = encoded_input['token_type_ids'],\n",
    "        position_ids = encoded_input['position_ids'],\n",
    "        attention_mask=encoded_input['attention_mask'],\n",
    "        max_length = 64,\n",
    "        min_length = 1,\n",
    "        decode_strategy='sampling',\n",
    "        top_k=5,\n",
    "        num_return_sequences=20)\n",
    "\n",
    "    response = []\n",
    "    for sequence_ids in ids.numpy().tolist():\n",
    "        sequence_ids = sequence_ids[:sequence_ids.index(tokenizer.sep_token_id)]\n",
    "        text = tokenizer.convert_ids_to_string(sequence_ids,keep_space=False)\n",
    "        response.append(text)\n",
    "    i = random.randint(0,len(response)-1)\n",
    "    print(\"问：\",line)\n",
    "    print(\"答：\",response[i])\n",
    "    line = input()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "memory = torch.rand(10, 32, 512)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "out = decoder_layer(tgt, memory)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d76abebb651b803d7523773c2538185af67bbe68e12b1f9d8b8e0e281792ff9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
