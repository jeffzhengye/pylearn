{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "* [离线识别率高达 99% 的 Python 人脸识别系统，开源~](https://www.51cto.com/article/708144.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\73915\\.conda\\envs\\open_editor\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "from PIL import Image\n",
    "\n",
    "image = face_recognition.load_image_file(\"../cv2/1.JPEG\")\n",
    "# face_locations = face_recognition.face_locations(image, model='cnn')\n",
    "# face_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found 2 face(s) in this photograph.\n",
      "A face is located at pixel location Top: 205, Left: 269, Bottom: 245, Right: 308\n",
      "A face is located at pixel location Top: 45, Left: 505, Bottom: 85, Right: 544\n"
     ]
    }
   ],
   "source": [
    "face_locations = face_recognition.face_locations(image, number_of_times_to_upsample=1, model=\"cnn\")\n",
    "\n",
    "print(\"I found {} face(s) in this photograph.\".format(len(face_locations)))\n",
    "\n",
    "for face_location in face_locations:\n",
    "\n",
    "    # Print the location of each face in this image\n",
    "    top, right, bottom, left = face_location\n",
    "    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n",
    "\n",
    "    # You can access the actual face itself like this:\n",
    "    face_image = image[top:bottom, left:right]\n",
    "    pil_image = Image.fromarray(face_image)\n",
    "    pil_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found 3 face(s) in this photograph.\n"
     ]
    }
   ],
   "source": [
    "image = face_recognition.load_image_file(\"../cv2/1.JPEG\")\n",
    "face_locations = face_recognition.face_locations(image, number_of_times_to_upsample=4, model=\"cnn\")\n",
    "print(\"I found {} face(s) in this photograph.\".format(len(face_locations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepface\n",
    "\n",
    "* depend on tensorflow, not very convenient due to too many version problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "backends = [\n",
    "  'opencv', \n",
    "  'ssd', \n",
    "  'dlib', \n",
    "  'mtcnn', \n",
    "  'retinaface', \n",
    "  'mediapipe',\n",
    "  'yolov8',\n",
    "  'yunet',\n",
    "]\n",
    "\n",
    "#face verification\n",
    "# obj = DeepFace.verify(img1_path = \"img1.jpg\", \n",
    "#         img2_path = \"img2.jpg\", \n",
    "#         detector_backend = backends[0]\n",
    "# )\n",
    "\n",
    "#face recognition\n",
    "# dfs = DeepFace.find(img_path = \"img.jpg\", \n",
    "#         db_path = \"my_db\", \n",
    "#         detector_backend = backends[1]\n",
    "# )\n",
    "\n",
    "# #embeddings\n",
    "# embedding_objs = DeepFace.represent(img_path = \"img.jpg\", \n",
    "#         detector_backend = backends[2]\n",
    "# )\n",
    "\n",
    "# #facial analysis\n",
    "# demographies = DeepFace.analyze(img_path = \"img4.jpg\", \n",
    "#         detector_backend = backends[3]\n",
    "# )\n",
    "\n",
    "# #face detection and alignment\n",
    "face_objs = DeepFace.extract_faces(img_path = \"2.jpg\", \n",
    "        target_size = (224, 224), \n",
    "        detector_backend = backends[4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_objs = DeepFace.extract_faces(img_path = \"../cv2/1.JPEG\", \n",
    "        target_size = (224, 224), \n",
    "        detector_backend = backends[4]\n",
    ")\n",
    "len(face_objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "face_detection = pipeline(task=Tasks.face_detection, model='damo/cv_ddsar_face-detection_iclr23-damofd')\n",
    "# 支持 url image and abs dir image path\n",
    "img_path = 'https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/mog_face_detection.jpg'\n",
    "result = face_detection(img_path)\n",
    "\n",
    "# 提供可视化结果\n",
    "from modelscope.utils.cv.image_utils import draw_face_detection_result\n",
    "from modelscope.preprocessors.image import LoadImage\n",
    "img = LoadImage.convert_to_ndarray(img_path)\n",
    "cv2.imwrite('srcImg.jpg', img)\n",
    "img_draw = draw_face_detection_result('srcImg.jpg', result)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img_draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface/summary\n",
    "# https://modelscope.cn/models/damo/cv_resnet101_face-detection_cvpr22papermogface/summary\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "mog_face_detection_func = pipeline(Tasks.face_detection, 'damo/cv_resnet101_face-detection_cvpr22papermogface')\n",
    "src_img_path = 'https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/mog_face_detection.jpg'\n",
    "src_img_path = \"D:\\\\wps\\\\剪印\\\\auto_generate/aigc_girls\\\\0276ea824d22ecad359ce0d8addd3a0c-精美壁纸：碧海蓝天/62acfe00ffc18c1f7e955b0d6caec110.JPEG\"\n",
    "raw_result = mog_face_detection_func(src_img_path)\n",
    "print('face detection output: {}.'.format(raw_result))\n",
    "\n",
    "# if you want to show the result, you can run\n",
    "from modelscope.utils.cv.image_utils import draw_face_detection_no_lm_result\n",
    "from modelscope.preprocessors.image import LoadImage\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# load image from url as rgb order\n",
    "src_img = LoadImage.convert_to_ndarray(src_img_path)\n",
    "# save src image as bgr order to local\n",
    "src_img  = cv2.cvtColor(np.asarray(src_img), cv2.COLOR_RGB2BGR)\n",
    "cv2.imwrite('src_img.jpg', src_img) \n",
    "# draw dst image from local src image as bgr order\n",
    "dst_img = draw_face_detection_no_lm_result('src_img.jpg', raw_result)\n",
    "# save dst image as bgr order to local\n",
    "cv2.imwrite('dst_img.jpg', dst_img)\n",
    "# show dst image by rgb order\n",
    "import matplotlib.pyplot as plt\n",
    "dst_img  = cv2.cvtColor(np.asarray(dst_img), cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(dst_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "retina_face_detection = pipeline(Tasks.face_detection, 'damo/cv_resnet50_face-detection_retinaface')\n",
    "img_path = 'https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/retina_face_detection.jpg'\n",
    "img_path = \"D:\\\\wps\\\\剪印\\\\auto_generate/aigc_girls\\\\0276ea824d22ecad359ce0d8addd3a0c-精美壁纸：碧海蓝天/62acfe00ffc18c1f7e955b0d6caec110.JPEG\"\n",
    "result = retina_face_detection(img_path)\n",
    "print(f'face detection output: {result}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人脸融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-13 11:20:10,489 - modelscope - INFO - PyTorch version 2.0.1+cu117 Found.\n",
      "2023-10-13 11:20:10,502 - modelscope - INFO - TensorFlow version 2.14.0 Found.\n",
      "2023-10-13 11:20:10,503 - modelscope - INFO - Loading ast index from C:\\Users\\73915\\.cache\\modelscope\\ast_indexer\n",
      "2023-10-13 11:20:10,732 - modelscope - INFO - Loading done! Current index file version is 1.9.2, with md5 3315cddc66f26e5d24d726457a584f8b and a total number of 941 components indexed\n",
      "2023-10-13 11:20:12,336 - modelscope - INFO - Model revision not specified, use revision: v1.2\n",
      "Downloading: 100%|██████████| 6.00k/6.00k [00:00<00:00, 944kB/s]\n",
      "Downloading: 100%|██████████| 72.0M/72.0M [00:49<00:00, 1.52MB/s]\n",
      "Downloading: 100%|██████████| 21.7k/21.7k [00:00<00:00, 569kB/s]\n",
      "Downloading: 100%|██████████| 521k/521k [00:00<00:00, 1.27MB/s]\n",
      "Downloading: 100%|██████████| 121M/121M [01:21<00:00, 1.56MB/s] \n",
      "Downloading: 100%|██████████| 182/182 [00:00<00:00, 30.3kB/s]\n",
      "Downloading: 100%|██████████| 249M/249M [02:45<00:00, 1.58MB/s] \n",
      "Downloading: 100%|██████████| 1.77M/1.77M [00:01<00:00, 1.64MB/s]\n",
      "Downloading: 100%|██████████| 4.27M/4.27M [00:03<00:00, 1.47MB/s]\n",
      "Downloading: 100%|██████████| 91.9M/91.9M [00:55<00:00, 1.75MB/s]\n",
      "Downloading: 100%|██████████| 272M/272M [02:41<00:00, 1.77MB/s] \n",
      "Downloading: 100%|██████████| 3.95M/3.95M [00:02<00:00, 1.79MB/s]\n",
      "Downloading: 100%|██████████| 1.05M/1.05M [00:00<00:00, 1.66MB/s]\n",
      "Downloading: 100%|█████████▉| 855M/855M [03:28<00:00, 4.29MB/s]\n",
      "Downloading: 100%|██████████| 3.54k/3.54k [00:00<00:00, 518kB/s]\n",
      "Downloading: 100%|██████████| 521k/521k [00:00<00:00, 1.41MB/s]\n",
      "2023-10-13 11:32:34,574 - modelscope - INFO - initiate model from C:\\Users\\73915\\.cache\\modelscope\\hub\\damo\\cv_unet-image-face-fusion_damo\n",
      "2023-10-13 11:32:34,576 - modelscope - INFO - initiate model from location C:\\Users\\73915\\.cache\\modelscope\\hub\\damo\\cv_unet-image-face-fusion_damo.\n",
      "2023-10-13 11:32:34,580 - modelscope - INFO - initialize model from C:\\Users\\73915\\.cache\\modelscope\\hub\\damo\\cv_unet-image-face-fusion_damo\n",
      "C:\\Users\\73915\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "2023-10-13 11:32:50,473 - modelscope - INFO - load facefusion models done\n",
      "2023-10-13 11:33:03,688 - modelscope - INFO - init done\n",
      "2023-10-13 11:33:03,765 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-10-13 11:33:03,767 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-10-13 11:33:03,767 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\73915\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\cv_unet-image-face-fusion_damo'}. trying to build by task and model information.\n",
      "2023-10-13 11:33:03,768 - modelscope - WARNING - No preprocessor key ('image-face-fusion', 'image-face-fusion') found in PREPROCESSOR_MAP, skip building preprocessor.\n",
      "2023-10-13 11:33:03,772 - modelscope - INFO - image face fusion model init done\n",
      "C:\\Users\\73915\\AppData\\Roaming\\Python\\Python39\\site-packages\\modelscope\\models\\cv\\image_face_fusion\\facelib\\matlab_cp2tform.py:81: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  r, _, _, _ = lstsq(X, U)\n",
      "C:\\Users\\73915\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument grid in method wrapper_CUDA__cudnn_grid_sampler)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26796\\3651994540.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtemplate_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/facefusion_template.jpg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0muser_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/facefusion_user.jpg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_face_fusion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemplate_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'result.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutputKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOUTPUT_IMG\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\modelscope\\pipelines\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\modelscope\\pipelines\\base.py\u001b[0m in \u001b[0;36m_process_single\u001b[1;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_auto_collate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m                     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\modelscope\\pipelines\\cv\\image_face_fusion_pipeline.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mtemplate_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'template'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0muser_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'outputs'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\modelscope\\models\\cv\\image_face_fusion\\image_face_fusion.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, template_img, user_img)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mYt_trans_inv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwarp_affine_torch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans_inv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mori_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mori_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[0mmask_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwarp_affine_torch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans_inv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mori_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mori_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[0mYt_trans_inv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mYt_trans_inv\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmask_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mXt_raw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\modelscope\\models\\cv\\image_face_fusion\\network\\ops.py\u001b[0m in \u001b[0;36mwarp_affine_torch\u001b[1;34m(src, M, dsize, mode, padding_mode, align_corners)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0msrc_norm_trans_dst_norm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdsize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdsize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         align_corners=align_corners)\n\u001b[1;32m--> 206\u001b[1;33m     return F.grid_sample(\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mgrid_sample\u001b[1;34m(input, grid, mode, padding_mode, align_corners)\u001b[0m\n\u001b[0;32m   4242\u001b[0m         \u001b[0malign_corners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4244\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_sampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode_enum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_mode_enum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument grid in method wrapper_CUDA__cudnn_grid_sampler)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from modelscope.outputs import OutputKeys\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "image_face_fusion = pipeline(Tasks.image_face_fusion, \n",
    "                       model='damo/cv_unet-image-face-fusion_damo')\n",
    "template_path = 'https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/facefusion_template.jpg'\n",
    "user_path = 'https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/facefusion_user.jpg'\n",
    "result = image_face_fusion(dict(template=template_path, user=user_path))\n",
    "\n",
    "cv2.imwrite('result.png', result[OutputKeys.OUTPUT_IMG])\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_editor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
