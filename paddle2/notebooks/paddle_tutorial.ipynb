{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paddle\n",
    "\n",
    "## general docs\n",
    "\n",
    "* https://www.paddlepaddle.org.cn/documentation/docs/en/guides\n",
    "* 高阶api: https://www.paddlepaddle.org.cn/documentation/docs/zh/tutorial/quick_start/high_level_api/high_level_api.html\n",
    "* https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html\n",
    "\n",
    "## tricks of training for Paddldclas\n",
    "\n",
    "* https://paddleclas.readthedocs.io/en/latest/models/Tricks_en.html\n",
    "\n",
    "## install problems\n",
    "\n",
    "* core_avx.so: undefined symbol: _dl_sym, version GLIBC_PRIVATE\n",
    "* install lighting version.\n",
    "\n",
    "## Paddle dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "#### map-style, MapDataset\n",
    "\n",
    "* 映射式(map-style) 数据集需要继承这个基类,映射式数据集 可以通过一个键值 索引并获取指定样本的数据集,所有映射式数据集必须实现以下方法: \n",
    "* 1.___getitem__: 根据给定索引 获取数据集中指定样本,在 paddle.io.DataLoader 中需要使用此函数通过下标获取样本。  \n",
    "* 2.__len__: 返回数据集样本个数, <span style=\"color:red\">paddle.io.BatchSampler 中需要样本个数生成 下标序列。</span> \n",
    "* 可以直接继承 Dataset, 实现以上两个方法即可\n",
    "* MapDataset 进行了包装，比如可以直接传入list 数据集合\n",
    "\n",
    "\n",
    "#### iterable-style\n",
    "\n",
    "\n",
    "### transformation\n",
    "\n",
    "* 在自定义Dataset 的 __getitem__中就处理\n",
    "  * 例子参考： https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/beginner/data_preprocessing_cn.html\n",
    "* 直接调用.map(fn), 其中fn为处理函数\n",
    "* 在Dataloader 中传入 collate_fn\n",
    "  * paddlenlp\\data\\data_collator.py， 如DataCollatorWithPadding，DataCollatorForSeq2Seq\n",
    "  ```python\n",
    "  DataLoader(\n",
    "            train_dataset,\n",
    "            batch_sampler=train_sampler,\n",
    "            collate_fn=self.data_collator,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "        )\n",
    "  ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "* doc: https://www.paddlepaddle.org.cn/documentation/api/paddle/io/DataLoader_cn.html\n",
    "* 多进程读取： DataLoader支持单进程和多进程的数据加载方式，当 num_workers 大于0时，将使用多进程方式异步加载数据。\n",
    "* DataLoader当前支持 map-style 和 iterable-style 的数据集\n",
    "\n",
    "#### 禁用自动组batch\n",
    "* DataLoader 支持在 batch_size 和 batch_sampler 均为None的时候禁用自动组batch功能，此时需求从 dataset 中获取的数据为已经组好batch的数据，该数据将不做任何处理直接传到 collate_fn\n",
    "\n",
    "#### collate_fn 作用\n",
    "* 组batch的方法\n",
    "* collate_fn (callable，可选) - 通过此参数指定如果将样本列表组合为 mini-batch 数据，当 collate_fn 为 None 时，默认为将样本个字段在第 0 维上堆叠(同 np.stack(..., axis=0) )为 mini-batch 的数据。默认值为 None。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paddle BatchSampler\n",
    "\n",
    "* https://www.paddlepaddle.org.cn/documentation/docs/en/api/paddle/io/BatchSampler_en.html\n",
    "* Batch sampler used by paddle.io.DataLoader should be a subclass of paddle.io.BatchSampler, BatchSampler subclasses should implement following methods:\n",
    "    * __iter__: return <span style=\"color:red\">***mini-batch indices***</span> iterably. \n",
    "    * __len__: get mini-batch number in an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchSampler: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "BatchSampler: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "BatchSampler: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n",
      "BatchSampler: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n",
      "BatchSampler: [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "BatchSampler: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]\n",
      "BatchSampler: [96, 97, 98, 99]\n",
      "RandomSampler: [6, 47, 35, 37, 82, 58, 3, 11]\n",
      "RandomSampler: [33, 56, 95, 45, 87, 10, 4, 52]\n",
      "RandomSampler: [57, 41, 71, 55, 83, 96, 13, 97]\n",
      "RandomSampler: [36, 99, 8, 69, 25, 92, 44, 73]\n",
      "RandomSampler: [62, 89, 51, 72, 14, 66, 84, 98]\n",
      "RandomSampler: [20, 75, 48, 68, 15, 43, 65, 39]\n",
      "RandomSampler: [23, 40, 64, 16, 63, 76, 86, 50]\n",
      "RandomSampler: [59, 22, 54, 46, 77, 53, 67, 0]\n",
      "RandomSampler: [90, 34, 74, 18, 42, 80, 27, 32]\n",
      "RandomSampler: [79, 70, 93, 60, 29, 17, 61, 5]\n",
      "RandomSampler: [91, 9, 81, 31, 19, 28, 21, 1]\n",
      "RandomSampler: [78, 88, 2, 94, 38, 26, 85, 7]\n"
     ]
    }
   ],
   "source": [
    "from paddle.io import RandomSampler, BatchSampler, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# init with dataset\n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, num_samples):\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.random.random([784]).astype('float32')\n",
    "        label = np.random.randint(0, 9, (1, )).astype('int64')\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "bs = BatchSampler(dataset=RandomDataset(100),\n",
    "                  shuffle=False,\n",
    "                  batch_size=16,\n",
    "                  drop_last=False)\n",
    "\n",
    "for batch_indices in bs:\n",
    "    print(\"BatchSampler:\", batch_indices)\n",
    "\n",
    "# init with sampler\n",
    "sampler = RandomSampler(RandomDataset(100))\n",
    "bs = BatchSampler(sampler=sampler,\n",
    "                  batch_size=8,\n",
    "                  drop_last=True)\n",
    "\n",
    "for batch_indices in bs:\n",
    "    print(\"RandomSampler:\", batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cache file /home/jeffye/.cache/paddle/dataset/mnist/train-images-idx3-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/train-images-idx3-ubyte.gz \n",
      "Begin to download\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 8/8 [============================>.] - ETA: 0s - 6ms/item"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Download finished\n",
      "Cache file /home/jeffye/.cache/paddle/dataset/mnist/train-labels-idx1-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/train-labels-idx1-ubyte.gz \n",
      "Begin to download\n",
      "\n",
      "Download finished\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "\n",
    "data = paddle.vision.datasets.MNIST(mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffye/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paddle.vision.datasets.mnist.MNIST"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffye/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[10, 1], dtype=int64, place=Place(gpu:0), stop_gradient=True,\n",
       "       [[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6],\n",
       "        [7],\n",
       "        [8],\n",
       "        [9]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cProfile import label\n",
    "\n",
    "labels = paddle.reshape(labels, shape=[-1, 1])\n",
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paddle layers \n",
    "\n",
    "### Cosine Similarity Operator - paddle.fluid.layers.nn.cos_sim(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[3, 1], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [[0.83597958],\n",
      "        [0.93105304],\n",
      "        [0.85824275]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffye/anaconda3/lib/python3.8/site-packages/paddle/fluid/framework.py:3792: DeprecationWarning: Op `cos_sim` is executed through `append_op` under the dynamic mode, the corresponding API implementation needs to be upgraded to using `_C_ops` method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "\n",
    "x = paddle.rand(shape=[3, 7], dtype='float32')\n",
    "y = paddle.rand(shape=[1, 7], dtype='float32')\n",
    "out = paddle.fluid.layers.cos_sim(x, y)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [[0.8024077,0.9927354,0.27238318,0.8344984 ],\n",
    "        [0.48949873,0.5797396,0.65444374,0.66510963],\n",
    "        [0.1031398,0.9614342,0.08365563,0.6796464 ],\n",
    "        [0.1031398,0.9614342,0.08365563,0.6796464 ],\n",
    "        [0.10760343,0.7461209,0.7726148,0.5801006 ]]\n",
    "x2 = [[0.62913156,0.1536727,0.9847992,0.04591406],\n",
    "        [0.9098952,0.15715368,0.8671125,0.3156102 ],\n",
    "        [0.4427798,0.54136837,0.5276275,0.32394758],\n",
    "        [0.1031398,0.9614342,0.08365563,0.6796464 ],\n",
    "        [0.3769419,0.8535014,0.48041078,0.9256797 ]]\n",
    "x1 = paddle.to_tensor(x1)\n",
    "x2 = paddle.to_tensor(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[5], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [0.10000000, 0.10000000, 0.10000000, 0.10000000, 0.10000000])\n",
      "Tensor(shape=[5, 5], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [[0.10000000, 0.        , 0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.10000000, 0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.10000000, 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        , 0.10000000, 0.        ],\n",
      "        [0.        , 0.        , 0.        , 0.        , 0.10000000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffye/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = paddle.matmul(x1, x2, transpose_y=True)\n",
    "margin_diag = paddle.full(shape=[cosine_sim.shape[0]],\n",
    "                                  fill_value=0.1,\n",
    "                                  dtype=paddle.get_default_dtype())\n",
    "print(margin_diag)\n",
    "print(paddle.diag(margin_diag))     \n",
    "cosine_sim = cosine_sim - paddle.diag(margin_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[5, 1, 4], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [[[0.80240768, 0.99273539, 0.27238318, 0.83449841]],\n",
      "\n",
      "        [[0.48949873, 0.57973957, 0.65444374, 0.66510963]],\n",
      "\n",
      "        [[0.10313980, 0.96143419, 0.08365563, 0.67964637]],\n",
      "\n",
      "        [[0.10313980, 0.96143419, 0.08365563, 0.67964637]],\n",
      "\n",
      "        [[0.10760343, 0.74612093, 0.77261478, 0.58010060]]])\n",
      "Tensor(shape=[1, 5, 4], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [[[0.62913156, 0.15367270, 0.98479921, 0.04591406],\n",
      "         [0.90989518, 0.15715368, 0.86711252, 0.31561020],\n",
      "         [0.44277981, 0.54136837, 0.52762753, 0.32394758],\n",
      "         [0.10313980, 0.96143419, 0.08365563, 0.67964637],\n",
      "         [0.37694189, 0.85350138, 0.48041078, 0.92567968]]])\n",
      "Tensor(shape=[5, 5], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [[0.52750373, 0.68519437, 0.90307665, 0.88645834, 0.94705576],\n",
      "        [0.75573283, 0.83689672, 0.97151995, 0.78222287, 0.95629632],\n",
      "        [0.23341376, 0.34393719, 0.75037485, 1.        , 0.92203134],\n",
      "        [0.23341376, 0.34393719, 0.75037485, 1.        , 0.92203134],\n",
      "        [0.67095637, 0.66773933, 0.91482407, 0.81773114, 0.92458993]])\n"
     ]
    }
   ],
   "source": [
    "x = x1.unsqueeze(1)\n",
    "y = x2.unsqueeze(0)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "cos_sim_func = paddle.nn.CosineSimilarity(axis=-1)\n",
    "result = cos_sim_func(x, y)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffye/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[5], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
       "       [0.52750373, 0.83689672, 0.75037485, 1.        , 0.92458993])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paddle.diag(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[4], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [0.87143707, 0.80905795, 0.79628360, 0.70043772])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffye/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "cos_sim_func = paddle.nn.CosineSimilarity(axis=0)\n",
    "result = cos_sim_func(x1, x2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[5], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [0.52750373, 0.83689672, 0.75037485, 1.        , 0.92458993])\n"
     ]
    }
   ],
   "source": [
    "cos_sim_func = paddle.nn.CosineSimilarity(axis=1)\n",
    "result = cos_sim_func(x1, x2)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [0.87143707])\n"
     ]
    }
   ],
   "source": [
    "cos_sim_func = paddle.nn.CosineSimilarity(axis=0)\n",
    "result = cos_sim_func(x1[:, 0], x2[:, 0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [0.52750373])\n"
     ]
    }
   ],
   "source": [
    "cos_sim_func = paddle.nn.CosineSimilarity(axis=0)\n",
    "result = cos_sim_func(x1[0, :], x2[0, :])\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paddle Search operator\n",
    "\n",
    "* useful for implementing special process\n",
    "* paddle/tensor/search.py\n",
    "    * masked_select : https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/masked_select_cn.html#masked-select\n",
    "    * topk\n",
    "    * searchsorted\n",
    "    * index_sample\n",
    "    * sort\n",
    "    * kthvalue\n",
    "    * bucketize\n",
    "    * index_select"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paddle Train\n",
    "\n",
    "### fp16 training\n",
    "\n",
    "* \n",
    "\n",
    "### Gradient Accumulation in dygraph graph mode\n",
    "* <https://www.paddlepaddle.org.cn/documentation/docs/en/guides/performance_improving/amp_en.html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = paddle.nn.MSELoss() # Define loss calculation function\n",
    "model = SimpleNet(input_size, output_size)  # Define SimpleNet model\n",
    "optimizer = paddle.optimizer.SGD(learning_rate=0.0001, parameters=model.parameters())  # Define SGD optimizer\n",
    "\n",
    "accumulate_batchs_num = 10 # the batch numbers of gradients accumulation\n",
    "\n",
    "# define GradScaler\n",
    "scaler = paddle.amp.GradScaler(init_loss_scaling=1024)\n",
    "\n",
    "train_time = 0\n",
    "for epoch in range(epochs):\n",
    "    for i, (data, label) in enumerate(loader):\n",
    "        start_time = time.time() # get start time\n",
    "        label._to(place) # Copy label to GPU\n",
    "         # create AMP context environment\n",
    "        with paddle.amp.auto_cast(level='O1'):\n",
    "            output = model(data)\n",
    "            loss = mse(output, label)\n",
    "        # use GradScaler complete the loss scaling\n",
    "        scaled = scaler.scale(loss)\n",
    "        scaled.backward()\n",
    "\n",
    "        #  when the accumulated batch is accumulate_batchs_num, update the model parameters\n",
    "        if (i + 1) % accumulate_batchs_num == 0:\n",
    "            # update parameters\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.clear_grad(set_to_zero=False)\n",
    "        # record training loss and training time\n",
    "        train_loss = loss.numpy()\n",
    "        train_time += time.time() - start_time\n",
    "\n",
    "print(\"loss:\", train_loss)\n",
    "print(\"Time consuming using AMP-O1 mode:{:.3f} sec\".format(train_time/(epochs*nums_batch)))\n",
    "# loss: [0.6602017]\n",
    "# Time consuming using AMP-O1 mode:0.113 sec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use high level API with AMP usage\n",
    "* https://www.paddlepaddle.org.cn/documentation/docs/zh/tutorial/quick_start/high_level_api/high_level_api.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.vision.transforms as T\n",
    "\n",
    "\n",
    "def run_example_code():\n",
    "    device = paddle.set_device('gpu')\n",
    "    # Using high level API to define neural network\n",
    "    net = nn.Sequential(nn.Flatten(1), nn.Linear(\n",
    "        784, 200), nn.Tanh(), nn.Linear(200, 10))\n",
    "    model = paddle.Model(net)\n",
    "    # Define optimizer\n",
    "    optim = paddle.optimizer.SGD(\n",
    "        learning_rate=1e-3, parameters=model.parameters())\n",
    "    # Initialize neural network\n",
    "    amp_configs = {\n",
    "        \"level\": \"O1\",                    # Level corresponds to amp mode: O1, O2\n",
    "        # Customize the white list and support custom_black_list\n",
    "        \"custom_white_list\": {'conv2d'},\n",
    "        \"use_dynamic_loss_scaling\": True  # Dynamic loss_scaling\n",
    "    }\n",
    "    model.prepare(optim,\n",
    "                  paddle.nn.CrossEntropyLoss(),\n",
    "                  paddle.metric.Accuracy(),\n",
    "                  amp_configs=amp_configs)\n",
    "    # prepare data\n",
    "    transform = T.Compose([T.Transpose(), T.Normalize([127.5], [127.5])])\n",
    "    data = paddle.vision.datasets.MNIST(mode='train', transform=transform)\n",
    "    # use AMP training\n",
    "    model.fit(data, epochs=2, batch_size=32, verbose=1)\n",
    "\n",
    "\n",
    "if paddle.is_compiled_with_cuda():\n",
    "    run_example_code()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count parameters of a model/layer\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def calculate_params(model):\n",
    "    n_train = 0\n",
    "    n_non_train = 0\n",
    "    for p in model.parameters():\n",
    "        if p.trainable:\n",
    "            n_train += np.prod(p.shape)\n",
    "        else:\n",
    "            n_non_train += np.prod(p.shape)\n",
    "    return n_train + n_non_train, n_train, n_non_train\n",
    "\n",
    "```\n",
    "\n",
    "### dispaly network structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "   Conv2D-1       [[1, 1, 28, 28]]      [1, 6, 28, 28]          60       \n",
      "    ReLU-1        [[1, 6, 28, 28]]      [1, 6, 28, 28]           0       \n",
      "  MaxPool2D-1     [[1, 6, 28, 28]]      [1, 6, 14, 14]           0       \n",
      "   Conv2D-2       [[1, 6, 14, 14]]     [1, 16, 10, 10]         2,416     \n",
      "    ReLU-2       [[1, 16, 10, 10]]     [1, 16, 10, 10]           0       \n",
      "  MaxPool2D-2    [[1, 16, 10, 10]]      [1, 16, 5, 5]            0       \n",
      "   Linear-1          [[1, 400]]            [1, 120]           48,120     \n",
      "   Linear-2          [[1, 120]]            [1, 84]            10,164     \n",
      "   Linear-3          [[1, 84]]             [1, 10]              850      \n",
      "===========================================================================\n",
      "Total params: 61,610\n",
      "Trainable params: 61,610\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.35\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "{'total_params': 61610, 'trainable_params': 61610}\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "class LeNet(nn.Layer):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2D(\n",
    "                1, 6, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2D(2, 2),\n",
    "            nn.Conv2D(\n",
    "                6, 16, 5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2D(2, 2))\n",
    "\n",
    "        if num_classes > 0:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(400, 120),\n",
    "                nn.Linear(120, 84),\n",
    "                nn.Linear(\n",
    "                    84, 10))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.features(inputs)\n",
    "\n",
    "        if self.num_classes > 0:\n",
    "            x = paddle.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "lenet = LeNet()\n",
    "\n",
    "params_info = paddle.summary(lenet, (1, 1, 28, 28))\n",
    "print(params_info)\n",
    "\n",
    "# list input demo\n",
    "\n",
    "\n",
    "class LeNetListInput(LeNet):\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.features(inputs[0])\n",
    "\n",
    "        if self.num_classes > 0:\n",
    "            x = paddle.flatten(x, 1)\n",
    "            x = self.fc(x + inputs[1])\n",
    "        return x\n",
    "\n",
    "\n",
    "lenet_list_input = LeNetListInput()\n",
    "input_data = [paddle.rand([1, 1, 28, 28]), paddle.rand([1, 400])]\n",
    "params_info = paddle.summary(lenet_list_input, input=input_data)\n",
    "print(params_info)\n",
    "\n",
    "# dict input demo\n",
    "\n",
    "\n",
    "class LeNetDictInput(LeNet):\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.features(inputs['x1'])\n",
    "\n",
    "        if self.num_classes > 0:\n",
    "            x = paddle.flatten(x, 1)\n",
    "            x = self.fc(x + inputs['x2'])\n",
    "        return x\n",
    "\n",
    "\n",
    "lenet_dict_input = LeNetDictInput()\n",
    "input_data = {'x1': paddle.rand([1, 1, 28, 28]),\n",
    "              'x2': paddle.rand([1, 400])}\n",
    "params_info = paddle.summary(lenet_dict_input, input=input_data)\n",
    "print(params_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential sequential_0\n",
      "Conv2D conv2d_0\n",
      "ReLU re_lu_0\n",
      "MaxPool2D max_pool2d_0\n",
      "Conv2D conv2d_1\n",
      "ReLU re_lu_1\n",
      "MaxPool2D max_pool2d_1\n",
      "Sequential sequential_1\n",
      "Linear linear_0\n",
      "Linear linear_1\n",
      "Linear linear_2\n",
      "features Sequential(\n",
      "  (0): Conv2D(1, 6, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2D(kernel_size=2, stride=2, padding=0)\n",
      "  (3): Conv2D(6, 16, kernel_size=[5, 5], data_format=NCHW)\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2D(kernel_size=2, stride=2, padding=0)\n",
      ")\n",
      "features.0 Conv2D(1, 6, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "features.1 ReLU()\n",
      "features.2 MaxPool2D(kernel_size=2, stride=2, padding=0)\n",
      "features.3 Conv2D(6, 16, kernel_size=[5, 5], data_format=NCHW)\n",
      "features.4 ReLU()\n",
      "features.5 MaxPool2D(kernel_size=2, stride=2, padding=0)\n",
      "fc Sequential(\n",
      "  (0): Linear(in_features=400, out_features=120, dtype=float32)\n",
      "  (1): Linear(in_features=120, out_features=84, dtype=float32)\n",
      "  (2): Linear(in_features=84, out_features=10, dtype=float32)\n",
      ")\n",
      "fc.0 Linear(in_features=400, out_features=120, dtype=float32)\n",
      "fc.1 Linear(in_features=120, out_features=84, dtype=float32)\n",
      "fc.2 Linear(in_features=84, out_features=10, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 输出每一层的名字，然后可以通过名字冻结我们想冻结的层次。 \n",
    "# 如果想知道layer 间层次结构，可以看看paddle.summary() 代码\n",
    "\n",
    "for layer in lenet.sublayers():\n",
    "    print(type(layer).__name__, layer._full_name)\n",
    "\n",
    "for prefix, layer in lenet.named_sublayers():\n",
    "    print(prefix, layer)\n",
    "    \n",
    "import paddle.nn as nn\n",
    "def unfreeze(model):\n",
    "    for p in model.parameters():\n",
    "        p.trainable = True\n",
    "        \n",
    "for layer in model.sublayers():\n",
    "    # print(type(layer).__name__, layer._full_name)\n",
    "    if isinstance(layer, nn.TransformerEncoderLayer):\n",
    "        if layer._full_name.endswith(\"_1\"):\n",
    "            unfreeze(layer)\n",
    "            print(f'freeze layer 1 {layer._full_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 16, 16]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# max pool2d\n",
    "input = paddle.to_tensor(np.random.uniform(-1, 1, [1, 3, 32, 32]).astype(np.float32))\n",
    "MaxPool2D = nn.MaxPool2D(kernel_size=2, stride=1)   \n",
    "output = MaxPool2D(input)\n",
    "# output.shape [1, 3, 16, 16]\n",
    "output.shape\n",
    "\n",
    "# for return_mask=True\n",
    "# MaxPool2D = nn.MaxPool2D(kernel_size=2, stride=2, padding=0, return_mask=True)\n",
    "# output, max_indices = MaxPool2D(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 617396)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffye/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "paddle.disable_static()\n",
    "\n",
    "x_var = paddle.uniform((1, 13, 64, 768), dtype='float32', min=-1., max=1.)\n",
    "\n",
    "conv = nn.Conv2D(13, 13, (3, 768), padding=0)\n",
    "pool = nn.MaxPool2D(kernel_size=3, stride=1)\n",
    "flat = nn.Flatten()\n",
    "\n",
    "# y_var = pool(conv(x_var))\n",
    "# y_np = y_var.numpy()\n",
    "# print(y_np.shape)\n",
    "\n",
    "conv = nn.Conv2D(13, 13, (3, 768), padding=1)\n",
    "y_var = flat(pool(conv(x_var)))\n",
    "y_np = y_var.numpy()\n",
    "print(y_np.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaddleHub\n",
    "\n",
    "* https://www.paddlepaddle.org.cn/hublist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LD_LIBRARY_PATH=/home/$USER/anaconda3/lib:$LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: Can not import paddle core while this file exists: /home/jeffye/anaconda3/lib/python3.8/site-packages/paddle/fluid/libpaddle.so\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "libpython3.8.so.1.0: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7a947b2a4a94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m x_data = np.array([[[0, 1, 0],\n\u001b[1;32m      5\u001b[0m                     [ 1,  0, 1]]]).astype(\"float32\")\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/paddle/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmonkey_patch_variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmonkey_patch_math_varbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/paddle/framework/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# TODO: import framework api under this directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseed\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_default_dtype\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/paddle/framework/random.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# TODO: define random api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfluid\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfluid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfluid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/paddle/fluid/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# import all class inside framework into fluid module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# import all class inside executor into fluid module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/paddle/fluid/framework.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mframework_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munique_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfluid_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/paddle/fluid/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;34m\"Error: Your machine doesn't support AVX, but the installed PaddlePaddle is avx core, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \"you should reinstall paddlepaddle with no-avx core.\\n\")\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/paddle/fluid/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibpaddle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mavx_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlibpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compiled_with_avx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         sys.stderr.write(\n",
      "\u001b[0;31mImportError\u001b[0m: libpython3.8.so.1.0: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[[0, 1, 0],\n",
    "                    [ 1,  0, 1]]]).astype(\"float32\")\n",
    "print(x_data.shape)\n",
    "paddle.disable_static()\n",
    "x = paddle.to_tensor(x_data, stop_gradient=False)\n",
    "output = paddle.nn.functional.label_smooth(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d76abebb651b803d7523773c2538185af67bbe68e12b1f9d8b8e0e281792ff9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
