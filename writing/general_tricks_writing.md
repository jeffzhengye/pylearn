# general writing skills

## when idea is simply and straightforward

- æƒ³æ³•ç®€å•ï¼Œä½†æ˜¯å¯èƒ½å…·ä½“å®ç°æœ‰å¾ˆå¤šå›°éš¾ã€‚

  - In this way, it is able to better learn the similarity relations among query, positive passages and negative passages. Although the idea is appealing, it is not easy to implement due to three major issues. First, it is unclear how to formalize and learn both query-centric and passage-centric similarity relations. Second, it requires arge-scale and highquality training data to incorporate passage-centric similarity relation. However, it is expensive to manually label data. Additionally, there might be a large number of unlabeled positives even in the existing manually labeled datasets (Qu et al., 2020), and it is likely to bring false negatives when sampling hard negatives. Finally, learning passagecentric similarity relation (an auxiliary task) is not directly related to the query-centric similarity relation (a target task). In terms of multi-task viewpoint, multi-task models often perform worse than their single-task counterparts (Alonso and Plank, 2017; McCann et al., 2018; Clark et al., 2019). Hence, it needs a more elaborate design for the training procedure.
  - To this end, in this paper, we propose a novel approach that leverages both query-centric and PAssage-centric sImilarity Relations (called PAIR) for dense passage retrieval. In order to address the aforementioned issues, we have made three important technical contributions. First,
  - è¿™ä¸ªèŒƒä¾‹å¾ˆå¥½çš„å±•ç¤ºè¿™ç§æƒ…å†µæ€ä¹ˆå†™ã€‚é‡ç‚¹è¯´æ˜ä¸ºä»€ä¹ˆä¸å¥½å®ç°æˆ–ä¸­é—´å…·ä½“çš„å›°éš¾åœ¨å“ªé‡Œï¼Œä¸ç„¶çš„è¯å¦‚æœçœŸçš„é‚£ä¹ˆå®¹æ˜“æ–‡ç« å°±æ²¡é‚£ä¹ˆå¤§ä»·å€¼äº†ï¼Œè°éƒ½å¯ä»¥åšå—ã€‚é¿å…å®¡ç¨¿æ—¶ï¼Œå®¡ç¨¿äººå¯èƒ½ä¸€çœ‹å¯èƒ½è§‰å¾—è¿™ä¸ªå¥½åƒä¹Ÿå¤§çš„åˆ›æ–°ã€‚

- æ–¹æ³•ç®€å•ï¼Œä½†æ˜¯æ•ˆæœå¥½
  - danqi chen: SimCSE
  - This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. **_This simple method works surprisingly well, performing on par with previous supervised counterparts._** We hypothesize that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we draw inspiration from the recent success of learning sentence embeddings from natural language inference (NLI) datasets and incorporate annotated pairs from NLI datasets into contrastive learning by using â€œentailmentâ€ pairs as positives and â€œcontradictionâ€ pairs as hard negatives.
  - è¿™ç¯‡æ–‡ç« æƒ³æ³•å’Œå®ç°éƒ½ç®€å•å¾—ä¸åƒè¯ï¼Œå¦‚æœåŠŸåº•ä¸€èˆ¬å“ªæ€•å‘ç°äº† SimCSE ä¸­çš„ tricks å¯ä»¥æé«˜ä¹Ÿå‘ä¸äº†é¡¶ä¼šï¼Œç”šè‡³è‡ªå·±éƒ½è§‰å¾—æƒ³æ³•å¤ªç®€å•ï¼Œå°±ä¸€ä¸ª trick è€Œå·²å°±ä¸äº›è®ºæ–‡äº†ã€‚ä½†æ˜¯å¤§ç¥å°±æ˜¯å¤§ç¥ï¼Œåˆ†æçš„ç¥å‡ºé¬¼æ²¡ï¼Œæ„Ÿå¹ä¸€ä¸ªç‰›å­—ã€‚åšç ”ç©¶åŸºæœ¬åŠŸå¾ˆé‡è¦ã€‚

## è™½ç„¶å‰äººæœ‰è¿›å±•ï¼Œä½†æ˜¯è¿˜æ˜¯å€¼å¾—ç ”ç©¶

- Despite the progress made so far, there is still a need for developing a more precise method for peptideâ€“MHC binding prediction to reduce the large number of false positives and thus improve the confidence of the predicted peptideâ€“MHC interactions. In addition, improving the correlations between predicted and measured binding affinities may help quantify the binding advantage of neoantigens compared to the wild-type version, which can further facilitate vaccine development. Moreover, the prediction results from most of the previous methods

## æ–¹æ³•æœ‰æ˜æ˜¾ä¸è¶³ä¸èƒ½å®ç°ï¼Œä½†ä¹Ÿæœ‰å¥½çš„æ–¹é¢

- In this paper, DSI is applied to moderate-sized corpora (from 10k to 320k documents), all of which
  are derived from one challenging retrieval task, and we leave the important question of the scaling
  DSI to larger corpora to future work.
-

## conclusion is in contrast to prior work.

- However, we observe that BM25 could show a competitive ranking quality compared to TILDE and TILDEv2 which is in contrast to the findings about the relative performance of these three models on retrieval for short queries reported in prior work. This result raises the question about the use of contextualized term-based ranking models being beneficial in QBE setting. We follow-up on our findings by studying the score interpolation between the relevance score from TILDE (TILDEv2) and BM25.

## ä¸åŒ components æˆ–æ–¹æ³•æ€§èƒ½å¯ä»¥ç´¯åŠ 

We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%.

## è·Ÿå‰äººä¸åŒ

- StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered
  by diffusion models.

-

## æ²¡æœ‰æŒ‡å‡ºå…·ä½“å‰äººçš„ä¸è¶³

- However, the quest for robust and accessible human-level TTS synthesis remains an ongoing challenge because there is still room for improvement in terms of diverse and expressive speech [5, 6], robustness for out-of-distribution (OOD) texts [7], and the requirements of massive datasets for high-performing zero-shot TTS systems [8].

- However, our focus is more on how to better process and present data based on human preference, rather than merely retrieving it from databases. Additionally, while SQL is convenient, it can not directly satisfy common data analysis needs such as prediction and visualization.

## absent of something æ²¡äººåšè¿‡

- Absent of a clear benchmark for evaluating the performance of LLM routers, progress in this area has been hampered. To bridge this gap, we present RouterBench, a novel evaluation framework designed to systematically assess the efficacy of LLM routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative LLMs to support the development of routing strategies. We further propose a theoretical framework for LLM routing, which provides a principled explanation for the observed performance differences between routing systems.

- Yet, the absence of a standardized benchmark for evaluating the performance of LLM routers hinders progress in this area. To bridge this gap, we present RouterBench, a novel evaluation framework designed to systematically assess the efficacy of LLM routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative LLMs to support the development of routing strategies. We further propose a theoretical framework for LLM routing,

## æå‡ºè¦æ¢è®¨çš„é—®é¢˜

- We raise the question to what extent LLMs are capable of handling these applications off-the-shelf, i.e. without finetuning.

## present ç»“è®º

- While there may be room for improvement through prompt engineering, our results aim to show the out-of-the-box LLM capabilities.

## present algorithm

- ![alt text](image-5.png)

## å°çš„ä¼˜åŒ–ï¼Œé›†æˆåˆ›æ–°

- Retrieve Anything To Augment Large Language Models
  - Training such a unified model is non-trivial, as various retrieval tasks aim to capture distinct semantic relationships, often subject to mutual interference. To address this challenge, we systematically optimize our training methodology. This includes reward formulation based on LLMsâ€™ feedback, the stabilization of knowledge distillation, multi-task fine-tuning with explicit instructions, and homogeneous in-batch negative sampling.

## others

- The challenges faced by RAG systems, such as ensuring contextually appropriate and up-to-date data, are addressed by the dynamic nature of knowledge graphs.
-

# Paper Structure

## Abstract

- 1. ç ”ç©¶çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼ŒåŠé‡è¦æ€§ã€‚
- 2. å½“å‰ä¸»æµçš„æ–¹æ³•ï¼ˆç‰¹åˆ«æ˜¯ä½ è¦æ¯”è¾ƒçš„æ–¹æ³•ï¼‰æ˜¯ä»€ä¹ˆï¼Œå­˜åœ¨é‚£äº›é—®é¢˜ã€‚
- 3. ä½ æ˜¯å¦‚ä½•åˆ›é€ æ€§çš„è§£å†³äº†è¿™äº›é—®é¢˜çš„ï¼Œå…·ä½“æ€ä¹ˆåšçš„ã€‚
  - ![Alt text](image-1.png)
- 4. å®éªŒç»“æœæ˜¯ä»€ä¹ˆï¼Ÿç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿæœ‰ä»€ä¹ˆé‡è¦/æœ‰è¶£çš„å‘ç°ã€‚
  - ä¾‹å¦‚åœ¨ xx æœ€æœ‰ä»£è¡¨æ€§çš„æ•°æ®é›†ä¸Šï¼Œæ¯” SOTA åœ¨é‚£äº›ç‚¹ä¸Šè¿˜å¥½ã€‚
  - ä¸ªäººåå‘ï¼šç»“æœå‘ˆç°æ—¶ç›´æ¥ç»™å‡ºå…·ä½“æ•°æ®ã€‚ä¾‹å¦‚ï¼šWe use a test set annotated by academic researchers in the fields of quantum physics and computer vision to evaluate our systemâ€™s performance. The results show that DocReLM achieves a Top 10 accuracy of 44.12% in computer vision, compared to Google Scholarâ€™s 15.69%, and an increase to 36.21% in quantum physics, while that of Google Scholar is 12.96%.
  -

### å¼•å‡ºä½ åšçš„äº‹

- However, despite
the success of foundation models in modalities
such as natural language processing and computer
vision, the development of foundation models for
time series forecasting has lagged behind. We
present Lag-Llama, a general-purpose foundation model for univariate probabilistic time series forecasting based on a decoder-only transformer architecture that uses lags as covariates.

## Introduction

- ç ”ç©¶çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆé‡è¦ã€‚
- å›é¡¾æœ€ä¸»è¦çš„å¯¹è®ºæ–‡ç ”ç©¶é—®é¢˜çš„ä¸»è¦ç ”ç©¶å·¥ä½œã€è¿›å±•ï¼Œ
- å‰ä»»ç ”ç©¶å·¥ä½œçš„ä¸»è¦ä¸è¶³åœ¨å“ªé‡Œï¼Œä¸ºä»€ä¹ˆåˆ°ç°åœ¨è¿˜æ²¡äººè§£å†³éš¾ç‚¹åœ¨å“ªé‡Œï¼Ÿï¼ˆå¦‚æœæ²¡æœ‰éš¾ç‚¹ï¼Œé‚£ä½ çš„ç ”ç©¶å·¥ä½œä¹Ÿæ²¡æ„ä¹‰ï¼Ÿï¼‰|æˆ–è€…ä½ å‘ç°äº†ä»€ä¹ˆç ”ç©¶ç©ºç™½ï¼Œç¡®ç«‹ç ”ç©¶æœºä¼šã€‚
  - ä»æ™®éçš„ problem åˆ°å…·ä½“çš„ question
- é’ˆå¯¹ä»¥ä¸Šé—®é¢˜ï¼Œä½ çš„ç ”ç©¶æ€è·¯æ˜¯ä»€ä¹ˆï¼Ÿä½ æ˜¯å¦‚ä½•åˆ›é€ æ€§çš„è§£å†³è¿™ä¸ªé—®é¢˜çš„ï¼Ÿ
- ä½ çš„æ–¹æ³•æ•ˆæœæ€ä¹ˆæ ·ï¼Ÿå®éªŒç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ
- æ€»ç»“è¯¥æ–‡çš„ä¸»è¦è´¡çŒ®

### å¦‚ä½•è¯´ç°æœ‰å·¥ä½œçš„ç¼ºç‚¹

- SPACE-3
- ![current problems](image-3.png)

### samples

- Top-Down Partitioning for Efficient List-Wise Ranking [Abstract]. Argument æ¸…æ™°å…·ä½“
  - Large Language Models (LLMs) have significantly impacted many facets of natural language processing
    and information retrieval. Unlike previous encoder-based approaches, the enlarged context window
    of these generative models allows for ranking multiple documents at once, commonly called list-wise
    ranking. However, there are still limits to the number of documents that can be ranked in a single
    inference of the model, leading to the broad adoption of a sliding window approach to identify the ğ‘˜
    most relevant items in a ranked list. We argue that the sliding window approach is not well-suited
    for list-wise re-ranking because it (1) cannot be parallelized in its current form, (2) leads to redundant
    computational steps repeatedly re-scoring the best set of documents as it works its way up the initial
    ranking, and (3) prioritizes the lowest-ranked documents for scoring rather than the highest-ranked
    documents by taking a bottom-up approach. Motivated by these shortcomings and an initial study that
    shows list-wise rankers are biased towards relevant documents at the start of their context window,
    we propose a novel algorithm that partitions a ranking to depth ğ‘˜ and processes documents top-down.
    Unlike sliding window approaches, our algorithm is inherently parallelizable due to the use of a pivot
    element, which can be compared to documents down to an arbitrary depth concurrently. In doing so, we
    reduce the number of expected inference calls by around 33% when ranking at depth 100 while matching
    the performance of prior approaches across multiple strong re-rankers.

## Related Work

- 1. æ·±å…¥åœ°åˆ†ç±»ä»‹ç»äº†ç›¸å…³çš„å·¥ä½œï¼š
  - 2. ä»‹ç»æ—¶ï¼Œè¦è¯´æ¸…æ¥šè·Ÿå½“å‰è®ºæ–‡çš„å…³ç³»ã€‚
  - 3.

## Method

- å¼•å‡ºéƒ¨åˆ†ï¼ˆæœ€å¥½æœ‰ä¸ªæ•´ä½“æ¶æ„/æµç¨‹å›¾ï¼‰ï¼š Figure 2 presents an overview of our approach for open-domain retrievalï¼Œ

### å¥½çš„å‚è€ƒ

- StyleTTSï¼š
  - å…ˆå†™ä¸€ä¸ª A. Proposed Framework æŠŠé—®é¢˜å’Œç¬¦å·å®šä¹‰æ¸…æ¥šã€‚
- ![example: atlas](image.png)

- are llms all you need for TOD
  ![alt text](image-2.png)

- itransformer

## Experiments

### Compared Methods

- We evaluate Mimir against the following methods:
- ![alt text](image-6.png)

## Discussion

- 1. æè¿°å®éªŒç»“æœ
  - æä¾›å¿…è¦çš„æ•°æ®å’Œæ•°é‡ï¼Œç¡®ä¿å‡†ç¡®æ€§ï¼›
  - ä½¿ç”¨å›¾æ ‡ã€è¡¨æ ¼æˆ–å›¾åƒå¯è§†åŒ–å·¥å…·æ¥å‘ˆç°
- 2. è§£é‡Šç»“æœ
  - è¯´æ˜ä½ è§‚å¯Ÿåˆ°çš„ç°è±¡æˆ–è¶‹åŠ¿ï¼Œå¹¶**è§£é‡Š**
  - ç»“æœä¸æœ€åˆå‡è®¾æ˜¯å¦æœ‰å·®å¼‚
  - è¿›è¡Œç»Ÿè®¡åˆ†æ

## Conclusion

- ä¸€æ€»ç»“ã€ä¸€ç»“æœã€ä¸€å±•æœ›
- æ€»ç»“åšäº†ä»€ä¹ˆå·¥ä½œï¼ˆ1-2 å¥ï¼‰
- ä¸»è¦ç»“æœæ˜¯ä»€ä¹ˆ
- å±•æœ›å·¥ä½œçš„é‡è¦æ€§ï¼Œå‡åç ”ç©¶æ„ä¹‰

# paper style examples

## good in general

- [Query in Your Tongue: Reinforce Large Language Models with
](https://dl.acm.org/doi/pdf/10.1145/3589334.3645701)
  - good in general
- [Optimizing Error-Bounded Lossy Compression for Scientific Data on GPUs](https://openreview.net/pdf?id=4SXEZSNxTe)

## comparable studies

## survey

- Dense Text Retrieval based on Pretrained Language Models: A Survey

## evaluation only

- More Room for Language: Investigating the Effect of Retrieval on Language Models
  - examine how retrieval augmentation affects the behavior of the underlying language model.

## no innovation but vertical application

- BMRETRIEVER: Tuning Large Language Models as Better Biomedical Text Retrievers

# Tables

## color and title

- ![alt text](image-4.png)
- from: More Room for Language: Investigating the Effect of Retrieval on Language Models


# tools

## polish

* <https://www.citexs.com/Editing>